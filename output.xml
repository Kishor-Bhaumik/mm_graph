<onefilellm_output>
<source type="github_repository" url="https://github.com/mm-graph-benchmark/mm-graph-benchmark/">

<file path="README.md">
# mm-graph-benchmark

This is the official repository of paper Multimodal Graph Benchmark.

# Installation

```bash
# Clone the repo
git clone https://github.com/mm-graph-benchmark/mm-graph-benchmark.git
cd mm-graph-benchmark

# Create the environment
conda create -n mm_bench python=3.10
conda activate mm_bench

# Install PyTorch and DGL. 
# Here we assume that the CUDA version is 11.8. You may need to modify this based on your CUDA version. 
# For more information, visit https://pytorch.org/get-started/previous-versions/ and https://www.dgl.ai/pages/start.html
conda install pytorch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 pytorch-cuda=11.8 -c pytorch -c nvidia
conda install -c dglteam/label/th21_cu118 dgl

# Install other dependencies
pip install pandas numpy scikit-learn
```

# Data Preparation

Download our datasets from [this link](https://huggingface.co/datasets/mm-graph-org/mm-graph). You may save them to any directory you like, such as `./Multimodal-Graph-Completed-Graph`. The structure should look like the following tree diagram. You can easily add new datasets following this format.

```bash
.
├── books-lp
│   ├── lp-edge-split-random.pt
│   ├── clip_feat.pt
│   ├── imagebind_feat.pt
│   ├── t5vit_feat.pt
│   └── t5dino_feat.pt
├── sports-copurchase
│   ├── lp-edge-split-hard.pt
│   ├── clip_feat.pt
│   ├── imagebind_feat.pt
│   ├── t5vit_feat.pt
│   └── t5dino_feat.pt
├── cloth-copurchase
│   ├── lp-edge-split-hard.pt
│   ├── clip_feat.pt
│   ├── imagebind_feat.pt
│   ├── t5vit_feat.pt
│   └── t5dino_feat.pt
├── ele-fashion
│   ├── nc_edges-nodeid.pt
│   ├── split.pt
│   ├── labels-w-missing.pt
│   ├── clip_feat.pt
│   ├── imagebind_feat.pt
│   ├── t5vit_feat.pt
│   └── t5dino_feat.pt
└── books-nc
    ├── nc_edges-nodeid.pt
    ├── split.pt
    ├── labels-w-missing.pt
    ├── clip_feat.pt
    ├── imagebind_feat.pt
    ├── t5vit_feat.pt
    └── t5dino_feat.pt
```

# Examples

## NodeClassificationDataset

```python
import os
from nc_dataset import NodeClassificationDataset, NodeClassificationEvaluator

data_path = './Multimodal-Graph-Completed-Graph' # replace this with the path where you save the datasets
dataset_name = 'books-nc'
feat_name = 't5vit'
verbose = True
device = 'cpu' # use 'cuda' if GPU is available

dataset = NodeClassificationDataset(
	root=os.path.join(data_path, dataset_name),
	feat_name=feat_name,
	verbose=verbose,
	device=device
)

graph = dataset.graph
# type(graph) would be dgl.DGLGraph
# use graph.ndata['feat'] to get the features
# use graph.ndata['label'] to get the labels (i.e., classes)
# use graph.ndata['train_mask'], graph.ndata['val_mask'], and graph.ndata['test_mask'] to get the corresponding masks

#########################

eval_metric = 'rocauc' # 'acc' is also supported
evaluator = NodeClassificationEvaluator(eval_metric=eval_metric)
# use evaluator.expected_input_format and evaluator.expected_output_format to see the details about the format

input_dict = {'y_true': ..., 'y_pred': ...} # get input_dict using the model you trained
result = evaluator.eval(input_dict=input_dict)
```

## LinkPredictionDataset

```python
import os
from lp_dataset import LinkPredictionDataset, LinkPredictionEvaluator

data_path = './Multimodal-Graph-Completed-Graph' # replace this with the path where you save the datasets
dataset_name = 'sports-copurchase'
feat_name = 't5vit'
edge_split_type = 'hard'
verbose = True
device = 'cpu' # use 'cuda' if GPU is available

dataset = LinkPredictionDataset(
	root=os.path.join(data_path, dataset_name),
	feat_name=feat_name,
	edge_split_type=edge_split_type,
	verbose=verbose,
	device=device
)

graph = dataset.graph
# type(graph) would be dgl.DGLGraph
# use graph.ndata['feat'] to get the features

edge_split = dataset.get_edge_split()
# edge_split = {
#     'train': {
#         'source_node': ...,
#         'target_node': ...,
#     },
#     'valid': {
#         'source_node': ...,
#         'target_node': ...,
#         'target_node_neg': ...,
#     }
#     'test': {
#         'source_node': ...,
#         'target_node': ...,
#         'target_node_neg': ...,
#     }
# }

#########################

evaluator = LinkPredictionEvaluator()
# these metrics will be automatically calculated: MRR, Hits@1, Hits@3, and Hits@10
# use evaluator.expected_input_format and evaluator.expected_output_format to see the details about the format

input_dict = {'y_pred_pos': ..., 'y_pred_neg': ...} # get input_dict using the model you trained
result = evaluator.eval(input_dict=input_dict)
```
#########################
## Raw Images

Raw images can be downloaded by using `node_mapping.pt` (which provides 1-1 mapping for node id and raw file id) for each dataset. A reference code for downloading can be found in `download_img.py`.
The products metadata can be obtained from: https://cseweb.ucsd.edu/~jmcauley/datasets.html#amazon_reviews and https://mengtingwan.github.io/data/goodreads.html. The zipped folder can be found [here](https://huggingface.co/datasets/mm-graph-org/mm-graph/tree/main).

## Raw text

Raw images can be downloaded by using `node_mapping.pt` (which provides 1-1 mapping for node id and raw file id) for each dataset. 
The products metadata can be obtained from: https://cseweb.ucsd.edu/~jmcauley/datasets.html#amazon_reviews and https://mengtingwan.github.io/data/goodreads.html.  The zipped folder can be found [here](https://huggingface.co/datasets/mm-graph-org/mm-graph/tree/main).

</file>

<file path="download_img.py">
import json
import os

import os
import requests
import concurrent.futures

import json
import gzip
from tqdm import tqdm
import torch

######## we only handle train splits here first  ############
patton_sports_dir = "./Patton/sports"
pairs_to_parse = torch.load(os.path.join(patton_sports_dir, 'splits.pt'))['train']
raw_data = torch.load('amazon_products_metadata.pt')

data_ls = []
seen_nodes = set()
for pair in pairs_to_parse:
    if pair[0] not in seen_nodes and pair[0] in raw_data and len(raw_data[pair[0]]['image_url']) > 0:
        node = {}
        node['asin'] = pair[0]
        node['image_url'] = raw_data[pair[0]]['image_url']
        data_ls.append(node)
        seen_nodes.add(pair[0])
    if pair[1] not in seen_nodes and pair[1] in raw_data and len(raw_data[pair[1]]['image_url']) > 0:
        node = {}
        node['asin'] = pair[1]
        node['image_url'] = raw_data[pair[1]]['image_url']
        data_ls.append(node)
        seen_nodes.add(pair[1])

path = './amazon-sports-images'
max_threads = 32

print('Start')
print(f'{len(data_ls)}')

completed_count = 0

def download_image(entry):
    global completed_count
    try:
        asin = entry["asin"]
        img_suffix = entry["image_url"][-3:]
        image_url = entry["image_url"].split("_")[0]+img_suffix
        file_path = os.path.join(path, f"{asin}.jpg")
    
        if os.path.exists(file_path):
            completed_count += 1
            return        
        
        response = requests.get(image_url)
        status = response.status_code
        if status == 200:
            with open(file_path, "wb") as file:
                file.write(response.content)
        completed_count += 1
        if completed_count % (len(data_ls) // 10000) == 0:
            with open("./log.txt", "a") as log_file:
                log_file.write(f"Downloaded {completed_count}/{len(data_ls)} - Book ID: {book_id} Status: {status}\n")
    except:
        return 

with concurrent.futures.ThreadPoolExecutor(max_threads) as executor:
    futures = [executor.submit(download_image, entry) for entry in data_ls]

    for future in futures:
        future.result()
</file>

<file path="lp/Buddy/CONTRIBUTING.md">
We're very happy to receive patches for this code base. If there's something you want to add, then please send us PRs using the standard [GitHub Flow
Workflow](https://guides.github.com/introduction/flow/)
- Fork the project
- Create a feature branch
- Raise a PR 

Many thanks,

the team

</file>

<file path="lp/Buddy/README.md">
# subgraph-sketching 

Many Graph Neural Networks (GNNs) perform poorly compared to simple heuristics on Link Prediction (LP) tasks. This is due to limitations in expressive power such as the inability to count triangles (the backbone of most LP heuristics) and because they can not distinguish automorphic nodes (those having identical structural roles). Both expressiveness issues can be alleviated by learning link (rather than node) representations and incorporating structural features such as triangle counts. Since explicit link representations are often prohibitively expensive, recent
works resorted to subgraph-based methods, which have achieved state-of-the-art performance for LP, but suffer from poor efficiency due to high levels of redundancy between subgraphs. We analyze the components of subgraph GNN (SGNN) methods for link prediction. Based on our analysis, we propose a novel full-graph GNN called ELPH (Efficient Link Prediction with Hashing) that passes subgraph
sketches as messages to approximate the key components of SGNNs without explicit subgraph construction. ELPH is provably more expressive than Message Passing GNNs (MPNNs). It outperforms existing SGNN models on many standard LP benchmarks while being orders of magnitude faster. However, it shares the common GNN limitation that it is only efficient when the dataset fits in GPU memory. Accordingly, we develop a highly scalable model, called BUDDY, which uses feature precomputation to circumvent this limitation without sacrificing predictive performance. Our experiments show that BUDDY also outperforms SGNNs on standard LP benchmarks while being highly scalable and faster than ELPH.

## Introduction

This is a reimplementation of the code used for "Graph Neural Networks for Link Prediction with Subgraph Sketching" https://openreview.net/pdf?id=m1oqEOAozQU which was accepted for oral presentation (top 5% of accepted papers) at ICLR 2023.

The high level structure of the code will not change, but some details such as default parameter settings remain work in progress.

## Dataset and Preprocessing

Create a root level folder
```
./dataset
``` 
Datasets will automatically be downloaded to this folder provided you are connected to the internet.

## Running experiments

### Requirements
Dependencies (with python >= 3.9):
Main dependencies are

pytorch==1.13

torch_geometric==2.2.0

torch-scatter==2.1.1+pt113cpu

torch-sparse==0.6.17+pt113cpu

torch-spline-conv==1.2.2+pt113cpu


Example commands to install the dependencies in a new conda environment (tested on a Linux machine without GPU).
```
conda create --name ss python=3.9
conda activate ss
conda install pytorch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 -c pytorch
pip install torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-1.13.0+cpu.html
pip install torch_geometric
pip install fast-pagerank wandb datasketch ogb
```


For GPU installation (assuming CUDA 11.8): 
```
conda create --name ss python=3.9
conda activate ss
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia
conda install pytorch-sparse -c pyg
conda install pyg -c pyg
pip install fast-pagerank wandb datasketch ogb
```


if you are unfamiliar with wandb, see
[wandb quickstart](https://docs.wandb.ai/quickstart)


### Experiments
To run experiments
```
cd subgraph-sketching/src
conda activate ss
python runners/run.py --dataset_name Cora --model ELPH
python runners/run.py --dataset_name Cora --model BUDDY
python runners/run.py --dataset_name Citeseer --model ELPH
python runners/run.py --dataset_name Citeseer --model BUDDY
python runners/run.py --dataset_name Pubmed --max_hash_hops 3 --feature_dropout 0.2 --model ELPH
python runners/run.py --dataset_name Pubmed --max_hash_hops 3 --feature_dropout 0.2 --model BUDDY
python runners/run.py --dataset_name ogbl-collab --K 50 --lr 0.01 --feature_dropout 0.05 --add_normed_features 1 --label_dropout 0.1 --batch_size 2048 --year 2007 --model ELPH
python runners/run.py --dataset_name ogbl-collab --K 50 --lr 0.02 --feature_dropout 0.05 --add_normed_features 1 --cache_subgraph_features --label_dropout 0.1 --year 2007 --model BUDDY
python runners/run.py --dataset_name ogbl-ppa --label_dropout 0.1 --use_feature 0 --use_RA 1 --lr 0.03 --epochs 100 --hidden_channels 256 --cache_subgraph_features --add_normed_features 1 ----use_zero_one 1 model BUDDY
python runners/run.py --dataset ogbl-ddi --K 20 --train_node_embedding --propagate_embeddings --label_dropout 0.25 --epochs 150 --hidden_channels 256 --lr 0.0015 --num_negs 6 --use_feature 0 --sign_k 2 --batch_size 131072 --model ELPH
python runners/run.py --dataset ogbl-ddi --K 20 --train_node_embedding --propagate_embeddings --label_dropout 0.25 --epochs 150 --hidden_channels 256 --lr 0.0015 --num_negs 6 --use_feature 0 --sign_k 2 --cache_subgraph_features --batch_size 131072 --model BUDDY
python runners/run.py --dataset ogbl-citation2 --hidden_channels 128 --num_negs 5 --lr 0.0005 --sign_dropout 0.2 --feature_dropout 0.7 --label_dropout 0.8 --sign_k 3 --batch_size 261424 --eval_batch_size 522848 --cache_subgraph_features --model BUDDY
```
You may need to adjust 
```
--batch_size 
--num_workers
```
and 
```
--eval_batch_size
```

based on available (GPU) memory and CPU cores.

Most of the runtime of BUDDY is building hashes and subgraph features. If you intend to run BUDDY more than once, then set the flag
```
--cache_subgraph_features
```
to store subgraph features on disk and read them if previously cached.

To reproduce the results submited to the OGB leaderboards https://ogb.stanford.edu/docs/leader_linkprop/ add
```
--reps 10
```
to the list of command line arguments


## Cite us
If you found this work useful, please cite our paper
```
@inproceedings
{chamberlain2023graph,
  title={Graph Neural Networks for Link Prediction with Subgraph Sketching},
  author={Chamberlain, Benjamin Paul and Shirobokov, Sergey and Rossi, Emanuele and Frasca, Fabrizio and Markovich, Thomas and Hammerla, Nils and     Bronstein, Michael M and Hansmire, Max},
  booktitle={ICLR}
  year={2023}
}
```

</file>

<file path="lp/Buddy/src/__init__.py">

</file>

<file path="lp/Buddy/src/data.py">
"""
Read and split ogb and planetoid datasets
"""

import os
import time
from typing import Optional, Tuple, Union
import itertools

import numpy as np
import torch
from torch.utils.data import DataLoader
from torch import Tensor
from ogb.linkproppred import PygLinkPropPredDataset
from torch_geometric.datasets import Planetoid
from torch_geometric.transforms import RandomLinkSplit
from torch_geometric.utils import (add_self_loops, negative_sampling,
                                   to_undirected)
from torch_geometric.utils._negative_sampling import vector_to_edge_index, edge_index_to_vector, sample
from torch_geometric.utils.num_nodes import maybe_num_nodes
from torch_geometric.loader import DataLoader as pygDataLoader
from torch_geometric.data import Dataset, Data, InMemoryDataset

import wandb

from src.utils import ROOT_DIR, get_same_source_negs, neighbors
from src.lcc import get_largest_connected_component, remap_edges, get_node_mapper
from src.datasets.seal import get_train_val_test_datasets
from src.datasets.elph import get_hashed_train_val_test_datasets, make_train_eval_data


class CustomPygLinkPropPredDataset(InMemoryDataset):
    def __init__(self, root, feat, hard=True, transform=None, pre_transform=None):
        # Load all necessary components before calling the super constructor
        if hard:
            self.edge_split = torch.load(os.path.join(root, 'lp-edge-split-hard.pt'))
        else:
            self.edge_split = torch.load(os.path.join(root, 'lp-edge-split-random.pt'))

        print(feat)

        if feat == "clip":
            self.node_features = torch.load(os.path.join(root, 'clip_feat.pt'))
        elif feat == "t5":
            self.node_features = torch.load(os.path.join(root, 't5vit_feat.pt'))
        elif feat == "imagebind":
            self.node_features = torch.load(os.path.join(root, 'imagebind_feat.pt'))
        else:
            raise ValueError('no such feature')
        super().__init__(root, transform, pre_transform)
        self.data = None
        self.process()

    def process(self):
        # Assuming that 'train' split contains positive edges already in the correct format
        train_pos_edge = torch.stack([
            self.edge_split['train']['source_node'],
            self.edge_split['train']['target_node']
        ], dim=0)

        # Initialize the graph data object for training edges
        self.data = Data(x=self.node_features, edge_index=train_pos_edge)

    def get_edge_split(self):
        # Return the edge splits with negative sampling for training
        return self.edge_split

    @property
    def raw_file_names(self):
        return []

    @property
    def processed_file_names(self):
        return []

    def download(self):
        pass  # Data is precomputed

    def len(self):
        return 1

    def get(self, idx):
        return self.data


def get_loaders(args, dataset, splits, directed):
    train_data, val_data, test_data = splits['train'], splits['valid'], splits['test']
    if args.model in {'ELPH', 'BUDDY'}:
        train_dataset, val_dataset, test_dataset = get_hashed_train_val_test_datasets(dataset, train_data, val_data,
                                                                                      test_data, args, directed)
    else:
        t0 = time.time()
        train_dataset, val_dataset, test_dataset = get_train_val_test_datasets(dataset, train_data, val_data, test_data,
                                                                               args)
        # print(f'SEAL preprocessing ran in {time.time() - t0} s')
        if args.wandb:
            wandb.log({"seal_preprocessing_time": time.time() - t0})

    dl = DataLoader if args.model in {'ELPH', 'BUDDY'} else pygDataLoader
    train_loader = dl(train_dataset, batch_size=args.batch_size,
                      shuffle=True, num_workers=args.num_workers)
    # as the val and test edges are often sampled they also need to be shuffled
    # the citation2 dataset has specific negatives for each positive and so can't be shuffled
    shuffle_val = False if args.dataset_name.startswith('ogbl-citation') else True
    val_loader = dl(val_dataset, batch_size=args.batch_size, shuffle=shuffle_val,
                    num_workers=args.num_workers)
    shuffle_test = False if args.dataset_name.startswith('ogbl-citation') else True
    test_loader = dl(test_dataset, batch_size=args.batch_size, shuffle=shuffle_test,
                     num_workers=args.num_workers)
    if (args.dataset_name == 'ogbl-citation2') and (args.model in {'ELPH', 'BUDDY'}):
        train_eval_loader = dl(
            make_train_eval_data(args, train_dataset, train_data.num_nodes,
                                 n_pos_samples=5000), batch_size=args.batch_size, shuffle=False,
            num_workers=args.num_workers)
    else:
        # todo change this so that eval doesn't have to use the full training set
        train_eval_loader = train_loader

    return train_loader, train_eval_loader, val_loader, test_loader


def get_data(args):
    """
    Read the dataset and generate train, val and test splits.
    For GNN link prediction edges play 2 roles 1/ message passing edges 2/ supervision edges
    - train message passing edges = train supervision edges
    - val message passing edges = train supervision edges
    val supervision edges are disjoint from the training edges
    - test message passing edges = val supervision + train message passing (= val message passing)
    test supervision edges are disjoint from both val and train supervision edges
    :param args: arguments Namespace object
    :return: dataset, dic splits, bool directed, str eval_metric
    """
    include_negatives = True
    dataset_name = args.dataset_name
    val_pct = args.val_pct
    test_pct = args.test_pct
    use_lcc_flag = True
    directed = False
    eval_metric = 'hits'
    path = os.path.join(ROOT_DIR, 'dataset', dataset_name)
    # print(f'reading data from: {path}')
    if dataset_name.startswith('ogbl'):
        use_lcc_flag = False
        dataset = PygLinkPropPredDataset(name=dataset_name, root=path)
        if dataset_name == 'ogbl-ddi':
            dataset.data.x = torch.ones((dataset.data.num_nodes, 1))
            dataset.data.edge_weight = torch.ones(dataset.data.edge_index.size(1), dtype=int)
    elif dataset_name.startswith('sports') or dataset_name.startswith('cloth'):
        use_lcc_flag = False
        dataset = CustomPygLinkPropPredDataset(root=f"{ROOT_DIR}/../../../../Patton/{dataset_name}",
                                               feat=args.feat)
    elif dataset_name.startswith('book'):
        use_lcc_flag = False
        dataset = CustomPygLinkPropPredDataset(root=f"{ROOT_DIR}/../../../../Patton/{dataset_name}",
                                               feat=args.feat, hard=False)
    else:
        dataset = Planetoid(path, dataset_name)

    # print(dataset.data)

    # set the metric
    if dataset_name.startswith('ogbl-citation'):
        eval_metric = 'mrr'
        directed = True

    if use_lcc_flag:
        dataset = use_lcc(dataset)

    undirected = not directed

    if dataset_name.startswith('ogbl') or dataset_name.startswith('sports') or dataset_name.startswith(
            'cloth') or dataset_name.startswith('book'):  # use the built in splits
        data = dataset[0]
        split_edge = dataset.get_edge_split()
        if dataset_name == 'ogbl-collab' and args.year > 0:  # filter out training edges before args.year
            data, split_edge = filter_by_year(data, split_edge, args.year)
        splits = get_ogb_data(data, split_edge, dataset_name, args.num_negs)
    else:  # make random splits
        transform = RandomLinkSplit(is_undirected=undirected, num_val=val_pct, num_test=test_pct,
                                    add_negative_train_samples=include_negatives)
        train_data, val_data, test_data = transform(dataset.data)
        splits = {'train': train_data, 'valid': val_data, 'test': test_data}

    # print(splits)

    return dataset, splits, directed, eval_metric


def filter_by_year(data, split_edge, year):
    """
    remove edges before year from data and split edge
    @param data: pyg Data, pyg SplitEdge
    @param split_edges:
    @param year: int first year to use
    @return: pyg Data, pyg SplitEdge
    """
    selected_year_index = torch.reshape(
        (split_edge['train']['year'] >= year).nonzero(as_tuple=False), (-1,))
    split_edge['train']['edge'] = split_edge['train']['edge'][selected_year_index]
    split_edge['train']['weight'] = split_edge['train']['weight'][selected_year_index]
    split_edge['train']['year'] = split_edge['train']['year'][selected_year_index]
    train_edge_index = split_edge['train']['edge'].t()
    # create adjacency matrix
    new_edges = to_undirected(train_edge_index, split_edge['train']['weight'], reduce='add')
    new_edge_index, new_edge_weight = new_edges[0], new_edges[1]
    data.edge_index = new_edge_index
    data.edge_weight = new_edge_weight.unsqueeze(-1)
    return data, split_edge


def get_ogb_data(data, split_edge, dataset_name, num_negs=1):
    """
    ogb datasets come with fixed train-val-test splits and a fixed set of negatives against which to evaluate the test set
    The dataset.data object contains all of the nodes, but only the training edges
    @param dataset:
    @param use_valedges_as_input:
    @return:
    """
    if num_negs == 1:
        negs_name = f'{ROOT_DIR}/dataset/{dataset_name}/negative_samples.pt'
    else:
        negs_name = f'{ROOT_DIR}/dataset/{dataset_name}/negative_samples_{num_negs}.pt'
    # print(f'looking for negative edges at {negs_name}')
    if os.path.exists(negs_name):
        # print('loading negatives from disk')
        train_negs = torch.load(negs_name)
    else:
        # print('negatives not found on disk. Generating negatives')
        train_negs = get_ogb_train_negs(split_edge, data.edge_index, data.num_nodes, num_negs, dataset_name)
        torch.save(train_negs, negs_name)
    # else:
    #     train_negs = get_ogb_train_negs(split_edge, data.edge_index, data.num_nodes, num_negs, dataset_name)
    splits = {}
    for key in split_edge.keys():
        # the ogb datasets come with test and valid negatives, but you have to cook your own train negs
        neg_edges = train_negs if key == 'train' else None
        # print(split_edge)
        # print(key)
        # print(split_edge[key])
        edge_label, edge_label_index = make_obg_supervision_edges(split_edge, key, neg_edges)
        # use the validation edges for message passing at test time
        # according to the rules https://ogb.stanford.edu/docs/leader_rules/ only collab can use val edges at test time
        if key == 'test' and dataset_name == 'ogbl-collab':
            vei, vw = to_undirected(split_edge['valid']['edge'].t(), split_edge['valid']['weight'])
            edge_index = torch.cat([data.edge_index, vei], dim=1)
            edge_weight = torch.cat([data.edge_weight, vw.unsqueeze(-1)], dim=0)
        else:
            edge_index = data.edge_index
            if hasattr(data, "edge_weight"):
                edge_weight = data.edge_weight
            else:
                edge_weight = torch.ones(data.edge_index.shape[1])
        splits[key] = Data(x=data.x, edge_index=edge_index, edge_weight=edge_weight, edge_label=edge_label,
                           edge_label_index=edge_label_index)
    return splits


def get_ogb_pos_edges(split_edge, split):
    if 'edge' in split_edge[split]:
        pos_edge = split_edge[split]['edge']
    elif 'source_node' in split_edge[split]:
        pos_edge = torch.stack([split_edge[split]['source_node'], split_edge[split]['target_node']],
                               dim=1)
    else:
        raise NotImplementedError
    return pos_edge


def get_ogb_train_negs(split_edge, edge_index, num_nodes, num_negs=1, dataset_name=None):
    """
    for some inexplicable reason ogb datasets split_edge object stores edge indices as (n_edges, 2) tensors
    @param split_edge:

    @param edge_index: A [2, num_edges] tensor
    @param num_nodes:
    @param num_negs: the number of negatives to sample for each positive
    @return: A [num_edges * num_negs, 2] tensor of negative edges
    """
    pos_edge = get_ogb_pos_edges(split_edge, 'train').t()
    if dataset_name is not None and dataset_name.startswith('ogbl-citation'):
        neg_edge = get_same_source_negs(num_nodes, num_negs, pos_edge)
    else:  # any source is fine
        new_edge_index, _ = add_self_loops(edge_index)
        neg_edge = negative_sampling(
            new_edge_index, num_nodes=num_nodes,
            num_neg_samples=pos_edge.size(1) * num_negs)
    return neg_edge.t()


def make_obg_supervision_edges(split_edge, split, neg_edges=None):
    if neg_edges is not None:
        neg_edges = neg_edges
    else:
        if 'edge_neg' in split_edge[split]:
            neg_edges = split_edge[split]['edge_neg']
        elif 'target_node_neg' in split_edge[split]:
            n_neg_nodes = split_edge[split]['target_node_neg'].shape[1]
            neg_edges = torch.stack([split_edge[split]['source_node'].unsqueeze(1).repeat(1, n_neg_nodes).ravel(),
                                     split_edge[split]['target_node_neg'].ravel()
                                     ]).t()
        else:
            raise NotImplementedError

    pos_edges = get_ogb_pos_edges(split_edge, split)
    n_pos, n_neg = pos_edges.shape[0], neg_edges.shape[0]
    edge_label = torch.cat([torch.ones(n_pos), torch.zeros(n_neg)], dim=0)
    edge_label_index = torch.cat([pos_edges, neg_edges], dim=0).t()
    return edge_label, edge_label_index


def use_lcc(dataset):
    lcc = get_largest_connected_component(dataset)

    x_new = dataset.data.x[lcc]
    y_new = dataset.data.y[lcc]

    row, col = dataset.data.edge_index.numpy()
    edges = [[i, j] for i, j in zip(row, col) if i in lcc and j in lcc]
    edges = remap_edges(edges, get_node_mapper(lcc))

    data = Data(
        x=x_new,
        edge_index=torch.LongTensor(edges),
        y=y_new,
        train_mask=torch.zeros(y_new.size()[0], dtype=torch.bool),
        test_mask=torch.zeros(y_new.size()[0], dtype=torch.bool),
        val_mask=torch.zeros(y_new.size()[0], dtype=torch.bool)
    )
    dataset.data = data
    return dataset


def sample_hard_negatives(edge_index: Tensor,
                          num_nodes: Optional[Union[int, Tuple[int, int]]] = None,
                          num_neg_samples: Optional[int] = None) -> Tensor:
    """
    Sample hard negatives for each edge in edge_index
    @param edge_index:
    @return:
    """
    if num_nodes is None:
        num_nodes = maybe_num_nodes(edge_index)
    # get the size of the population of edges and the index of the existing edges into this population
    idx, population = edge_index_to_vector(edge_index, (num_nodes, num_nodes), bipartite=False)
    # for each node, get all of the neighbours and produce all edges that have that node as a common neigbour
    common_neighbour_edges = []
    for node in range(num_nodes):
        neighbours = edge_index[1, edge_index[0] == node]
        # get all edges that have a common neighbour with node
        edges = list(itertools.combinations(neighbours, 2))
        common_neighbour_edges.extend(edges)
    unique_common_neighbour_edges = list(set(common_neighbour_edges))
    # get the unique edges that are not in the graph
    # 1. turn this into an edge index
    # 2. get the index of the common neighbour edges into the population
    # 3. get common neighbours that are not in the graph
    # 4. maybe sample

    # get the index of the common neighbour edges into the population

    # sample num_neg_samples edges from the population of common neighbour edges
    idx = idx.to('cpu')
    for _ in range(3):  # Number of tries to sample negative indices.
        rnd = sample(population, num_neg_samples, device='cpu')
        mask = np.isin(rnd, idx)
        if neg_idx is not None:
            mask |= np.isin(rnd, neg_idx.to('cpu'))
        mask = torch.from_numpy(mask).to(torch.bool)
        rnd = rnd[~mask].to(edge_index.device)
        neg_idx = rnd if neg_idx is None else torch.cat([neg_idx, rnd])
        if neg_idx.numel() >= num_neg_samples:
            neg_idx = neg_idx[:num_neg_samples]
            break

</file>

<file path="lp/Buddy/src/datasets/__init__.py">

</file>

<file path="lp/Buddy/src/datasets/elph.py">
"""
constructing the hashed data objects used by elph and buddy
"""

import os
from time import time

import torch
from torch_geometric.data import Dataset
from torch_geometric.utils import to_undirected
from torch_sparse import coalesce
import scipy.sparse as ssp
import torch_sparse
from torch_geometric.nn.conv.gcn_conv import gcn_norm

from src.heuristics import RA
from src.utils import ROOT_DIR, get_src_dst_degree, get_pos_neg_edges, get_same_source_negs
from src.hashing import ElphHashes


class HashDataset(Dataset):
    """
    A class that combines propagated node features x, and subgraph features that are encoded as sketches of
    nodes k-hop neighbors
    """

    def __init__(
            self, root, split, data, pos_edges, neg_edges, args, use_coalesce=False,
            directed=False, **kwargs):
        if args.model != 'ELPH':  # elph stores the hashes directly in the model class for message passing
            self.elph_hashes = ElphHashes(args)  # object for hash and subgraph feature operations
        self.split = split  # string: train, valid or test
        self.root = root
        self.pos_edges = pos_edges
        self.neg_edges = neg_edges
        self.use_coalesce = use_coalesce
        self.directed = directed
        self.args = args
        self.load_features = args.load_features
        self.load_hashes = args.load_hashes
        self.use_zero_one = args.use_zero_one
        self.cache_subgraph_features = args.cache_subgraph_features
        self.max_hash_hops = args.max_hash_hops
        self.use_feature = args.use_feature
        self.use_RA = args.use_RA
        self.hll_p = args.hll_p
        self.subgraph_features = None
        self.hashes = None
        super(HashDataset, self).__init__(root)

        self.links = torch.cat([self.pos_edges, self.neg_edges], 0)  # [n_edges, 2]
        self.labels = [1] * self.pos_edges.size(0) + [0] * self.neg_edges.size(0)

        if self.use_coalesce:  # compress multi-edge into edge with weight
            data.edge_index, data.edge_weight = coalesce(
                data.edge_index, data.edge_weight,
                data.num_nodes, data.num_nodes)

        if 'edge_weight' in data:
            self.edge_weight = data.edge_weight.view(-1)
        else:
            self.edge_weight = torch.ones(data.edge_index.size(1), dtype=int)
        if self.directed:  # make undirected graphs like citation2 directed
            # print(
            #     f'this is a directed graph. Making the adjacency matrix undirected to propagate features and calculate subgraph features')
            self.edge_index, self.edge_weight = to_undirected(data.edge_index, self.edge_weight)
        else:
            self.edge_index = data.edge_index
        self.A = ssp.csr_matrix(
            (self.edge_weight, (self.edge_index[0], self.edge_index[1])),
            shape=(data.num_nodes, data.num_nodes)
        )

        self.degrees = torch.tensor(self.A.sum(axis=0, dtype=float), dtype=torch.float).flatten()

        if self.use_RA:
            self.RA = RA(self.A, self.links, batch_size=2000000)[0]

        if args.model == 'ELPH':  # features propagated in the model instead of preprocessed
            self.x = data.x
        else:
            self.x = self._preprocess_node_features(data, self.edge_index, self.edge_weight, args.sign_k)
            # ELPH does hashing and feature prop on the fly
            # either set self.hashes or self.subgraph_features depending on cmd args
            self._preprocess_subgraph_features(self.edge_index.device, data.num_nodes, args.num_negs)

    def _generate_sign_features(self, data, edge_index, edge_weight, sign_k):
        """
        Generate features by preprocessing using the Scalable Inception Graph Neural Networks (SIGN) method
         https://arxiv.org/abs/2004.11198
        @param data: A pyg data object
        @param sign_k: the maximum number of times to apply the propagation operator
        @return:
        """
        try:
            num_nodes = data.x.size(0)
        except AttributeError:
            num_nodes = data.num_nodes
        edge_index, edge_weight = gcn_norm(  # yapf: disable
            edge_index, edge_weight.float(), num_nodes)
        if sign_k == 0:
            # for most datasets it works best do one step of propagation
            xs = torch_sparse.spmm(edge_index, edge_weight, data.x.shape[0], data.x.shape[0], data.x)
        else:
            xs = [data.x]
            for _ in range(sign_k):
                x = torch_sparse.spmm(edge_index, edge_weight, data.x.shape[0], data.x.shape[0], data.x)
                xs.append(x)
            xs = torch.cat(xs, dim=-1)
        return xs

    def _preprocess_node_features(self, data, edge_index, edge_weight, sign_k=0):
        """
        preprocess the node features
        @param data: pyg Data object
        @param edge_weight: pyg edge index Int Tensor [edges, 2]
        @param sign_k: the number of propagation steps used by SIGN
        @return: Float Tensor [num_nodes, hidden_dim]
        """
        if sign_k == 0:
            feature_name = f'{self.root}_{self.split}_featurecache.pt'
        else:
            feature_name = f'{self.root}_{self.split}_k{sign_k}_featurecache.pt'
        if self.load_features and os.path.exists(feature_name):
            # print('loading node features from disk')
            x = torch.load(feature_name).to(edge_index.device)
        else:
            # print('constructing node features')
            start_time = time()
            x = self._generate_sign_features(data, edge_index, edge_weight, sign_k)
            # print("Preprocessed features in: {:.2f} seconds".format(time() - start_time))
            if self.load_features:
                torch.save(x.cpu(), feature_name)
        return x

    def _read_subgraph_features(self, name, device):
        """
        return True if the subgraph features can be read off disk, otherwise returns False
        @param name:
        @param device:
        @return:
        """
        retval = False
        # look on disk
        if self.cache_subgraph_features and os.path.exists(name):
            # print(f'looking for subgraph features in {name}')
            self.subgraph_features = torch.load(name).to(device)
            # print(f"cached subgraph features found at: {name}")
            assert self.subgraph_features.shape[0] == len(
                self.links), 'subgraph features are inconsistent with the link object. Delete subgraph features file and regenerate'
            retval = True
        return retval

    def _generate_file_names(self, num_negs):
        """
        get the subgraph feature file name and the stubs needed to make a new one if necessary
        :param num_negs: Int negative samples / positive sample
        :return:
        """
        if self.max_hash_hops != 2:
            hop_str = f'{self.max_hash_hops}hop_'
        else:
            hop_str = ''
        end_str = f'_{hop_str}subgraph_featurecache.pt'
        if self.args.dataset_name == 'ogbl-collab' and self.args.year > 0:
            year_str = f'year_{self.args.year}'
        else:
            year_str = ''
        if num_negs == 1 or self.split != 'train':
            subgraph_cache_name = f'{self.root}{self.split}{year_str}{end_str}'
        else:
            subgraph_cache_name = f'{self.root}{self.split}_negs{num_negs}{year_str}{end_str}'
        return subgraph_cache_name, year_str, hop_str

    def _preprocess_subgraph_features(self, device, num_nodes, num_negs=1):
        """
        Handles caching of hashes and subgraph features where each edge is fully hydrated as a preprocessing step
        Sets self.subgraph_features
        @return:
        """
        subgraph_cache_name, year_str, hop_str = self._generate_file_names(num_negs)
        found_subgraph_features = self._read_subgraph_features(subgraph_cache_name, device)
        if not found_subgraph_features:
            # if self.cache_subgraph_features:
            # print(f'no subgraph features found at {subgraph_cache_name}')
            # print('generating subgraph features')
            hash_name = f'{self.root}{self.split}{year_str}_{hop_str}hashcache.pt'
            cards_name = f'{self.root}{self.split}{year_str}_{hop_str}cardcache.pt'
            if self.load_hashes and os.path.exists(hash_name):
                # print('loading hashes from disk')
                hashes = torch.load(hash_name)
                if os.path.exists(cards_name):
                    # print('loading cards from disk')
                    cards = torch.load(cards_name)
                else:
                    pass
                    # print(f'hashes found at {hash_name}, but cards not found. Delete hashes and run again')
            else:
                # print('no hashes found on disk, constructing hashes...')
                start_time = time()
                hashes, cards = self.elph_hashes.build_hash_tables(num_nodes, self.edge_index)
                # print("Preprocessed hashes in: {:.2f} seconds".format(time() - start_time))
                if self.load_hashes:
                    torch.save(cards, cards_name)
                    torch.save(hashes, hash_name)
            # print('constructing subgraph features')
            start_time = time()
            self.subgraph_features = self.elph_hashes.get_subgraph_features(self.links, hashes, cards,
                                                                            self.args.subgraph_feature_batch_size)
            # print("Preprocessed subgraph features in: {:.2f} seconds".format(time() - start_time))
            assert self.subgraph_features.shape[0] == len(
                self.links), 'subgraph features are a different shape link object. Delete subgraph features file and regenerate'
            if self.cache_subgraph_features:
                torch.save(self.subgraph_features, subgraph_cache_name)
        if self.args.floor_sf and self.subgraph_features is not None:
            self.subgraph_features[self.subgraph_features < 0] = 0
            # print(
            #     f'setting {torch.sum(self.subgraph_features[self.subgraph_features < 0]).item()} negative values to zero')
        if not self.use_zero_one and self.subgraph_features is not None:  # knock out the zero_one features (0,1) and (1,0)
            if self.max_hash_hops > 1:
                self.subgraph_features[:, [4, 5]] = 0
            if self.max_hash_hops == 3:
                self.subgraph_features[:, [11, 12]] = 0  # also need to get rid of (0, 2) and (2, 0)

    def len(self):
        return len(self.links)

    def get(self, idx):
        src, dst = self.links[idx]
        if self.args.use_struct_feature:
            subgraph_features = self.subgraph_features[idx]
        else:
            subgraph_features = torch.zeros(self.max_hash_hops * (2 + self.max_hash_hops))

        y = self.labels[idx]
        if self.use_RA:
            RA = self.A[src].dot(self.A_RA[dst].T)[0, 0]
            RA = torch.tensor([RA], dtype=torch.float)
        else:
            RA = -1
        src_degree, dst_degree = get_src_dst_degree(src, dst, self.A, None)
        node_features = torch.cat([self.x[src].unsqueeze(dim=0), self.x[dst].unsqueeze(dim=0)], dim=0)
        return subgraph_features, node_features, src_degree, dst_degree, RA, y


def get_hashed_train_val_test_datasets(dataset, train_data, val_data, test_data, args, directed=False):
    root = f'{dataset.root}/elph_'
    # print(f'data path: {root}')
    use_coalesce = True if args.dataset_name == 'ogbl-collab' else False
    pos_train_edge, neg_train_edge = get_pos_neg_edges(train_data)
    pos_val_edge, neg_val_edge = get_pos_neg_edges(val_data)
    pos_test_edge, neg_test_edge = get_pos_neg_edges(test_data)
    # print(
    #     f'before sampling, considering a superset of {pos_train_edge.shape[0]} pos, {neg_train_edge.shape[0]} neg train edges '
    #     f'{pos_val_edge.shape[0]} pos, {neg_val_edge.shape[0]} neg val edges '
    #     f'and {pos_test_edge.shape[0]} pos, {neg_test_edge.shape[0]} neg test edges for supervision')
    # print('constructing training dataset object')
    train_dataset = HashDataset(root, 'train', train_data, pos_train_edge, neg_train_edge, args,
                                use_coalesce=use_coalesce, directed=directed)
    # print('constructing validation dataset object')
    val_dataset = HashDataset(root, 'valid', val_data, pos_val_edge, neg_val_edge, args,
                              use_coalesce=use_coalesce, directed=directed)
    # print('constructing test dataset object')
    test_dataset = HashDataset(root, 'test', test_data, pos_test_edge, neg_test_edge, args,
                               use_coalesce=use_coalesce, directed=directed)
    return train_dataset, val_dataset, test_dataset


class HashedTrainEvalDataset(Dataset):
    """
    Subset of the full training dataset used to get unbiased estimate of training performance for large datasets
    where otherwise training eval is a significant % of runtime
    """

    def __init__(
            self, links, labels, subgraph_features, RA, dataset):
        super(HashedTrainEvalDataset, self).__init__()
        self.links = links
        self.labels = labels
        self.edge_index = dataset.edge_index
        self.subgraph_features = subgraph_features
        self.x = dataset.x
        self.degrees = dataset.degrees
        self.RA = RA

    def len(self):
        return len(self.links)

    def get(self, idx):
        return self.links[idx]


def make_train_eval_data(args, train_dataset, num_nodes, n_pos_samples=5000, negs_per_pos=1000):
    """
    A much smaller subset of the training data to get a comparable (with test and val) measure of training performance
    to diagnose overfitting
    @param args: Namespace object of cmd args
    @param train_dataset: pyG Dataset object
    @param n_pos_samples: The number of positive samples to evaluate the training set on
    @return: HashedTrainEvalDataset
    """
    # ideally the negatives and the subgraph features are cached and just read from disk
    # need to save train_eval_negs_5000 and train_eval_subgraph_features_5000 files
    # and ensure that the order is always the same just as with the other datasets
    # print('constructing dataset to evaluate training performance')
    dataset_name = args.dataset_name
    pos_sample = train_dataset.pos_edges[:n_pos_samples]  # [num_edges, 2]
    neg_sample = train_dataset.neg_edges[:n_pos_samples * negs_per_pos]  # [num_neg_edges, 2]
    assert torch.all(torch.eq(pos_sample[:, 0].repeat_interleave(negs_per_pos), neg_sample[:,
                                                                                0])), 'negatives have different source nodes to positives. Delete train_eval_negative_samples_* and subgraph features and regenerate'
    links = torch.cat([pos_sample, neg_sample], 0)  # [n_edges, 2]
    labels = [1] * pos_sample.size(0) + [0] * neg_sample.size(0)
    if train_dataset.use_RA:
        pos_RA = train_dataset.RA[:n_pos_samples]
        neg_RA = RA(train_dataset.A, neg_sample, batch_size=2000000)[0]
        RA_links = torch.cat([pos_RA, neg_RA], dim=0)
    else:
        RA_links = None
    pos_sf = train_dataset.subgraph_features[:n_pos_samples]
    n_pos_edges = len(train_dataset.pos_edges)
    neg_sf = train_dataset.subgraph_features[n_pos_edges: n_pos_edges + len(neg_sample)]
    # check these indices are all negative samples
    assert sum(train_dataset.labels[n_pos_edges: n_pos_edges + len(neg_sample)]) == 0
    subgraph_features = torch.cat([pos_sf, neg_sf], dim=0)
    train_eval_dataset = HashedTrainEvalDataset(links, labels, subgraph_features, RA_links, train_dataset)
    return train_eval_dataset

</file>

<file path="lp/Buddy/src/datasets/seal.py">
"""
Code based on
https://github.com/facebookresearch/SEAL_OGB
# Copyright (c) Facebook, Inc. and its affiliates.
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

SEAL reformulates link prediction as a subgraph classification problem. To do this subgraph datasets must first be constructed
"""

from math import inf
import random

import torch
from torch_geometric.data import Data, Dataset, InMemoryDataset
import numpy as np
from torch_geometric.utils import (negative_sampling, add_self_loops)
from torch_sparse import coalesce
from tqdm import tqdm
import scipy.sparse as ssp

from src.utils import get_src_dst_degree, neighbors, get_pos_neg_edges
from src.labelling_tricks import drnl_node_labeling, de_node_labeling, de_plus_node_labeling


class SEALDataset(InMemoryDataset):
    def __init__(self, root, data, pos_edges, neg_edges, num_hops, percent=1., split='train',
                 use_coalesce=False, node_label='drnl', ratio_per_hop=1.0,
                 max_nodes_per_hop=None, max_dist=1000, directed=False, sign=False, k=None):
        self.data = data
        self.pos_edges = pos_edges
        self.neg_edges = neg_edges
        self.num_hops = num_hops
        self.percent = int(percent) if percent >= 1.0 else percent
        self.split = split
        self.use_coalesce = use_coalesce
        self.node_label = node_label
        self.ratio_per_hop = ratio_per_hop
        self.max_nodes_per_hop = max_nodes_per_hop
        self.max_dist = max_dist
        self.directed = directed
        self.sign = sign
        self.k = k
        super(SEALDataset, self).__init__(root)
        self.data, self.slices = torch.load(self.processed_paths[0])

    @property
    def processed_file_names(self):
        if self.percent == 1.:
            name = f'SEAL_{self.split}_data'
        else:
            name = f'SEAL_{self.split}_data_{self.percent}'
        name += '.pt'
        return [name]

    def process(self):

        if self.use_coalesce:  # compress multi-edge into edge with weight
            self.data.edge_index, self.data.edge_weight = coalesce(
                self.data.edge_index, self.data.edge_weight,
                self.data.num_nodes, self.data.num_nodes)

        if 'edge_weight' in self.data:
            edge_weight = self.data.edge_weight.view(-1)
        else:
            edge_weight = torch.ones(self.data.edge_index.size(1), dtype=int)
        A = ssp.csr_matrix(
            (edge_weight, (self.data.edge_index[0], self.data.edge_index[1])),
            shape=(self.data.num_nodes, self.data.num_nodes)
        )

        if self.directed:
            A_csc = A.tocsc()
        else:
            A_csc = None

        # Extract enclosing subgraphs for pos and neg edges
        pos_list = extract_enclosing_subgraphs(
            self.pos_edges, A, self.data.x, 1, self.num_hops, self.node_label,
            self.ratio_per_hop, self.max_nodes_per_hop, self.max_dist, self.directed, A_csc)
        neg_list = extract_enclosing_subgraphs(
            self.neg_edges, A, self.data.x, 0, self.num_hops, self.node_label,
            self.ratio_per_hop, self.max_nodes_per_hop, self.max_dist, self.directed, A_csc)

        torch.save(self.collate(pos_list + neg_list), self.processed_paths[0])
        del pos_list, neg_list


class SEALDynamicDataset(Dataset):
    def __init__(self, root, data, pos_edges, neg_edges, num_hops, percent=1., use_coalesce=False, node_label='drnl',
                 ratio_per_hop=1.0, max_nodes_per_hop=None, max_dist=1000, directed=False, sign=False, k=None,
                 **kwargs):
        self.data = data
        self.pos_edges = pos_edges
        self.neg_edges = neg_edges
        self.num_hops = num_hops
        self.percent = percent
        self.use_coalesce = use_coalesce
        self.node_label = node_label
        self.ratio_per_hop = ratio_per_hop
        self.max_nodes_per_hop = max_nodes_per_hop
        self.max_dist = max_dist
        self.directed = directed
        self.sign = sign
        self.k = k
        super(SEALDynamicDataset, self).__init__(root)

        self.links = torch.cat([self.pos_edges, self.neg_edges], 0).tolist()
        self.labels = [1] * self.pos_edges.size(0) + [0] * self.neg_edges.size(0)

        if self.use_coalesce:  # compress mutli-edge into edge with weight
            self.data.edge_index, self.data.edge_weight = coalesce(
                self.data.edge_index, self.data.edge_weight,
                self.data.num_nodes, self.data.num_nodes)

        if 'edge_weight' in self.data:
            edge_weight = self.data.edge_weight.view(-1)
        else:
            edge_weight = torch.ones(self.data.edge_index.size(1), dtype=int)
        self.A = ssp.csr_matrix(
            (edge_weight, (self.data.edge_index[0], self.data.edge_index[1])),
            shape=(self.data.num_nodes, self.data.num_nodes)
        )
        if self.directed:
            self.A_csc = self.A.tocsc()
        else:
            self.A_csc = None

    def len(self):
        return len(self.links)

    def get(self, idx):
        src, dst = self.links[idx]
        y = self.labels[idx]
        src_degree, dst_degree = get_src_dst_degree(src, dst, self.A, self.max_nodes_per_hop)
        if self.sign:
            x = [self.data.x]
            x += [self.data[f'x{i}'] for i in range(1, self.k + 1)]
        else:
            x = self.data.x
        tmp = k_hop_subgraph(src, dst, self.num_hops, self.A, self.ratio_per_hop,
                             self.max_nodes_per_hop, node_features=x,
                             y=y, directed=self.directed, A_csc=self.A_csc)
        data = construct_pyg_graph(*tmp, self.node_label, self.max_dist, src_degree, dst_degree)

        return data


def sample_data(data, sample_arg):
    if sample_arg <= 1:
        samples = int(sample_arg * len(data))
    elif sample_arg != inf:
        samples = int(sample_arg)
    else:
        samples = len(data)
    if samples != inf:
        sample_indices = torch.randperm(len(data))[:samples]
    return data[sample_indices]


def get_train_val_test_datasets(dataset, train_data, val_data, test_data, args):
    sample = 'all' if not args.sample_size else args.sample_size
    path = f'{dataset.root}_seal_{sample}_hops_{args.num_hops}_maxdist_{args.max_dist}_mnph_{args.max_nodes_per_hop}{args.data_appendix}'
    print(f'seal data path: {path}')
    use_coalesce = True if args.dataset_name == 'ogbl-collab' else False
    # get percents used only for naming the SEAL dataset files and caching
    train_percent, val_percent, test_percent = 1 - (args.val_pct + args.test_pct), args.val_pct, args.test_pct
    # probably should be an attribute of the dataset and not hardcoded
    directed = False
    pos_train_edge, neg_train_edge = get_pos_neg_edges(train_data)
    pos_val_edge, neg_val_edge = get_pos_neg_edges(val_data)
    pos_test_edge, neg_test_edge = get_pos_neg_edges(test_data)
    print(
        f'before sampling, considering a superset of {pos_train_edge.shape[0]} pos, {neg_train_edge.shape[0]} neg train edges '
        f'{pos_val_edge.shape[0]} pos, {neg_val_edge.shape[0]} neg val edges '
        f'and {pos_test_edge.shape[0]} pos, {neg_test_edge.shape[0]} neg test edges for supervision')

    pos_train_edge = sample_data(pos_train_edge, args.train_samples)
    neg_train_edge = sample_data(neg_train_edge, args.train_samples)
    pos_val_edge = sample_data(pos_val_edge, args.val_samples)
    neg_val_edge = sample_data(neg_val_edge, args.val_samples)
    pos_test_edge = sample_data(pos_test_edge, args.test_samples)
    neg_test_edge = sample_data(neg_test_edge, args.test_samples)

    print(
        f'after sampling, using {pos_train_edge.shape[0]} pos, {neg_train_edge.shape[0]} neg train edges '
        f'{pos_val_edge.shape[0]} pos, {neg_val_edge.shape[0]} neg val edges '
        f'and {pos_test_edge.shape[0]} pos, {neg_test_edge.shape[0]} neg test edges for supervision')

    dataset_class = 'SEALDynamicDataset' if args.dynamic_train else 'SEALDataset'
    train_dataset = eval(dataset_class)(
        path,
        train_data,
        pos_train_edge,
        neg_train_edge,
        num_hops=args.num_hops,
        percent=train_percent,
        split='train',
        use_coalesce=use_coalesce,
        node_label=args.node_label,
        ratio_per_hop=args.ratio_per_hop,
        max_nodes_per_hop=args.max_nodes_per_hop,
        max_dist=args.max_dist,
        directed=directed,
        sign=args.model == 'sign',
        k=args.sign_k
    )
    dataset_class = 'SEALDynamicDataset' if args.dynamic_val else 'SEALDataset'
    val_dataset = eval(dataset_class)(
        path,
        val_data,
        pos_val_edge,
        neg_val_edge,
        num_hops=args.num_hops,
        percent=val_percent,
        split='valid',
        use_coalesce=use_coalesce,
        node_label=args.node_label,
        ratio_per_hop=args.ratio_per_hop,
        max_nodes_per_hop=args.max_nodes_per_hop,
        max_dist=args.max_dist,
        directed=directed,
        sign=args.model == 'sign',
        k=args.sign_k
    )
    dataset_class = 'SEALDynamicDataset' if args.dynamic_test else 'SEALDataset'
    test_dataset = eval(dataset_class)(
        path,
        test_data,
        pos_test_edge,
        neg_test_edge,
        num_hops=args.num_hops,
        percent=test_percent,
        split='test',
        use_coalesce=use_coalesce,
        node_label=args.node_label,
        ratio_per_hop=args.ratio_per_hop,
        max_nodes_per_hop=args.max_nodes_per_hop,
        max_dist=args.max_dist,
        directed=directed,
        sign=args.model == 'sign',
        k=args.sign_k
    )
    return train_dataset, val_dataset, test_dataset


def get_seal_pos_neg_edges(split, split_edge, edge_index, num_nodes, percent=100):
    if 'edge' in split_edge['train']:
        pos_edge = split_edge[split]['edge'].t()
        if split == 'train':
            new_edge_index, _ = add_self_loops(edge_index)
            neg_edge = negative_sampling(
                new_edge_index, num_nodes=num_nodes,
                num_neg_samples=pos_edge.size(1))
        else:
            neg_edge = split_edge[split]['edge_neg'].t()
        # subsample for pos_edge
        np.random.seed(123)
        num_pos = pos_edge.size(1)
        perm = np.random.permutation(num_pos)
        perm = perm[:int(percent / 100 * num_pos)]
        pos_edge = pos_edge[:, perm]
        # subsample for neg_edge
        np.random.seed(123)
        num_neg = neg_edge.size(1)
        perm = np.random.permutation(num_neg)
        perm = perm[:int(percent / 100 * num_neg)]
        neg_edge = neg_edge[:, perm]

    elif 'source_node' in split_edge['train']:
        source = split_edge[split]['source_node']
        target = split_edge[split]['target_node']
        if split == 'train':
            target_neg = torch.randint(0, num_nodes, [target.size(0), 1],
                                       dtype=torch.long)
        else:
            target_neg = split_edge[split]['target_node_neg']
        # subsample
        np.random.seed(123)
        num_source = source.size(0)
        perm = np.random.permutation(num_source)
        perm = perm[:int(percent / 100 * num_source)]
        source, target, target_neg = source[perm], target[perm], target_neg[perm, :]
        pos_edge = torch.stack([source, target])
        neg_per_target = target_neg.size(1)
        neg_edge = torch.stack([source.repeat_interleave(neg_per_target),
                                target_neg.view(-1)])
    return pos_edge, neg_edge


def k_hop_subgraph(src, dst, num_hops, A, sample_ratio=1.0,
                   max_nodes_per_hop=None, node_features=None,
                   y=1, directed=False, A_csc=None):
    """
    Extract the k-hop enclosing subgraph around link (src, dst) from A.
    it permutes the node indices so the returned subgraphs are not immediately recognisable as subgraphs and it is not
    parallelised.
    For directed graphs it adds both incoming and outgoing edges in the BFS equally and then for the target edge src->dst
    it will also delete any dst->src edge, it's unclear if this is a feature or a bug.
    :param src: source node for the edge
    :param dst: destination node for the edge
    :param num_hops:
    :param A:
    :param sample_ratio: This will sample down the total number of neighbours (from both src and dst) at each hop
    :param max_nodes_per_hop: This will sample down the total number of neighbours (from both src and dst) at each hop
                            can be used in conjunction with sample_ratio
    :param node_features:
    :param y:
    :param directed:
    :param A_csc:
    :return:
    """
    nodes = [src, dst]
    dists = [0, 0]
    visited = set([src, dst])
    fringe = set([src, dst])
    for hop in range(1, num_hops + 1):
        if not directed:
            fringe = neighbors(fringe, A)
        else:
            out_neighbors = neighbors(fringe, A)
            in_neighbors = neighbors(fringe, A_csc, False)
            fringe = out_neighbors.union(in_neighbors)
        fringe = fringe - visited
        visited = visited.union(fringe)
        if sample_ratio < 1.0:
            fringe = random.sample(fringe, int(sample_ratio * len(fringe)))
        if max_nodes_per_hop is not None:
            if max_nodes_per_hop < len(fringe):
                fringe = random.sample(fringe, max_nodes_per_hop)
        if len(fringe) == 0:
            break
        nodes = nodes + list(fringe)
        dists = dists + [hop] * len(fringe)
    # this will permute the rows and columns of the input graph and so the features must also be permuted
    subgraph = A[nodes, :][:, nodes]

    # Remove target link between the subgraph. Works as the first two elements of nodes are the src and dst node
    # this can throw warnings as csr sparse matrices aren't efficient for removing edges, but these graphs are quite sml
    subgraph[0, 1] = 0
    subgraph[1, 0] = 0

    if isinstance(node_features, list):
        node_features = [feat[nodes] for feat in node_features]
    elif node_features is not None:
        node_features = node_features[nodes]

    return nodes, subgraph, dists, node_features, y


def construct_pyg_graph(node_ids, adj, dists, node_features, y, node_label='drnl', max_dist=1000, src_degree=None,
                        dst_degree=None):
    """
    Constructs a pyg graph for this subgraph and adds an attribute z containing the node_label
    @param node_ids: list of node IDs in the subgraph
    @param adj: scipy sparse CSR adjacency matrix
    @param dists: an n_nodes list containing shortest distance (in hops) to the src or dst node
    @param node_features: The input node features corresponding to nodes in node_ids
    @param y: scalar, 1 if positive edges, 0 if negative edges
    @param node_label: method to add the z attribute to nodes
    @return:
    """
    u, v, r = ssp.find(adj)
    num_nodes = adj.shape[0]

    node_ids = torch.LongTensor(node_ids)
    u, v = torch.LongTensor(u), torch.LongTensor(v)
    r = torch.LongTensor(r)
    edge_index = torch.stack([u, v], 0)
    edge_weight = r.to(torch.float)
    y = torch.tensor([y])
    if node_label == 'drnl':  # DRNL
        z = drnl_node_labeling(adj, 0, 1, max_dist)
    elif node_label == 'hop':  # mininum distance to src and dst
        z = torch.tensor(dists)
    elif node_label == 'zo':  # zero-one labeling trick
        z = (torch.tensor(dists) == 0).to(torch.long)
    elif node_label == 'de':  # distance encoding
        z = de_node_labeling(adj, 0, 1, max_dist)
    elif node_label == 'de+':
        z = de_plus_node_labeling(adj, 0, 1, max_dist)
    elif node_label == 'degree':  # this is technically not a valid labeling trick
        z = torch.tensor(adj.sum(axis=0)).squeeze(0)
        z[z > 100] = 100  # limit the maximum label to 100
    else:
        z = torch.zeros(len(dists), dtype=torch.long)
    data = Data(node_features, edge_index, edge_weight=edge_weight, y=y, z=z,
                node_id=node_ids, num_nodes=num_nodes, src_degree=src_degree, dst_degree=dst_degree)
    return data


def extract_enclosing_subgraphs(link_index, A, x, y, num_hops, node_label='drnl',
                                ratio_per_hop=1.0, max_nodes_per_hop=None, max_dist=1000,
                                directed=False, A_csc=None):
    """
    Extract a num_hops subgraph around every edge in the link index
    @param link_index: positive or negative supervision edges from train, val or test
    @param A: A scipy sparse CSR matrix containing the message passing edge
    @param x: features on the data
    @param y: 1 for positive edges, 0 for negative edges
    @param num_hops: the number of hops from the src or dst node to expand the subgraph to
    @param node_label:
    @param ratio_per_hop:
    @param max_nodes_per_hop:
    @param directed:
    @param A_csc: None if undirected, otherwise converts to a CSC matrix
    @return:
    """
    data_list = []
    for src, dst in tqdm(link_index.tolist()):
        src_degree, dst_degree = get_src_dst_degree(src, dst, A, max_nodes_per_hop)
        tmp = k_hop_subgraph(src, dst, num_hops, A, ratio_per_hop,
                             max_nodes_per_hop, node_features=x, y=y,
                             directed=directed, A_csc=A_csc)
        data = construct_pyg_graph(*tmp, node_label, max_dist, src_degree, dst_degree)
        data_list.append(data)

    return data_list

</file>

<file path="lp/Buddy/src/evaluation.py">
"""
hitrate@k, mean reciprocal rank (MRR) and Area under the receiver operator characteristic curve (AUC) evaluation metrics
"""
from sklearn.metrics import roc_auc_score


def evaluate_hits(evaluator, pos_train_pred, neg_train_pred, pos_val_pred, neg_val_pred, pos_test_pred, neg_test_pred,
                  Ks=[20, 50, 100], use_val_negs_for_train=True):
    """
    Evaluate the hit rate at K
    :param evaluator: an ogb Evaluator object
    :param pos_val_pred: Tensor[val edges]
    :param neg_val_pred: Tensor[neg val edges]
    :param pos_test_pred: Tensor[test edges]
    :param neg_test_pred: Tensor[neg test edges]
    :param Ks: top ks to evaluatate for
    :return: dic[ks]
    """
    results = {}
    # As the training performance is used to assess overfitting it can help to use the same set of negs for
    # train and val comparisons.
    if use_val_negs_for_train:
        neg_train = neg_val_pred
    else:
        neg_train = neg_train_pred
    for K in Ks:
        evaluator.K = K
        train_hits = evaluator.eval({
            'y_pred_pos': pos_train_pred,
            'y_pred_neg': neg_train,
        })[f'hits@{K}']
        valid_hits = evaluator.eval({
            'y_pred_pos': pos_val_pred,
            'y_pred_neg': neg_val_pred,
        })[f'hits@{K}']
        test_hits = evaluator.eval({
            'y_pred_pos': pos_test_pred,
            'y_pred_neg': neg_test_pred,
        })[f'hits@{K}']

        results[f'Hits@{K}'] = (train_hits, valid_hits, test_hits)

    return results


def evaluate_mrr(evaluator, pos_train_pred, neg_train_pred, pos_val_pred, neg_val_pred, pos_test_pred, neg_test_pred):
    """
    Evaluate the mean reciprocal rank at K
    :param evaluator: an ogb Evaluator object
    :param pos_val_pred: Tensor[val edges]
    :param neg_val_pred: Tensor[neg val edges]
    :param pos_test_pred: Tensor[test edges]
    :param neg_test_pred: Tensor[neg test edges]
    :param Ks: top ks to evaluatate for
    :return: dic with single key 'MRR'
    """
    neg_train_pred = neg_train_pred.view(pos_train_pred.shape[0], -1)
    neg_val_pred = neg_val_pred.view(pos_val_pred.shape[0], -1)
    neg_test_pred = neg_test_pred.view(pos_test_pred.shape[0], -1)
    results = {}

    train_mrr = evaluator.eval({
        'y_pred_pos': pos_train_pred,
        # for mrr negs all have the same src, so can't use the val negs, but as long as the same  number of negs / pos are
        # used the results will be comparable.
        'y_pred_neg': neg_train_pred,
    })['mrr_list'].mean().item()

    valid_mrr = evaluator.eval({
        'y_pred_pos': pos_val_pred,
        'y_pred_neg': neg_val_pred,
    })['mrr_list'].mean().item()

    test_mrr = evaluator.eval({
        'y_pred_pos': pos_test_pred,
        'y_pred_neg': neg_test_pred,
    })['mrr_list'].mean().item()

    train_hits1 = evaluator.eval({
        'y_pred_pos': pos_train_pred,
        'y_pred_neg': neg_train_pred,
    })['hits@1_list'].mean().item()

    valid_hits1 = evaluator.eval({
        'y_pred_pos': pos_val_pred,
        'y_pred_neg': neg_val_pred,
    })['hits@1_list'].mean().item()

    test_hits1 = evaluator.eval({
        'y_pred_pos': pos_test_pred,
        'y_pred_neg': neg_test_pred,
    })['hits@1_list'].mean().item()

    train_hits10 = evaluator.eval({
        'y_pred_pos': pos_train_pred,
        'y_pred_neg': neg_train_pred,
    })['hits@10_list'].mean().item()

    valid_hits10 = evaluator.eval({
        'y_pred_pos': pos_val_pred,
        'y_pred_neg': neg_val_pred,
    })['hits@10_list'].mean().item()

    test_hits10 = evaluator.eval({
        'y_pred_pos': pos_test_pred,
        'y_pred_neg': neg_test_pred,
    })['hits@10_list'].mean().item()

    results['MRR'] = (train_mrr, valid_mrr, test_mrr)
    results['Hits@1'] = (train_hits1, valid_hits1, test_hits1)
    results['Hits@10'] = (train_hits10, valid_hits10, test_hits10)
    return results


def evaluate_auc(val_pred, val_true, test_pred, test_true):
    """
    the ROC AUC
    :param val_pred: Tensor[val edges] predictions
    :param val_true: Tensor[val edges] labels
    :param test_pred: Tensor[test edges] predictions
    :param test_true: Tensor[test edges] labels
    :return:
    """
    valid_auc = roc_auc_score(val_true, val_pred)
    test_auc = roc_auc_score(test_true, test_pred)
    results = {}
    results['AUC'] = (valid_auc, test_auc)

    return results

</file>

<file path="lp/Buddy/src/hashing.py">
"""
hashed based data sketching for graphs. Implemented in pytorch, but based on the datasketch library
"""
from time import time
import logging

from tqdm import tqdm
import torch
from torch import float
import numpy as np
from pandas.util import hash_array
from datasketch import HyperLogLogPlusPlus, hyperloglog_const
from torch_geometric.nn import MessagePassing
from torch_geometric.utils import add_self_loops
from torch_geometric.loader import DataLoader

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# the LABEL_LOOKUP primary key is the max hops, secondary key is an index into the feature vector and values are hops
# from nodes (u,v)
LABEL_LOOKUP = {1: {0: (1, 1), 1: (0, 1), 2: (1, 0)},
                2: {0: (1, 1), 1: (2, 1), 2: (1, 2), 3: (2, 2), 4: (0, 1), 5: (1, 0), 6: (0, 2), 7: (2, 0)},
                3: {0: (1, 1), 1: (2, 1), 2: (1, 2), 3: (2, 2), 4: (3, 1), 5: (1, 3), 6: (3, 2), 7: (2, 3), 8: (3, 3),
                    9: (0, 1), 10: (1, 0), 11: (0, 2), 12: (2, 0), 13: (0, 3), 14: (3, 0)}}


class MinhashPropagation(MessagePassing):
    def __init__(self):
        super().__init__(aggr='max')

    @torch.no_grad()
    def forward(self, x, edge_index):
        out = self.propagate(edge_index, x=-x)
        return -out


class HllPropagation(MessagePassing):
    def __init__(self):
        super().__init__(aggr='max')

    @torch.no_grad()
    def forward(self, x, edge_index):
        out = self.propagate(edge_index, x=x)
        return out


class ElphHashes(object):
    """
    class to store hashes and retrieve subgraph features
    """

    def __init__(self, args):
        assert args.max_hash_hops in {1, 2, 3}, f'hashing is not implemented for {args.max_hash_hops} hops'
        self.max_hops = args.max_hash_hops
        self.floor_sf = args.floor_sf  # if true set minimum sf to 0 (they're counts, so it should be)
        # minhash params
        self._mersenne_prime = np.uint64((1 << 61) - 1)
        self._max_minhash = np.uint64((1 << 32) - 1)
        self._minhash_range = (1 << 32)
        self.minhash_seed = 1
        self.num_perm = args.minhash_num_perm
        self.minhash_prop = MinhashPropagation()
        # hll params
        self.p = args.hll_p
        self.m = 1 << self.p  # the bitshift way of writing 2^p
        self.use_zero_one = args.use_zero_one
        self.label_lookup = LABEL_LOOKUP[self.max_hops]
        tmp = HyperLogLogPlusPlus(p=self.p)
        # store values that are shared and only depend on p
        self.hll_hashfunc = tmp.hashfunc
        self.alpha = tmp.alpha
        # the rank is the number of leading zeros. The max rank is the number of bits used in the hashes (64) minus p
        # as p bits are used to index the different counters
        self.max_rank = tmp.max_rank
        assert self.max_rank == 64 - self.p, 'not using 64 bits for hll++ hashing'
        self.hll_size = len(tmp.reg)
        self.hll_threshold = hyperloglog_const._thresholds[self.p - 4]
        self.bias_vector = torch.tensor(hyperloglog_const._bias[self.p - 4], dtype=torch.float)
        self.estimate_vector = torch.tensor(hyperloglog_const._raw_estimate[self.p - 4], dtype=torch.float)
        self.hll_prop = HllPropagation()

    def _np_bit_length(self, bits):
        """
        Get the number of bits required to represent each int in bits array
        @param bits: numpy [n_edges] array of ints
        @return:
        """
        return np.ceil(np.log2(bits + 1)).astype(int)

    def _get_hll_rank(self, bits):
        """
        get the number of leading zeros when each int in bits is represented as a self.max_rank-p bit array
        @param bits: a numpy array of ints
        @return:
        """
        # get the number of bits needed to represent each integer in bits
        bit_length = self._np_bit_length(bits)
        # the rank is the number of leading zeros, no idea about the +1 though
        rank = self.max_rank - bit_length + 1
        if min(rank) <= 0:
            raise ValueError("Hash value overflow, maximum size is %d\
                        bits" % self.max_rank)
        return rank

    def _init_permutations(self, num_perm):
        # Create parameters for a random bijective permutation function
        # that maps a 32-bit hash value to another 32-bit hash value.
        # http://en.wikipedia.org/wiki/Universal_hashing
        gen = np.random.RandomState(self.minhash_seed)
        return np.array([
            (gen.randint(1, self._mersenne_prime, dtype=np.uint64),
             gen.randint(0, self._mersenne_prime, dtype=np.uint64)) for
            _ in
            range(num_perm)
        ], dtype=np.uint64).T

    def initialise_minhash(self, n_nodes):
        init_hv = np.ones((n_nodes, self.num_perm), dtype=np.int64) * self._max_minhash
        a, b = self._init_permutations(self.num_perm)
        hv = hash_array(np.arange(1, n_nodes + 1))
        phv = np.bitwise_and((a * np.expand_dims(hv, 1) + b) % self._mersenne_prime, self._max_minhash)
        hv = np.minimum(phv, init_hv)
        return torch.tensor(hv, dtype=torch.int64)  # this conversion should be ok as self._max_minhash < max(int64)

    def initialise_hll(self, n_nodes):
        regs = np.zeros((n_nodes, self.m), dtype=np.int8)  # the registers to store binary values
        hv = hash_array(np.arange(1, n_nodes + 1))  # this function hashes 0 -> 0, so avoid
        # Get the index of the register using the first p bits of the hash
        # e.g. p=2, m=2^2=4, m-1=3=011 => only keep the right p bits
        reg_index = hv & (self.m - 1)
        # Get the rest of the hash. Python right shift drops the rightmost bits
        bits = hv >> self.p
        # Update the register
        ranks = self._get_hll_rank(bits)  # get the number of leading zeros in each element of bits
        regs[np.arange(n_nodes), reg_index] = np.maximum(regs[np.arange(n_nodes), reg_index], ranks)
        return torch.tensor(regs, dtype=torch.int8)  # int8 is fine as we're storing leading zeros in 64 bit numbers

    def build_hash_tables(self, num_nodes, edge_index):
        """
        Generate a hashing table that allows the size of the intersection of two nodes k-hop neighbours to be
        estimated in constant time
        @param num_nodes: The number of nodes in the graph
        @param adj: Int Tensor [2, edges] edges in the graph
        @return: hashes, cards. Hashes is a dictionary{dictionary}{tensor} with keys num_hops, 'hll' or 'minhash', cards
        is a tensor[n_nodes, max_hops-1]
        """
        hash_edge_index, _ = add_self_loops(edge_index)
        cards = torch.zeros((num_nodes, self.max_hops))
        node_hashings_table = {}
        for k in range(self.max_hops + 1):
            logger.info(f"Calculating hop {k} hashes")
            node_hashings_table[k] = {'hll': torch.zeros((num_nodes, self.hll_size), dtype=torch.int8),
                                      'minhash': torch.zeros((num_nodes, self.num_perm), dtype=torch.int64)}
            start = time()
            if k == 0:
                node_hashings_table[k]['minhash'] = self.initialise_minhash(num_nodes)
                node_hashings_table[k]['hll'] = self.initialise_hll(num_nodes)
            else:
                node_hashings_table[k]['hll'] = self.hll_prop(node_hashings_table[k - 1]['hll'], hash_edge_index)
                node_hashings_table[k]['minhash'] = self.minhash_prop(node_hashings_table[k - 1]['minhash'],
                                                                      hash_edge_index)
                cards[:, k - 1] = self.hll_count(node_hashings_table[k]['hll'])
            logger.info(f'{k} hop hash generation ran in {time() - start} s')
        return node_hashings_table, cards

    def _get_intersections(self, edge_list, hash_table):
        """
        extract set intersections as jaccard * union
        @param edge_list: [n_edges, 2] tensor to get intersections for
        @param hash_table:
        @param max_hops:
        @param p: hll precision parameter. hll uses 6 * 2^p bits
        @return:
        """
        intersections = {}
        # create features for each combination of hops.
        for k1 in range(1, self.max_hops + 1):
            for k2 in range(1, self.max_hops + 1):
                src_hll = hash_table[k1]['hll'][edge_list[:, 0]]
                src_minhash = hash_table[k1]['minhash'][edge_list[:, 0]]
                dst_hll = hash_table[k2]['hll'][edge_list[:, 1]]
                dst_minhash = hash_table[k2]['minhash'][edge_list[:, 1]]
                jaccard = self.jaccard(src_minhash, dst_minhash)
                unions = self._hll_merge(src_hll, dst_hll)
                union_size = self.hll_count(unions)
                intersection = jaccard * union_size
                intersections[(k1, k2)] = intersection
        return intersections

    def get_hashval(self, x):
        return x.hashvals

    def _linearcounting(self, num_zero):
        return self.m * torch.log(self.m / num_zero)

    def _estimate_bias(self, e):
        """
        Not exactly sure what this is doing or why exactly 6 nearest neighbours are used.
        @param e: torch tensor [n_links] of estimates
        @return:
        """
        nearest_neighbors = torch.argsort((e.unsqueeze(-1) - self.estimate_vector.to(e.device)) ** 2)[:, :6]
        return torch.mean(self.bias_vector.to(e.device)[nearest_neighbors], dim=1)

    def _refine_hll_count_estimate(self, estimate):
        idx = estimate <= 5 * self.m
        estimate_bias = self._estimate_bias(estimate)
        estimate[idx] = estimate[idx] - estimate_bias[idx]
        return estimate

    def hll_count(self, regs):
        """
        Estimate the size of set unions associated with regs
        @param regs: A tensor of registers [n_nodes, register_size]
        @return:
        """
        if regs.dim() == 1:
            regs = regs.unsqueeze(dim=0)
        retval = torch.ones(regs.shape[0], device=regs.device) * self.hll_threshold + 1
        num_zero = self.m - torch.count_nonzero(regs, dim=1)
        idx = num_zero > 0
        lc = self._linearcounting(num_zero[idx])
        retval[idx] = lc
        # we only keep lc values where lc <= self.hll_threshold, otherwise
        estimate_indices = retval > self.hll_threshold
        # Use HyperLogLog estimation function
        e = (self.alpha * self.m ** 2) / torch.sum(2.0 ** (-regs[estimate_indices]), dim=1)
        # for some reason there are two more cases
        e = self._refine_hll_count_estimate(e)
        retval[estimate_indices] = e
        return retval

    def _hll_merge(self, src, dst):
        if src.shape != dst.shape:
            raise ValueError('source and destination register shapes must be the same')
        return torch.maximum(src, dst)

    def hll_neighbour_merge(self, root, neighbours):
        all_regs = torch.cat([root.unsqueeze(dim=0), neighbours], dim=0)
        return torch.max(all_regs, dim=0)[0]

    def minhash_neighbour_merge(self, root, neighbours):
        all_regs = torch.cat([root.unsqueeze(dim=0), neighbours], dim=0)
        return torch.min(all_regs, dim=0)[0]

    def jaccard(self, src, dst):
        """
        get the minhash Jaccard estimate
        @param src: tensor [n_edges, num_perms] of hashvalues
        @param dst: tensor [n_edges, num_perms] of hashvalues
        @return: tensor [n_edges] jaccard estimates
        """
        if src.shape != dst.shape:
            raise ValueError('source and destination hash value shapes must be the same')
        return torch.count_nonzero(src == dst, dim=-1) / self.num_perm

    def get_subgraph_features(self, links, hash_table, cards, batch_size=11000000):
        """
        extracts the features that play a similar role to the labeling trick features. These can be thought of as approximations
        of path distances from the source and destination nodes. There are k+2+\sum_1^k 2k features
        @param links: tensor [n_edges, 2]
        @param hash_table: A Dict{Dict} of torch tensor [num_nodes, hash_size] keys are hop index and hash type (hyperlogloghash, minhash)
        @param cards: Tensor[n_nodes, max_hops] of hll neighbourhood cardinality estimates
        @param batch_size: batch size for computing intersections. 11m splits the large ogb datasets into 3.
        @return: Tensor[n_edges, max_hops(max_hops+2)]
        """
        if links.dim() == 1:
            links = links.unsqueeze(0)
        link_loader = DataLoader(range(links.size(0)), batch_size, shuffle=False, num_workers=0)
        all_features = []
        for batch in tqdm(link_loader):
            intersections = self._get_intersections(links[batch], hash_table)
            cards1, cards2 = cards.to(links.device)[links[batch, 0]], cards.to(links.device)[links[batch, 1]]
            features = torch.zeros((len(batch), self.max_hops * (self.max_hops + 2)), dtype=float, device=links.device)
            features[:, 0] = intersections[(1, 1)]
            if self.max_hops == 1:
                features[:, 1] = cards2[:, 0] - features[:, 0]
                features[:, 2] = cards1[:, 0] - features[:, 0]
            elif self.max_hops == 2:
                features[:, 1] = intersections[(2, 1)] - features[:, 0]  # (2,1)
                features[:, 2] = intersections[(1, 2)] - features[:, 0]  # (1,2)
                features[:, 3] = intersections[(2, 2)] - features[:, 0] - features[:, 1] - features[:, 2]  # (2,2)
                features[:, 4] = cards2[:, 0] - torch.sum(features[:, 0:2], dim=1)  # (0, 1)
                features[:, 5] = cards1[:, 0] - features[:, 0] - features[:, 2]  # (1, 0)
                features[:, 6] = cards2[:, 1] - torch.sum(features[:, 0:5], dim=1)  # (0, 2)
                features[:, 7] = cards1[:, 1] - features[:, 0] - torch.sum(features[:, 0:4], dim=1) - features[:,
                                                                                                      5]  # (2, 0)
            elif self.max_hops == 3:
                features[:, 1] = intersections[(2, 1)] - features[:, 0]  # (2,1)
                features[:, 2] = intersections[(1, 2)] - features[:, 0]  # (1,2)
                features[:, 3] = intersections[(2, 2)] - features[:, 0] - features[:, 1] - features[:, 2]  # (2,2)
                features[:, 4] = intersections[(3, 1)] - features[:, 0] - features[:, 1]  # (3,1)
                features[:, 5] = intersections[(1, 3)] - features[:, 0] - features[:, 2]  # (1, 3)
                features[:, 6] = intersections[(3, 2)] - torch.sum(features[:, 0:4], dim=1) - features[:, 4]  # (3,2)
                features[:, 7] = intersections[(2, 3)] - torch.sum(features[:, 0:4], dim=1) - features[:, 5]  # (2,3)
                features[:, 8] = intersections[(3, 3)] - torch.sum(features[:, 0:8], dim=1)  # (3,3)
                features[:, 9] = cards2[:, 0] - features[:, 0] - features[:, 1] - features[:, 4]  # (0, 1)
                features[:, 10] = cards1[:, 0] - features[:, 0] - features[:, 2] - features[:, 5]  # (1, 0)
                features[:, 11] = cards2[:, 1] - torch.sum(features[:, 0:5], dim=1) - features[:, 6] - features[:,
                                                                                                       9]  # (0, 2)
                features[:, 12] = cards1[:, 1] - torch.sum(features[:, 0:5], dim=1) - features[:, 7] - features[:,
                                                                                                       10]  # (2, 0)
                features[:, 13] = cards2[:, 2] - torch.sum(features[:, 0:9], dim=1) - features[:, 9] - features[:,
                                                                                                       11]  # (0, 3)
                features[:, 14] = cards1[:, 2] - torch.sum(features[:, 0:9], dim=1) - features[:, 10] - features[:,
                                                                                                        12]  # (3, 0)
            else:
                raise NotImplementedError("Only 1, 2 and 3 hop hashes are implemented")
            if not self.use_zero_one:
                if self.max_hops == 2:  # for two hops any positive edge that's dist 1 from u must be dist 2 from v etc.
                    features[:, 4] = 0
                    features[:, 5] = 0
                elif self.max_hops == 3:  # in addition for three hops 0,2 is impossible for positive edges
                    features[:, 4] = 0
                    features[:, 5] = 0
                    features[:, 11] = 0
                    features[:, 12] = 0
            if self.floor_sf:  # should be more accurate, but in practice makes no difference
                features[features < 0] = 0
            all_features.append(features)
        features = torch.cat(all_features, dim=0)
        return features

</file>

<file path="lp/Buddy/src/heuristics.py">
"""
A selection of heuristic methods (Personalized PageRank, Adamic Adar and Common Neighbours) for link prediction
"""

import numpy as np
from tqdm import tqdm
import torch
from torch_geometric.loader import DataLoader


def CN(A, edge_index, batch_size=100000):
    """
    Common neighbours
    :param A: scipy sparse adjacency matrix
    :param edge_index: pyg edge_index
    :param batch_size: int
    :return: FloatTensor [edges] of scores, pyg edge_index
    """
    link_loader = DataLoader(range(edge_index.size(0)), batch_size)
    scores = []
    for ind in tqdm(link_loader):
        src, dst = edge_index[ind, 0], edge_index[ind, 1]
        cur_scores = np.array(np.sum(A[src].multiply(A[dst]), 1)).flatten()
        scores.append(cur_scores)
    scores = np.concatenate(scores, 0)
    print(f'evaluated Common Neighbours for {len(scores)} edges')
    return torch.FloatTensor(scores), edge_index


def AA(A, edge_index, batch_size=100000):
    """
    Adamic Adar
    :param A: scipy sparse adjacency matrix
    :param edge_index: pyg edge_index
    :param batch_size: int
    :return: FloatTensor [edges] of scores, pyg edge_index
    """
    multiplier = 1 / np.log(A.sum(axis=0))
    multiplier[np.isinf(multiplier)] = 0
    A_ = A.multiply(multiplier).tocsr()
    link_loader = DataLoader(range(edge_index.size(0)), batch_size)
    scores = []
    for ind in tqdm(link_loader):
        src, dst = edge_index[ind, 0], edge_index[ind, 1]
        cur_scores = np.array(np.sum(A[src].multiply(A_[dst]), 1)).flatten()
        scores.append(cur_scores)
    scores = np.concatenate(scores, 0)
    print(f'evaluated Adamic Adar for {len(scores)} edges')
    return torch.FloatTensor(scores), edge_index


def RA(A, edge_index, batch_size=100000):
    """
    Resource Allocation https://arxiv.org/pdf/0901.0553.pdf
    :param A: scipy sparse adjacency matrix
    :param edge_index: pyg edge_index
    :param batch_size: int
    :return: FloatTensor [edges] of scores, pyg edge_index
    """
    multiplier = 1 / A.sum(axis=0)
    multiplier[np.isinf(multiplier)] = 0
    A_ = A.multiply(multiplier).tocsr()
    link_loader = DataLoader(range(edge_index.size(0)), batch_size)
    scores = []
    for ind in tqdm(link_loader):
        src, dst = edge_index[ind, 0], edge_index[ind, 1]
        cur_scores = np.array(np.sum(A[src].multiply(A_[dst]), 1)).flatten()
        scores.append(cur_scores)
    scores = np.concatenate(scores, 0)
    print(f'evaluated Resource Allocation for {len(scores)} edges')
    return torch.FloatTensor(scores), edge_index


def PPR(A, edge_index):
    """
    The Personalized PageRank heuristic score.
    Need to install fast_pagerank by "pip install fast-pagerank"
    Too slow for large datasets now.
    :param A: A CSR matrix using the 'message passing' edges
    :param edge_index: The supervision edges to be scored
    :return:
    """
    from fast_pagerank import pagerank_power
    num_nodes = A.shape[0]
    src_index, sort_indices = torch.sort(edge_index[:, 0])
    dst_index = edge_index[sort_indices, 1]
    edge_reindex = torch.stack([src_index, dst_index])
    scores = []
    visited = set([])
    j = 0
    for i in tqdm(range(edge_reindex.shape[1])):
        if i < j:
            continue
        src = edge_reindex[0, i]
        personalize = np.zeros(num_nodes)
        personalize[src] = 1
        # get the ppr for the current source node
        ppr = pagerank_power(A, p=0.85, personalize=personalize, tol=1e-7)
        j = i
        # get ppr for all links that start at this source to save recalculating the ppr score
        while edge_reindex[0, j] == src:
            j += 1
            if j == edge_reindex.shape[1]:
                break
        all_dst = edge_reindex[1, i:j]
        cur_scores = ppr[all_dst]
        if cur_scores.ndim == 0:
            cur_scores = np.expand_dims(cur_scores, 0)
        scores.append(np.array(cur_scores))

    scores = np.concatenate(scores, 0)
    print(f'evaluated PPR for {len(scores)} edges')
    return torch.FloatTensor(scores), edge_reindex

</file>

<file path="lp/Buddy/src/labelling_tricks.py">
"""
labelling tricks as described in
https://proceedings.neurips.cc/paper/2021/hash/4be49c79f233b4f4070794825c323733-Abstract.html
"""

import torch
import numpy as np
from scipy.sparse.csgraph import shortest_path


def drnl_hash_function(dist2src, dist2dst):
    """
    mapping from source and destination distances to a single node label e.g. (1,1)->2, (1,2)->3
    @param dist2src: Int Tensor[edges] shortest graph distance to source node
    @param dist2dst: Int Tensor[edges] shortest graph distance to source node
    @return: Int Tensor[edges] of labels
    """
    dist = dist2src + dist2dst

    dist_over_2, dist_mod_2 = torch.div(dist, 2, rounding_mode='floor'), dist % 2

    z = 1 + torch.min(dist2src, dist2dst)
    z += dist_over_2 * (dist_over_2 + dist_mod_2 - 1)
    # the src and dst nodes always get a score of 1
    z[dist2src == 0] = 1
    z[dist2dst == 0] = 1
    return z


def get_drnl_lookup(max_dist, num_hops):
    """
    A lookup table from DRNL labels to index into a contiguous tensor. DRNL labels are not contiguous and this
    lookup table is used to index embedded labels
    """
    max_label = get_max_label('drnl', max_dist, num_hops)
    res_arr = [None] * (max_label + 1)
    res_arr[1] = (1, 0)
    for src in range(1, num_hops + 1):
        for dst in range(1, max_dist + 1):
            label = drnl_hash_function(torch.tensor([src]), torch.tensor([dst]))
            res_arr[label] = (src, dst)
    z_to_idx = {}
    idx_to_dst = {}
    counter = 0
    for idx, elem in enumerate(res_arr):
        if elem is not None:
            z_to_idx[idx] = counter
            idx_to_dst[counter] = (elem)
            counter += 1
    return z_to_idx, idx_to_dst


def get_max_label(method, max_dist, num_hops):
    if method in {'de', 'de+'}:
        max_label = max_dist
    elif method in {'drnl-', 'drnl'}:
        max_label = drnl_hash_function(torch.tensor([num_hops]), torch.tensor([max_dist])).item()
    else:
        raise NotImplementedError
    return max_label


def drnl_node_labeling(adj, src, dst, max_dist=100):
    """
    The heuristic proposed in "Link prediction based on graph neural networks". It is an integer value giving the 'distance'
    to the (src,dst) edge such that src = dst = 1, neighours of dst,src = 2 etc. It implements
    z = 1 + min(d_x, d_y) + (d//2)[d//2 + d%2 - 1] where d = d_x + d_y
    z is treated as a node label downstream. Even though the labels measures of distance from the central edge, they are treated as
    categorical objects and embedded in an embedding table of size max_z * hidden_dim
    @param adj:
    @param src:
    @param dst:
    @return:
    """
    src, dst = (dst, src) if src > dst else (src, dst)

    idx = list(range(src)) + list(range(src + 1, adj.shape[0]))
    adj_wo_src = adj[idx, :][:, idx]

    idx = list(range(dst)) + list(range(dst + 1, adj.shape[0]))
    adj_wo_dst = adj[idx, :][:, idx]

    dist2src = shortest_path(adj_wo_dst, directed=False, unweighted=True, indices=src)
    dist2src = np.insert(dist2src, dst, 0, axis=0)
    dist2src = torch.from_numpy(dist2src)
    dist2src[dist2src > max_dist] = max_dist

    dist2dst = shortest_path(adj_wo_src, directed=False, unweighted=True, indices=dst - 1)
    dist2dst = np.insert(dist2dst, src, 0, axis=0)
    dist2dst = torch.from_numpy(dist2dst)
    dist2dst[dist2dst > max_dist] = max_dist

    z = drnl_hash_function(dist2src, dist2dst)
    return z.to(torch.long)


def de_node_labeling(adj, src, dst, max_dist=3):
    # Distance Encoding. See "Li et. al., Distance Encoding: Design Provably More
    # Powerful Neural Networks for Graph Representation Learning."
    src, dst = (dst, src) if src > dst else (src, dst)

    dist = shortest_path(adj, directed=False, unweighted=True, indices=[src, dst])
    dist = torch.from_numpy(dist)

    dist[dist > max_dist] = max_dist
    dist[torch.isnan(dist)] = max_dist + 1

    return dist.to(torch.long).t()


def de_plus_node_labeling(adj, src, dst, max_dist=100):
    # Distance Encoding Plus. When computing distance to src, temporarily mask dst;
    # when computing distance to dst, temporarily mask src. Essentially the same as DRNL.
    src, dst = (dst, src) if src > dst else (src, dst)

    idx = list(range(src)) + list(range(src + 1, adj.shape[0]))
    adj_wo_src = adj[idx, :][:, idx]

    idx = list(range(dst)) + list(range(dst + 1, adj.shape[0]))
    adj_wo_dst = adj[idx, :][:, idx]

    dist2src = shortest_path(adj_wo_dst, directed=False, unweighted=True, indices=src)
    dist2src = np.insert(dist2src, dst, 1, axis=0)
    dist2src = torch.from_numpy(dist2src)

    dist2dst = shortest_path(adj_wo_src, directed=False, unweighted=True, indices=dst - 1)
    dist2dst = np.insert(dist2dst, src, 1, axis=0)
    dist2dst = torch.from_numpy(dist2dst)

    dist = torch.cat([dist2src.view(-1, 1), dist2dst.view(-1, 1)], 1)
    dist[dist > max_dist] = max_dist
    dist[torch.isnan(dist)] = max_dist

    return dist.to(torch.long)
</file>

<file path="lp/Buddy/src/lcc.py">
"""
utils for getting the largest connected component of a graph
"""
import numpy as np
from torch_geometric.data import Data, InMemoryDataset

def get_largest_connected_component(dataset: InMemoryDataset) -> np.ndarray:
  remaining_nodes = set(range(dataset.data.x.shape[0]))
  comps = []
  while remaining_nodes:
    start = min(remaining_nodes)
    comp = get_component(dataset, start)
    comps.append(comp)
    remaining_nodes = remaining_nodes.difference(comp)
  return np.array(list(comps[np.argmax(list(map(len, comps)))]))


def get_node_mapper(lcc: np.ndarray) -> dict:
  mapper = {}
  counter = 0
  for node in lcc:
    mapper[node] = counter
    counter += 1
  return mapper


def remap_edges(edges: list, mapper: dict) -> list:
  row = [e[0] for e in edges]
  col = [e[1] for e in edges]
  row = list(map(lambda x: mapper[x], row))
  col = list(map(lambda x: mapper[x], col))
  return [row, col]

def get_component(dataset: InMemoryDataset, start: int = 0) -> set:
  visited_nodes = set()
  queued_nodes = set([start])
  row, col = dataset.data.edge_index.numpy()
  while queued_nodes:
    current_node = queued_nodes.pop()
    visited_nodes.update([current_node])
    neighbors = col[np.where(row == current_node)[0]]
    neighbors = [n for n in neighbors if n not in visited_nodes and n not in queued_nodes]
    queued_nodes.update(neighbors)
  return visited_nodes
</file>

<file path="lp/Buddy/src/models/__init__.py">

</file>

<file path="lp/Buddy/src/models/elph.py">
"""
The ELPH model
"""

from time import time
import logging

import torch
import torch.nn.functional as F
from torch.nn import Linear
from torch_geometric.nn import GCNConv
from torch_geometric.nn.conv.gcn_conv import gcn_norm
from torch_geometric.utils import add_self_loops

from src.models.gnn import SIGN, SIGNEmbedding
from src.hashing import ElphHashes

logger = logging.getLogger(__name__)
logger.setLevel(logging.WARN)


class LinkPredictor(torch.nn.Module):
    def __init__(self, args, use_embedding=False):
        super(LinkPredictor, self).__init__()
        self.use_embedding = use_embedding
        self.use_feature = args.use_feature
        self.feature_dropout = args.feature_dropout
        self.label_dropout = args.label_dropout
        self.dim = args.max_hash_hops * (args.max_hash_hops + 2)
        self.label_lin_layer = Linear(self.dim, self.dim)
        if args.use_feature:
            self.bn_feats = torch.nn.BatchNorm1d(args.hidden_channels)
        if self.use_embedding:
            self.bn_embs = torch.nn.BatchNorm1d(args.hidden_channels)
        self.bn_labels = torch.nn.BatchNorm1d(self.dim)
        if args.use_feature:
            self.lin_feat = Linear(args.hidden_channels,
                                   args.hidden_channels)
            self.lin_out = Linear(args.hidden_channels, args.hidden_channels)
        out_channels = self.dim + args.hidden_channels if self.use_feature else self.dim
        if self.use_embedding:
            self.lin_emb = Linear(args.hidden_channels,
                                  args.hidden_channels)
            self.lin_emb_out = Linear(args.hidden_channels, args.hidden_channels)
            out_channels += args.hidden_channels
        self.lin = Linear(out_channels, 1)

    def feature_forward(self, x):
        """
        small neural network applied edgewise to hadamard product of node features
        @param x: node features torch tensor [batch_size, 2, hidden_dim]
        @return: torch tensor [batch_size, hidden_dim]
        """
        x = x[:, 0, :] * x[:, 1, :]
        # mlp at the end
        x = self.lin_out(x)
        x = self.bn_feats(x)
        x = F.relu(x)
        x = F.dropout(x, p=self.feature_dropout, training=self.training)
        return x

    def embedding_forward(self, x):
        x = self.lin_emb(x)
        x = x[:, 0, :] * x[:, 1, :]
        # mlp at the end
        x = self.lin_emb_out(x)
        x = self.bn_embs(x)
        x = F.relu(x)
        x = F.dropout(x, p=self.feature_dropout, training=self.training)

        return x

    def forward(self, sf, node_features, emb=None):
        sf = self.label_lin_layer(sf)
        sf = self.bn_labels(sf)
        sf = F.relu(sf)
        x = F.dropout(sf, p=self.label_dropout, training=self.training)
        # process node features
        if self.use_feature:
            node_features = self.feature_forward(node_features)
            x = torch.cat([x, node_features.to(torch.float)], 1)
        if emb is not None:
            node_embedding = self.embedding_forward(emb)
            x = torch.cat([x, node_embedding.to(torch.float)], 1)
        x = self.lin(x)
        return x

    def print_params(self):
        print(f'model bias: {self.lin.bias.item():.3f}')
        print('model weights')
        for idx, elem in enumerate(self.lin.weight.squeeze()):
            if idx < self.dim:
                print(f'{self.idx_to_dst[idx % self.emb_dim]}: {elem.item():.3f}')
            else:
                print(f'feature {idx - self.dim}: {elem.item():.3f}')


class ELPH(torch.nn.Module):
    """
    propagating hashes, features and degrees with message passing
    """

    def __init__(self, args, num_features, node_embedding=None):
        super(ELPH, self).__init__()
        # hashing things
        self.elph_hashes = ElphHashes(args)
        self.init_hashes = None
        self.init_hll = None
        self.num_perm = args.minhash_num_perm
        self.hll_size = 2 ^ args.hll_p
        # gnn things
        self.use_feature = args.use_feature
        self.feature_prop = args.feature_prop  # None, residual, cat
        self.node_embedding = node_embedding
        self.propagate_embeddings = args.propagate_embeddings
        self.sign_k = args.sign_k
        self.label_dropout = args.label_dropout
        self.feature_dropout = args.feature_dropout
        self.num_layers = args.max_hash_hops
        self.dim = args.max_hash_hops * (args.max_hash_hops + 2)
        # construct the nodewise NN components
        self._convolution_builder(num_features, args.hidden_channels,
                                  args)  # build the convolutions for features and embs
        # construct the edgewise NN components
        self.predictor = LinkPredictor(args, node_embedding is not None)
        if self.sign_k != 0 and self.propagate_embeddings:
            # only used for the ddi where nodes have no features and transductive node embeddings are needed
            self.sign_embedding = SIGNEmbedding(args.hidden_channels, args.hidden_channels, args.hidden_channels,
                                                args.sign_k, args.sign_dropout)

    def _convolution_builder(self, num_features, hidden_channels, args):
        self.convs = torch.nn.ModuleList()
        if args.feature_prop in {'residual', 'cat'}:  # use a linear encoder
            self.feature_encoder = Linear(num_features, hidden_channels)
            self.convs.append(
                GCNConv(hidden_channels, hidden_channels))
        else:
            self.convs.append(
                GCNConv(num_features, hidden_channels))
        for _ in range(self.num_layers - 1):
            self.convs.append(
                GCNConv(hidden_channels, hidden_channels))
        if self.node_embedding is not None:
            self.emb_convs = torch.nn.ModuleList()
            for _ in range(self.num_layers):  # assuming the embedding has hidden_channels dims
                self.emb_convs.append(GCNConv(hidden_channels, hidden_channels))

    def propagate_embeddings_func(self, edge_index):
        num_nodes = self.node_embedding.num_embeddings
        gcn_edge_index, _ = gcn_norm(edge_index, num_nodes=num_nodes)
        return self.sign_embedding(self.node_embedding.weight, gcn_edge_index, num_nodes)

    def feature_conv(self, x, edge_index, k):
        if not self.use_feature:
            return None
        out = self.convs[k - 1](x, edge_index)
        out = F.dropout(out, p=self.feature_dropout, training=self.training)
        if self.feature_prop == 'residual':
            out = x + out
        return out

    def embedding_conv(self, x, edge_index, k):
        if x is None:
            return x
        out = self.emb_convs[k - 1](x, edge_index)
        out = F.dropout(out, p=self.feature_dropout, training=self.training)
        if self.feature_prop == 'residual':
            out = x + out
        return out

    def _encode_features(self, x):
        if self.use_feature:
            x = self.feature_encoder(x)
            x = F.dropout(x, p=self.feature_dropout, training=self.training)
        else:
            x = None

        return x

    def forward(self, x, edge_index):
        """
        @param x: raw node features tensor [n_nodes, n_features]
        @param adj_t: edge index tensor [2, num_links]
        @return:
        """
        hash_edge_index, _ = add_self_loops(edge_index)  # unnormalised, but with self-loops
        # if this is the first call then initialise the minhashes and hlls - these need to be the same for every model call
        num_nodes, num_features = x.shape
        if self.init_hashes == None:
            self.init_hashes = self.elph_hashes.initialise_minhash(num_nodes).to(x.device)
        if self.init_hll == None:
            self.init_hll = self.elph_hashes.initialise_hll(num_nodes).to(x.device)
        # initialise data tensors for storing k-hop hashes
        cards = torch.zeros((num_nodes, self.num_layers))
        node_hashings_table = {}
        for k in range(self.num_layers + 1):
            logger.info(f"Calculating hop {k} hashes")
            node_hashings_table[k] = {
                'hll': torch.zeros((num_nodes, self.hll_size), dtype=torch.int8, device=edge_index.device),
                'minhash': torch.zeros((num_nodes, self.num_perm), dtype=torch.int64, device=edge_index.device)}
            start = time()
            if k == 0:
                node_hashings_table[k]['minhash'] = self.init_hashes
                node_hashings_table[k]['hll'] = self.init_hll
                if self.feature_prop in {'residual', 'cat'}:  # need to get features to the hidden dim
                    x = self._encode_features(x)

            else:
                node_hashings_table[k]['hll'] = self.elph_hashes.hll_prop(node_hashings_table[k - 1]['hll'],
                                                                          hash_edge_index)
                node_hashings_table[k]['minhash'] = self.elph_hashes.minhash_prop(node_hashings_table[k - 1]['minhash'],
                                                                                  hash_edge_index)
                cards[:, k - 1] = self.elph_hashes.hll_count(node_hashings_table[k]['hll'])
                x = self.feature_conv(x, edge_index, k)

            logger.info(f'{k} hop hash generation ran in {time() - start} s')

        return x, node_hashings_table, cards


class BUDDY(torch.nn.Module):
    """
    Scalable version of ElPH that uses precomputation of subgraph features and SIGN style propagation
    of node features
    """

    def __init__(self, args, num_features=None, node_embedding=None):
        super(BUDDY, self).__init__()

        self.use_feature = args.use_feature
        self.label_dropout = args.label_dropout
        self.feature_dropout = args.feature_dropout
        self.node_embedding = node_embedding
        self.propagate_embeddings = args.propagate_embeddings
        # using both unormalised and degree normalised counts as features, hence * 2
        self.append_normalised = args.add_normed_features
        ra_counter = 1 if args.use_RA else 0
        num_labelling_features = args.max_hash_hops * (args.max_hash_hops + 2)
        self.dim = num_labelling_features * 2 if self.append_normalised else num_labelling_features
        self.use_RA = args.use_RA
        self.sign_k = args.sign_k
        if self.sign_k != 0:
            if self.propagate_embeddings:
                # this is only used for the ddi dataset where nodes have no features and transductive node embeddings are needed
                self.sign_embedding = SIGNEmbedding(args.hidden_channels, args.hidden_channels, args.hidden_channels,
                                                    args.sign_k, args.sign_dropout)
            else:
                self.sign = SIGN(num_features, args.hidden_channels, args.hidden_channels, args.sign_k,
                                 args.sign_dropout)
        self.label_lin_layer = Linear(self.dim, self.dim)
        if args.use_feature:
            self.bn_feats = torch.nn.BatchNorm1d(args.hidden_channels)
        if self.node_embedding is not None:
            self.bn_embs = torch.nn.BatchNorm1d(args.hidden_channels)
        self.bn_labels = torch.nn.BatchNorm1d(self.dim)
        self.bn_RA = torch.nn.BatchNorm1d(1)

        if args.use_feature:
            self.lin_feat = Linear(num_features,
                                   args.hidden_channels)
            self.lin_out = Linear(args.hidden_channels, args.hidden_channels)
        hidden_channels = self.dim + args.hidden_channels if self.use_feature else self.dim
        if self.node_embedding is not None:
            self.lin_emb = Linear(args.hidden_channels,
                                  args.hidden_channels)
            self.lin_emb_out = Linear(args.hidden_channels, args.hidden_channels)
            hidden_channels += self.node_embedding.embedding_dim

        self.lin = Linear(hidden_channels + ra_counter, 1)

    def propagate_embeddings_func(self, edge_index):
        num_nodes = self.node_embedding.num_embeddings
        gcn_edge_index, _ = gcn_norm(edge_index, num_nodes=num_nodes)
        return self.sign_embedding(self.node_embedding.weight, gcn_edge_index, num_nodes)

    def _append_degree_normalised(self, x, src_degree, dst_degree):
        """
        Create a set of features that have the spirit of a cosine similarity x.y / ||x||.||y||. Some nodes (particularly negative samples)
        have zero degree
        because part of the graph is held back as train / val supervision edges and so divide by zero needs to be handled.
        Note that we always divide by the src and dst node's degrees for every node in ¬the subgraph
        @param x: unormalised features - equivalent to x.y
        @param src_degree: equivalent to sum_i x_i^2 as x_i in (0,1)
        @param dst_degree: equivalent to sum_i y_i^2 as y_i in (0,1)
        @return:
        """
        # this doesn't quite work with edge weights as instead of degree, the sum_i w_i^2 is needed,
        # but it probably doesn't matter
        normaliser = torch.sqrt(src_degree * dst_degree)
        normed_x = torch.divide(x, normaliser.unsqueeze(dim=1))
        normed_x[torch.isnan(normed_x)] = 0
        normed_x[torch.isinf(normed_x)] = 0
        return torch.cat([x, normed_x], dim=1)

    def feature_forward(self, x):
        """
        small neural network applied edgewise to hadamard product of node features
        @param x: node features torch tensor [batch_size, 2, hidden_dim]
        @return: torch tensor [batch_size, hidden_dim]
        """
        if self.sign_k != 0:
            x = self.sign(x)
        else:
            x = self.lin_feat(x)
        x = x[:, 0, :] * x[:, 1, :]
        # mlp at the end
        x = self.lin_out(x)
        x = self.bn_feats(x)
        x = F.relu(x)
        x = F.dropout(x, p=self.feature_dropout, training=self.training)
        return x

    def embedding_forward(self, x):
        x = self.lin_emb(x)
        x = x[:, 0, :] * x[:, 1, :]
        # mlp at the end
        x = self.lin_emb_out(x)
        x = self.bn_embs(x)
        x = F.relu(x)
        x = F.dropout(x, p=self.feature_dropout, training=self.training)

        return x

    def forward(self, sf, node_features, src_degree=None, dst_degree=None, RA=None, emb=None):
        """
        forward pass for one batch of edges
        @param sf: subgraph features [batch_size, num_hops*(num_hops+2)]
        @param node_features: raw node features [batch_size, 2, num_features]
        @param src_degree: degree of source nodes in batch
        @param dst_degree:
        @param RA:
        @param emb:
        @return:
        """
        if self.append_normalised:
            sf = self._append_degree_normalised(sf, src_degree, dst_degree)
        x = self.label_lin_layer(sf)
        x = self.bn_labels(x)
        x = F.relu(x)
        x = F.dropout(x, p=self.label_dropout, training=self.training)
        if self.use_feature:
            node_features = self.feature_forward(node_features)
            x = torch.cat([x, node_features.to(torch.float)], 1)
        if self.node_embedding is not None:
            node_embedding = self.embedding_forward(emb)
            x = torch.cat([x, node_embedding.to(torch.float)], 1)
        if self.use_RA:
            RA = RA.unsqueeze(-1)
            RA = self.bn_RA(RA)
            x = torch.cat([x, RA], 1)
        x = self.lin(x)
        return x

    def print_params(self):
        print(f'model bias: {self.lin.bias.item():.3f}')
        print('model weights')
        for idx, elem in enumerate(self.lin.weight.squeeze()):
            if idx < self.dim:
                print(f'{self.idx_to_dst[idx % self.emb_dim]}: {elem.item():.3f}')
            else:
                print(f'feature {idx - self.dim}: {elem.item():.3f}')

</file>

<file path="lp/Buddy/src/models/gnn.py">
"""
Baseline GNN models
"""

import torch
from torch import Tensor
import torch.nn.functional as F
from torch.nn import Linear, Parameter, BatchNorm1d as BN
from torch_sparse import SparseTensor
from torch_geometric.nn import GCNConv, SAGEConv
from torch_geometric.nn.dense.linear import Linear as pygLinear
from torch_geometric.nn.conv.gcn_conv import gcn_norm
from torch_geometric.typing import Adj, OptTensor
from torch_geometric.nn.inits import zeros
import torch_sparse


class GCN(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,
                 dropout):
        super(GCN, self).__init__()

        self.convs = torch.nn.ModuleList()
        self.convs.append(GCNConv(in_channels, hidden_channels, cached=True))
        for _ in range(num_layers - 2):
            self.convs.append(
                GCNConv(hidden_channels, hidden_channels, cached=True))
        self.convs.append(GCNConv(hidden_channels, out_channels, cached=True))

        self.dropout = dropout

    def reset_parameters(self):
        for conv in self.convs:
            conv.reset_parameters()

    def forward(self, x, adj_t):
        for conv in self.convs[:-1]:
            x = conv(x, adj_t)
            x = F.relu(x)
            x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.convs[-1](x, adj_t)
        return x


class GCNCustomConv(torch.nn.Module):
    """
    Class to propagate features
    """

    def __init__(self, in_channels, out_channels, bias=True, propagate_features=False, **kwargs):
        super().__init__(**kwargs)

        self.in_channels = in_channels
        self.out_channels = out_channels
        self._cached_edge_index = None
        self._cached_adj_t = None
        self.propagate_features = propagate_features

        self.lin = pygLinear(in_channels, out_channels, bias=False,
                             weight_initializer='glorot')

        if bias:
            self.bias = Parameter(torch.Tensor(out_channels))
        else:
            self.register_parameter('bias', None)

        self.reset_parameters()

    def forward(self, x: Tensor, edge_index: Adj,
                edge_weight: OptTensor = None) -> Tensor:

        # do the XW bit first
        x = self.lin(x)
        # propagate_type: (x: Tensor, edge_weight: OptTensor)
        edge_index, edge_weight = gcn_norm(  # yapf: disable
            edge_index, edge_weight, x.size(0))
        if self.propagate_features:
            out = torch_sparse.spmm(edge_index, edge_weight, x.shape[0], x.shape[0], x)
        else:
            out = x
        if self.bias is not None:
            out += self.bias
        return out

    def reset_parameters(self):
        self.lin.reset_parameters()
        zeros(self.bias)


class SAGE(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,
                 dropout, residual):
        super(SAGE, self).__init__()

        self.convs = torch.nn.ModuleList()
        self.convs.append(SAGEConv(in_channels, hidden_channels, root_weight=residual))
        for _ in range(num_layers - 2):
            self.convs.append(SAGEConv(hidden_channels, hidden_channels, root_weight=residual))
        self.convs.append(SAGEConv(hidden_channels, out_channels, root_weight=residual))

        self.dropout = dropout

    def reset_parameters(self):
        for conv in self.convs:
            conv.reset_parameters()

    def forward(self, x, adj_t):
        for conv in self.convs[:-1]:
            x = conv(x, adj_t)
            x = F.relu(x)
            x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.convs[-1](x, adj_t)
        return x


class SIGNBaseClass(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels, K, dropout):
        super(SIGNBaseClass, self).__init__()

        self.K = K
        self.lins = torch.nn.ModuleList()
        self.bns = torch.nn.ModuleList()
        for _ in range(self.K + 1):
            self.lins.append(Linear(in_channels, hidden_channels))
            self.bns.append(BN(hidden_channels))
        self.lin_out = Linear((K + 1) * hidden_channels, out_channels)
        self.dropout = dropout
        self.adj_t = None

    def reset_parameters(self):
        for lin, bn in zip(self.lins, self.bns):
            lin.reset_parameters()
            bn.reset_parameters()

    def cache_adj_t(self, edge_index, num_nodes):
        row, col = edge_index
        adj_t = SparseTensor(row=col, col=row,
                             sparse_sizes=(num_nodes, num_nodes))

        deg = adj_t.sum(dim=1).to(torch.float)
        deg_inv_sqrt = deg.pow(-0.5)
        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0
        return deg_inv_sqrt.view(-1, 1) * adj_t * deg_inv_sqrt.view(1, -1)

    def forward(self, *args):
        raise NotImplementedError


class SIGNEmbedding(SIGNBaseClass):
    def __init__(self, in_channels, hidden_channels, out_channels, K, dropout):
        super(SIGNEmbedding, self).__init__(in_channels, hidden_channels, out_channels, K, dropout)

    def forward(self, x, adj_t, num_nodes):
        if self.adj_t is None:
            self.adj_t = self.cache_adj_t(adj_t, num_nodes)
        hs = []
        for lin, bn in zip(self.lins, self.bns):
            h = lin(x)
            h = bn(h)
            h = F.relu(h)
            h = F.dropout(h, p=self.dropout, training=self.training)
            hs.append(h)
            x = self.adj_t @ x
        h = torch.cat(hs, dim=-1)
        x = self.lin_out(h)
        return x


class SIGN(SIGNBaseClass):
    def __init__(self, in_channels, hidden_channels, out_channels, K, dropout):
        super(SIGN, self).__init__(in_channels, hidden_channels, out_channels, K, dropout)

    def forward(self, xs):
        """
        apply the sign feature transform where each component of the polynomial A^n x is treated independently
        @param xs: [batch_size, 2, n_features * (K + 1)]
        @return: [batch_size, 2, hidden_dim]
        """
        xs = torch.tensor_split(xs, self.K + 1, dim=-1)
        hs = []
        # split features into k+1 chunks and put each tensor in a list
        for lin, bn, x in zip(self.lins, self.bns, xs):
            h = lin(x)
            # the next line is a fuggly way to apply the same batch norm to both source and destination edges
            h = torch.cat((bn(h[:, 0, :]).unsqueeze(1), bn(h[:, 1, :]).unsqueeze(1)), dim=1)
            h = F.relu(h)
            h = F.dropout(h, p=self.dropout, training=self.training)
            hs.append(h)
        h = torch.cat(hs, dim=-1)
        x = self.lin_out(h)
        return x


class LinkPredictor(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,
                 dropout):
        super(LinkPredictor, self).__init__()

        self.lins = torch.nn.ModuleList()
        self.lins.append(torch.nn.Linear(in_channels, hidden_channels))
        for _ in range(num_layers - 2):
            self.lins.append(torch.nn.Linear(hidden_channels, hidden_channels))
        self.lins.append(torch.nn.Linear(hidden_channels, out_channels))

        self.dropout = dropout

    def reset_parameters(self):
        for lin in self.lins:
            lin.reset_parameters()

    def forward(self, x_i, x_j):
        x = x_i * x_j
        for lin in self.lins[:-1]:
            x = lin(x)
            x = F.relu(x)
            x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.lins[-1](x)
        return torch.sigmoid(x)

</file>

<file path="lp/Buddy/src/models/seal.py">
"""
Code based on
https://github.com/facebookresearch/SEAL_OGB
# Copyright (c) Facebook, Inc. and its affiliates.
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.
"""

import math

import numpy as np
import torch
from torch.nn import (ModuleList, Linear, Conv1d, MaxPool1d, Embedding, ReLU,
                      Sequential, BatchNorm1d as BN)
import torch.nn.functional as F
from torch_geometric.nn import (GCNConv, SAGEConv, GINConv,
                                global_sort_pool, global_add_pool, global_mean_pool)

from src.labelling_tricks import get_max_label


class SEALMLP(torch.nn.Module):
    def __init__(self, args):
        super(SEALMLP, self).__init__()
        self.emb_dim = get_max_label(args.node_label, args.max_dist) + 1
        self.dropout = args.dropout
        self.lin1 = Linear(self.emb_dim, self.emb_dim)
        self.lin2 = Linear(self.emb_dim, 1)
        self.pooling = args.label_pooling

    def forward(self, z, dummy1, batch, dummy2, dummy3, dummy4, src_degree=None, dst_degree=None):
        x = F.one_hot(z, self.emb_dim).type(torch.FloatTensor).to(z.device)
        if self.pooling == 'add':
            x = global_add_pool(x, batch)
        else:
            x = global_mean_pool(x, batch)
        x = F.relu(self.lin1(x))
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.lin2(x)
        return x


class SEALGCN(torch.nn.Module):
    def __init__(self, hidden_channels, num_layers, max_z, num_features=None,
                 use_feature=False, node_embedding=None, dropout=0.5, pooling='edge'):
        super(SEALGCN, self).__init__()
        self.use_feature = use_feature
        self.node_embedding = node_embedding
        self.max_z = max_z
        self.z_embedding = Embedding(self.max_z, hidden_channels)
        self.pooling = pooling

        self.convs = ModuleList()
        initial_channels = hidden_channels
        if self.use_feature:
            initial_channels += num_features
        if self.node_embedding is not None:
            initial_channels += node_embedding.embedding_dim
        self.convs.append(GCNConv(initial_channels, hidden_channels))
        for _ in range(num_layers - 1):
            self.convs.append(GCNConv(hidden_channels, hidden_channels))

        self.dropout = dropout
        self.lin1 = Linear(hidden_channels, hidden_channels)
        self.lin2 = Linear(hidden_channels, 1)

    def reset_parameters(self):
        for conv in self.convs:
            conv.reset_parameters()

    def forward(self, z, edge_index, batch, x=None, edge_weight=None, node_id=None, src_degree=None, dst_degree=None):
        z_emb = self.z_embedding(z)
        if z_emb.ndim == 3:  # in case z has multiple integer labels
            # in the de paper they sum the one-hot representation too though
            z_emb = z_emb.sum(dim=1)
        if self.use_feature and x is not None:
            x = torch.cat([z_emb, x.to(torch.float)], 1)
        else:
            x = z_emb
        if self.node_embedding is not None and node_id is not None:
            n_emb = self.node_embedding(node_id)
            x = torch.cat([x, n_emb], 1)
        for conv in self.convs[:-1]:
            x = conv(x, edge_index, edge_weight)
            x = F.relu(x)
            x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.convs[-1](x, edge_index, edge_weight)
        if self.pooling == 'edge':  # center pooling
            # the batch is a map from node_idx -> batch e.g. [0,0,1,1,1] means nodes 0,1 are batch 0 and nodes 2,3,4
            # are batch 1. The next line gets the node index of the first node in each batch
            _, center_indices = np.unique(batch.cpu().numpy(), return_index=True)
            # the subgraphs are constructed so that index 0 in each batch is always src and index 1 is dst
            x_src = x[center_indices]
            x_dst = x[center_indices + 1]
            x = (x_src * x_dst)
            # mlp at the end
            x = F.relu(self.lin1(x))
            x = F.dropout(x, p=self.dropout, training=self.training)
            x = self.lin2(x)
        elif self.pooling == 'sum':  # sum pooling
            x = global_add_pool(x, batch)
            x = F.relu(self.lin1(x))
            x = F.dropout(x, p=self.dropout, training=self.training)
            x = self.lin2(x)
        elif self.pooling == 'mean':
            x = global_add_pool(x, batch)
            x = F.relu(self.lin1(x))
            x = F.dropout(x, p=self.dropout, training=self.training)
            x = self.lin2(x)
        else:
            raise NotImplementedError(f'invalid pooling: {self.pooling}. Options are "edge","sum","mean"')

        return x


class SEALSAGE(torch.nn.Module):
    def __init__(self, hidden_channels, num_layers, max_z, num_features=None,
                 use_feature=False, node_embedding=None, dropout=0.5):
        super(SEALSAGE, self).__init__()
        self.use_feature = use_feature
        self.node_embedding = node_embedding
        self.max_z = max_z
        self.z_embedding = Embedding(self.max_z, hidden_channels)

        self.convs = ModuleList()
        initial_channels = hidden_channels
        if self.use_feature:
            initial_channels += num_features
        if self.node_embedding is not None:
            initial_channels += node_embedding.embedding_dim
        self.convs.append(SAGEConv(initial_channels, hidden_channels))
        for _ in range(num_layers - 1):
            self.convs.append(SAGEConv(hidden_channels, hidden_channels))

        self.dropout = dropout
        self.lin1 = Linear(hidden_channels, hidden_channels)
        self.lin2 = Linear(hidden_channels, 1)

    def reset_parameters(self):
        for conv in self.convs:
            conv.reset_parameters()

    def forward(self, z, edge_index, batch, x=None, edge_weight=None, node_id=None, src_degree=None, dst_degree=None):
        z_emb = self.z_embedding(z)
        if z_emb.ndim == 3:  # in case z has multiple integer labels
            z_emb = z_emb.sum(dim=1)
        if self.use_feature and x is not None:
            x = torch.cat([z_emb, x.to(torch.float)], 1)
        else:
            x = z_emb
        if self.node_embedding is not None and node_id is not None:
            n_emb = self.node_embedding(node_id)
            x = torch.cat([x, n_emb], 1)
        for conv in self.convs[:-1]:
            x = conv(x, edge_index)
            x = F.relu(x)
            x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.convs[-1](x, edge_index)
        if True:  # center pooling
            _, center_indices = np.unique(batch.cpu().numpy(), return_index=True)
            x_src = x[center_indices]
            x_dst = x[center_indices + 1]
            x = (x_src * x_dst)
            x = F.relu(self.lin1(x))
            x = F.dropout(x, p=self.dropout, training=self.training)
            x = self.lin2(x)
        else:  # sum pooling
            x = global_add_pool(x, batch)
            x = F.relu(self.lin1(x))
            x = F.dropout(x, p=self.dropout, training=self.training)
            x = self.lin2(x)

        return x


# An end-to-end deep learning architecture for graph classification, AAAI-18.
class SEALDGCNN(torch.nn.Module):
    def __init__(self, hidden_channels, num_layers, max_z, k=0.6, train_dataset=None,
                 dynamic_train=False, GNN=GCNConv, use_feature=False,
                 node_embedding=None):
        super(SEALDGCNN, self).__init__()

        self.use_feature = use_feature
        self.node_embedding = node_embedding

        if k <= 1:  # Transform percentile to number.
            if train_dataset is None:
                k = 30
            else:
                if dynamic_train:
                    sampled_train = train_dataset[:1000]
                else:
                    sampled_train = train_dataset
                num_nodes = sorted([g.num_nodes for g in sampled_train])
                k = num_nodes[int(math.ceil(k * len(num_nodes))) - 1]
                k = max(10, k)
        self.k = int(k)

        self.max_z = max_z
        self.z_embedding = Embedding(self.max_z, hidden_channels)

        self.convs = ModuleList()
        initial_channels = hidden_channels
        if self.use_feature:
            initial_channels += train_dataset.num_features
        if self.node_embedding is not None:
            initial_channels += node_embedding.embedding_dim

        self.convs.append(GNN(initial_channels, hidden_channels))
        for i in range(0, num_layers - 1):
            self.convs.append(GNN(hidden_channels, hidden_channels))
        self.convs.append(GNN(hidden_channels, 1))

        conv1d_channels = [16, 32]
        total_latent_dim = hidden_channels * num_layers + 1
        conv1d_kws = [total_latent_dim, 5]
        self.conv1 = Conv1d(1, conv1d_channels[0], conv1d_kws[0],
                            conv1d_kws[0])
        self.maxpool1d = MaxPool1d(2, 2)
        self.conv2 = Conv1d(conv1d_channels[0], conv1d_channels[1],
                            conv1d_kws[1], 1)
        dense_dim = int((self.k - 2) / 2 + 1)
        dense_dim = (dense_dim - conv1d_kws[1] + 1) * conv1d_channels[1]
        self.lin1 = Linear(dense_dim, 128)
        self.lin2 = Linear(128, 1)

    def forward(self, z, edge_index, batch, x=None, edge_weight=None, node_id=None, src_degree=None, dst_degree=None):
        z_emb = self.z_embedding(z)
        if z_emb.ndim == 3:  # in case z has multiple integer labels
            z_emb = z_emb.sum(dim=1)
        if self.use_feature and x is not None:
            x = torch.cat([z_emb, x.to(torch.float)], 1)
        else:
            x = z_emb
        if self.node_embedding is not None and node_id is not None:
            n_emb = self.node_embedding(node_id)
            x = torch.cat([x, n_emb], 1)
        xs = [x]

        for conv in self.convs:
            xs += [torch.tanh(conv(xs[-1], edge_index, edge_weight))]
        x = torch.cat(xs[1:], dim=-1)

        # Global pooling.
        x = global_sort_pool(x, batch, self.k)
        x = x.unsqueeze(1)  # [num_graphs, 1, k * hidden]
        x = F.relu(self.conv1(x))
        x = self.maxpool1d(x)
        x = F.relu(self.conv2(x))
        x = x.view(x.size(0), -1)  # [num_graphs, dense_dim]

        # MLP.
        x = F.relu(self.lin1(x))
        x = F.dropout(x, p=0.5, training=self.training)
        x = self.lin2(x)
        return x


class SEALGIN(torch.nn.Module):
    def __init__(self, hidden_channels, num_layers, max_z, num_features=None,
                 use_feature=False, node_embedding=None, dropout=0.5,
                 jk=True, train_eps=False):
        super(SEALGIN, self).__init__()
        self.use_feature = use_feature
        self.node_embedding = node_embedding
        self.max_z = max_z
        self.z_embedding = Embedding(self.max_z, hidden_channels)
        self.jk = jk

        initial_channels = hidden_channels
        if self.use_feature:
            initial_channels += num_features
        if self.node_embedding is not None:
            initial_channels += node_embedding.embedding_dim
        self.conv1 = GINConv(
            Sequential(
                Linear(initial_channels, hidden_channels),
                ReLU(),
                Linear(hidden_channels, hidden_channels),
                ReLU(),
                BN(hidden_channels),
            ),
            train_eps=train_eps)
        self.convs = torch.nn.ModuleList()
        for i in range(num_layers - 1):
            self.convs.append(
                GINConv(
                    Sequential(
                        Linear(hidden_channels, hidden_channels),
                        ReLU(),
                        Linear(hidden_channels, hidden_channels),
                        ReLU(),
                        BN(hidden_channels),
                    ),
                    train_eps=train_eps))

        self.dropout = dropout
        if self.jk:
            self.lin1 = Linear(num_layers * hidden_channels, hidden_channels)
        else:
            self.lin1 = Linear(hidden_channels, hidden_channels)
        self.lin2 = Linear(hidden_channels, 1)

    def forward(self, z, edge_index, batch, x=None, edge_weight=None, node_id=None, src_degree=None, dst_degree=None):
        z_emb = self.z_embedding(z)
        if z_emb.ndim == 3:  # in case z has multiple integer labels
            z_emb = z_emb.sum(dim=1)
        if self.use_feature and x is not None:
            x = torch.cat([z_emb, x.to(torch.float)], 1)
        else:
            x = z_emb
        if self.node_embedding is not None and node_id is not None:
            n_emb = self.node_embedding(node_id)
            x = torch.cat([x, n_emb], 1)
        x = self.conv1(x, edge_index)
        xs = [x]
        for conv in self.convs:
            x = conv(x, edge_index)
            xs += [x]
        if self.jk:
            x = global_mean_pool(torch.cat(xs, dim=1), batch)
        else:
            x = global_mean_pool(xs[-1], batch)
        x = F.relu(self.lin1(x))
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.lin2(x)

        return x

</file>

<file path="lp/Buddy/src/models/transx.py">
"""
TransE family of knowledge graph embedding models transE, complEx, DistMult, rotatE
This code is adapted form the OGB examples https://github.com/snap-stanford/ogb/tree/master/examples/linkproppred/biokg
"""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import logging
from collections import defaultdict

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader


class KGEModel(nn.Module):
    def __init__(self, model_name, nentity, nrelation, hidden_dim, gamma, criterion,
                 double_entity_embedding=False, double_relation_embedding=False):
        super(KGEModel, self).__init__()
        self.model_name = model_name
        self.nentity = nentity
        self.nrelation = nrelation
        self.hidden_dim = hidden_dim
        self.epsilon = 2.0
        self.criterion = self.set_criterion(criterion, gamma)
        model_dict = {
            'transE': self.transE,
            'distmult': self.distMult,
            'complEx': self.complEx,
            'rotatE': self.rotatE,
        }
        self.model_func = model_dict[model_name]

        self.gamma = nn.Parameter(
            torch.Tensor([gamma]),
            requires_grad=False
        )

        self.embedding_range = nn.Parameter(
            torch.Tensor([(self.gamma.item() + self.epsilon) / hidden_dim]),
            requires_grad=False
        )

        self.entity_dim = hidden_dim * 2 if double_entity_embedding else hidden_dim
        self.relation_dim = hidden_dim * 2 if double_relation_embedding else hidden_dim

        self.entity_embedding = nn.Parameter(torch.zeros(nentity, self.entity_dim))
        nn.init.uniform_(
            tensor=self.entity_embedding,
            a=-self.embedding_range.item(),
            b=self.embedding_range.item()
        )

        self.relation_embedding = nn.Parameter(torch.zeros(nrelation, self.relation_dim))
        nn.init.uniform_(
            tensor=self.relation_embedding,
            a=-self.embedding_range.item(),
            b=self.embedding_range.item()
        )

        if model_name == 'rotatE' and (not double_entity_embedding or double_relation_embedding):
            raise ValueError('rotatE should use --double_entity_embedding')

        if model_name == 'complEx' and (not double_entity_embedding or not double_relation_embedding):
            raise ValueError('complEx should use --double_entity_embedding and --double_relation_embedding')

        # self.evaluator = evaluator

    def nll(self, pos_dist, neg_dist, *args):
        positive_score = F.logsigmoid(-pos_dist).squeeze(dim=1)
        negative_score = F.logsigmoid(neg_dist).squeeze(dim=1)
        return -(positive_score + negative_score) / 2

    def set_criterion(self, criterion, gamma):
        if criterion == 'mrl':
            retval = nn.MarginRankingLoss(margin=gamma, reduction='none')
        elif criterion == 'nll':
            retval = self.nll
        return retval

    def score_sample(self, sample, mode='single'):
        if mode == 'single':
            batch_size, negative_sample_size = sample.size(0), 1

            # this is a hack as there are no relations in this dataset
            head = torch.index_select(
                self.entity_embedding,
                dim=0,
                index=sample[0, :]
            ).unsqueeze(1).to(sample.device)

            # relation = self.relation_embedding
            relation = torch.index_select(
                self.relation_embedding,
                dim=0,
                index=torch.zeros(sample.shape[1], dtype=torch.int32, device=sample.device)
            ).unsqueeze(1).to(sample.device)

            tail = torch.index_select(
                self.entity_embedding,
                dim=0,
                index=sample[1, :]
            ).unsqueeze(1).to(sample.device)

        elif mode == 'head-batch':
            tail_part, head_part = sample
            batch_size, negative_sample_size = head_part.size(0), head_part.size(1)

            head = torch.index_select(
                self.entity_embedding,
                dim=0,
                index=head_part.view(-1)
            ).view(batch_size, negative_sample_size, -1).to(sample.device)

            relation = torch.index_select(
                self.relation_embedding,
                dim=0,
                index=tail_part[:, 1]
            ).unsqueeze(1).to(sample.device)

            tail = torch.index_select(
                self.entity_embedding,
                dim=0,
                index=tail_part[:, 2]
            ).unsqueeze(1).to(sample.device)

        elif mode == 'tail-batch':
            head_part, tail_part = sample
            batch_size, negative_sample_size = tail_part.size(0), tail_part.size(1)

            head = torch.index_select(
                self.entity_embedding,
                dim=0,
                index=head_part[:, 0]
            ).unsqueeze(1).to(sample.device)

            relation = torch.index_select(
                self.relation_embedding,
                dim=0,
                index=head_part[:, 1]
            ).unsqueeze(1).to(sample.device)

            tail = torch.index_select(
                self.entity_embedding,
                dim=0,
                index=tail_part.view(-1)
            ).view(batch_size, negative_sample_size, -1).to(sample.device)

        else:
            raise ValueError(f'mode {mode} not supported')

        return self.model_func(head, relation, tail, mode)

    def forward(self, pos_sample, neg_sample, mode='single'):
        '''
        Forward function that calculate the score of a batch of triples.
        In the 'single' mode, sample is a batch of triples.
        In the 'head-batch' or 'tail-batch' mode, sample consists of two parts.
        The first part is usually the positive sample.
        And the second part is the entities in the negative samples.
        Because negative samples and positive samples usually share two elements
        in their triple ((head, relation) or (relation, tail)).
        '''
        pos_dist = self.score_sample(pos_sample)
        neg_dist = self.score_sample(neg_sample)
        # For the target If :math:`y = 1` then it assumed the first input should be ranked higher
        #     (have a larger value) than the second input, and vice-versa for :math:`y = -1`.
        target = torch.tensor([-1], dtype=torch.long, device=pos_sample.device)
        loss = self.criterion(pos_dist, neg_dist, target)

        return loss

    def transE(self, head, relation, tail, mode):
        if mode == 'head-batch':
            score = head + (relation - tail)
        else:
            score = (head + relation) - tail

        score = torch.norm(score, p=1, dim=2)
        return score

    def distMult(self, head, relation, tail, mode):
        if mode == 'head-batch':
            score = head * (relation * tail)
        else:
            score = (head * relation) * tail

        score = score.sum(dim=2)
        return score

    def complEx(self, head, relation, tail, mode):
        re_head, im_head = torch.chunk(head, 2, dim=2)
        re_relation, im_relation = torch.chunk(relation, 2, dim=2)
        re_tail, im_tail = torch.chunk(tail, 2, dim=2)

        if mode == 'head-batch':
            re_score = re_relation * re_tail + im_relation * im_tail
            im_score = re_relation * im_tail - im_relation * re_tail
            score = re_head * re_score + im_head * im_score
        else:
            re_score = re_head * re_relation - im_head * im_relation
            im_score = re_head * im_relation + im_head * re_relation
            score = re_score * re_tail + im_score * im_tail

        score = score.sum(dim=2)
        return score

    def rotatE(self, head, relation, tail, mode):
        pi = 3.14159265358979323846

        re_head, im_head = torch.chunk(head, 2, dim=2)
        re_tail, im_tail = torch.chunk(tail, 2, dim=2)

        # Make phases of relations uniformly distributed in [-pi, pi]

        phase_relation = relation / (self.embedding_range.item() / pi)

        re_relation = torch.cos(phase_relation)
        im_relation = torch.sin(phase_relation)

        if mode == 'head-batch':
            re_score = re_relation * re_tail + im_relation * im_tail
            im_score = re_relation * im_tail - im_relation * re_tail
            re_score = re_score - re_head
            im_score = im_score - im_head
        else:
            re_score = re_head * re_relation - im_head * im_relation
            im_score = re_head * im_relation + im_head * re_relation
            re_score = re_score - re_tail
            im_score = im_score - im_tail

        score = torch.stack([re_score, im_score], dim=0)
        score = score.norm(dim=0)

        score = self.gamma.item() - score.sum(dim=2)
        return score

    @staticmethod
    def train_step(model, optimizer, train_iterator, args):
        '''
        A single train step. Apply back-propation and return the loss
        '''

        model.train()
        optimizer.zero_grad()
        positive_sample, negative_sample, subsampling_weight, mode = next(train_iterator)

        if args.cuda:
            positive_sample = positive_sample.cuda()
            negative_sample = negative_sample.cuda()
            subsampling_weight = subsampling_weight.cuda()

        negative_score = model((positive_sample, negative_sample), mode=mode)
        if args.negative_adversarial_sampling:
            # In self-adversarial sampling, we do not apply back-propagation on the sampling weight
            negative_score = (F.softmax(negative_score * args.adversarial_temperature, dim=1).detach()
                              * F.logsigmoid(-negative_score)).sum(dim=1)
        else:
            negative_score = F.logsigmoid(-negative_score).mean(dim=1)

        positive_score = model(positive_sample)
        positive_score = F.logsigmoid(positive_score).squeeze(dim=1)

        if args.uni_weight:
            positive_sample_loss = - positive_score.mean()
            negative_sample_loss = - negative_score.mean()
        else:
            positive_sample_loss = - (subsampling_weight * positive_score).sum() / subsampling_weight.sum()
            negative_sample_loss = - (subsampling_weight * negative_score).sum() / subsampling_weight.sum()

        loss = (positive_sample_loss + negative_sample_loss) / 2

        if args.regularization != 0.0:
            # Use L3 regularization for ComplEx and DistMult
            regularization = args.regularization * (
                    model.entity_embedding.norm(p=3) ** 3 +
                    model.relation_embedding.norm(p=3).norm(p=3) ** 3
            )
            loss = loss + regularization
            regularization_log = {'regularization': regularization.item()}
        else:
            regularization_log = {}

        loss.backward()

        optimizer.step()

        log = {
            **regularization_log,
            'positive_sample_loss': positive_sample_loss.item(),
            'negative_sample_loss': negative_sample_loss.item(),
            'loss': loss.item()
        }

        return log

    @staticmethod
    def test_step(model, test_triples, args, entity_dict, random_sampling=False):
        '''
        Evaluate the model on test or valid datasets
        '''

        model.eval()

        # Prepare dataloader for evaluation
        test_dataloader_head = DataLoader(
            TestDataset(
                test_triples,
                args,
                'head-batch',
                random_sampling,
                entity_dict
            ),
            batch_size=args.test_batch_size,
            num_workers=max(1, args.cpu_num // 2),
            collate_fn=TestDataset.collate_fn
        )

        test_dataloader_tail = DataLoader(
            TestDataset(
                test_triples,
                args,
                'tail-batch',
                random_sampling,
                entity_dict
            ),
            batch_size=args.test_batch_size,
            num_workers=max(1, args.cpu_num // 2),
            collate_fn=TestDataset.collate_fn
        )

        test_dataset_list = [test_dataloader_head, test_dataloader_tail]

        test_logs = defaultdict(list)

        step = 0
        total_steps = sum([len(dataset) for dataset in test_dataset_list])

        with torch.no_grad():
            for test_dataset in test_dataset_list:
                for positive_sample, negative_sample, mode in test_dataset:
                    if args.cuda:
                        positive_sample = positive_sample.cuda()
                        negative_sample = negative_sample.cuda()

                    batch_size = positive_sample.size(0)
                    score = model((positive_sample, negative_sample), mode)

                    batch_results = model.evaluator.eval({'y_pred_pos': score[:, 0],
                                                          'y_pred_neg': score[:, 1:]})
                    for metric in batch_results:
                        test_logs[metric].append(batch_results[metric])

                    if step % args.test_log_steps == 0:
                        logging.info('Evaluating the model... (%d/%d)' % (step, total_steps))

                    step += 1

            metrics = {}
            for metric in test_logs:
                metrics[metric] = torch.cat(test_logs[metric]).mean().item()

        return metrics

</file>

<file path="lp/Buddy/src/runners/__init__.py">

</file>

<file path="lp/Buddy/src/runners/configs/defaults.yaml">
defaults:
  # - override hydra/launcher: submitit_slurm
  - _self_

# Configuration Settings for Efficient Link Prediction with Hashes (ELPH)
dataset_name: "Cora"
val_pct: 0.1
test_pct: 0.2
preprocessing: null
sign_k: 1
load_features: False
load_hashes: False
cache_subgraph_features: True
year: 0

# GNN Settings
model: "BUDDY"
hidden_channels: 1024
batch_size: 512
eval_batch_size: 50000
label_dropout: 0.5
feature_dropout: 0.5
sign_dropout: 0.5
save_model: False
feature_prop: "gcn"

# SEAL Settings
dropout: 0.5
num_seal_layers: 3
sortpool_k: 0.6
label_pooling: "add"
seal_pooling: "edge"

# Subgraph Settings
num_hops: 1
ratio_per_hop: 1.0
max_nodes_per_hop: null
node_label: "drnl"
max_dist: 4
max_z: 1000
use_feature: True
use_struct_feature: True
use_edge_weight: False
feat: "imagebind"

# Training Settings
lr: 0.0001
weight_decay: 0
epochs: 30
num_workers: 4
num_negs: 1
train_node_embedding: False
propagate_embeddings: False
loss: "bce"
add_normed_features: False
use_RA: False

# SEAL Specific Arguments
dynamic_train: False
dynamic_val: False
dynamic_test: False
pretrained_node_embedding: null

# Testing Settings
reps: 3
use_valedges_as_input: False
eval_steps: 5
log_steps: 1
eval_metric: "hits"
K: 100

# Hash Settings
use_zero_one: False
floor_sf: False
hll_p: 8
minhash_num_perm: 128
max_hash_hops: 2
subgraph_feature_batch_size: 11000000

# Weights & Biases (wandb) Settings
wandb: False
wandb_offline: False
wandb_sweep: False
wandb_watch_grad: False
wandb_track_grad_flow: False
wandb_entity: "link-prediction"
wandb_project: "link-prediction"
wandb_group: "testing"
wandb_run_name: null
wandb_output_dir: "./wandb_output"
wandb_log_freq: 1
wandb_epoch_list:
  - 0
  - 1
  - 2
  - 4
  - 8
  - 16
log_features: False

hydra:
  job:
    chdir: True
  # launcher:
  #   name: 'gnn'
  #   timeout_min: 4320
  #   cpus_per_task: 4
  #   gres: 'gpu:1'
  #   tasks_per_node: 1
  #   mem_per_cpu: 10000
  #   partition: 'spgpu'
  #   account: 'dkoutra1'
  #   array_parallelism: 3 # limit the number of jobs can be run at the same time
  #   exclude: 'gl1507, gl1510, gl1513'
</file>

<file path="lp/Buddy/src/runners/configs/optuna.yaml">
defaults:
  - defaults
  - override hydra/sweeper: optuna
  - _self_


hydra:
  sweeper:
    sampler:
      seed: 123
    direction: maximize
    study_name: gnn
    storage: null
    n_trials: 20
    n_jobs: 1
    # max_failure_rate: 0.0
    params:
      lr: tag(log, interval(1e-4, 1e-2))
      sign_k: choice(0, 1, 2)


</file>

<file path="lp/Buddy/src/runners/inference.py">
"""
testing / inference functions
"""
import time
from math import inf
from ogb.linkproppred import Evaluator
import torch
from torch.utils.data import DataLoader
from tqdm import tqdm
import wandb
import numpy as np

from src.evaluation import evaluate_auc, evaluate_hits, evaluate_mrr
from src.utils import get_num_samples


def get_test_func(model_str):
    if model_str == 'ELPH':
        return get_elph_preds
    elif model_str == 'BUDDY':
        return get_buddy_preds
    else:
        return get_preds


@torch.no_grad()
def test(model, evaluator, train_loader, val_loader, test_loader, args, device, emb=None, eval_metric='hits'):
    # print('starting testing')
    t0 = time.time()
    model.eval()
    print("get train predictions")
    test_func = get_test_func(args.model)
    pos_train_pred, neg_train_pred, train_pred, train_true = test_func(model, train_loader, device, args, split='train')
    print("get val predictions")
    pos_val_pred, neg_val_pred, val_pred, val_true = test_func(model, val_loader, device, args, split='val')
    print("get test predictions")
    pos_test_pred, neg_test_pred, test_pred, test_true = test_func(model, test_loader, device, args, split='test')

    if args.dataset_name.startswith('sports') or args.dataset_name.startswith('cloth') or args.dataset_name.startswith(
            'book'):
        evaluator = Evaluator(name="ogbl-citation2")
        results = evaluate_mrr(evaluator, pos_train_pred, neg_train_pred, pos_val_pred, neg_val_pred, pos_test_pred,
                               neg_test_pred)
        # print(results)
    elif eval_metric == 'hits':
        results = evaluate_hits(evaluator, pos_train_pred, neg_train_pred, pos_val_pred, neg_val_pred, pos_test_pred,
                                neg_test_pred, Ks=[args.K])
    elif eval_metric == 'mrr':

        results = evaluate_mrr(evaluator, pos_train_pred, neg_train_pred, pos_val_pred, neg_val_pred, pos_test_pred,
                               neg_test_pred)
    elif eval_metric == 'auc':
        results = evaluate_auc(val_pred, val_true, test_pred, test_true)
    # print(f'testing ran in {time.time() - t0}')

    return results


@torch.no_grad()
def get_preds(model, loader, device, args, emb=None, split=None):
    n_samples = get_split_samples(split, args, len(loader.dataset))
    y_pred, y_true = [], []
    pbar = tqdm(loader, ncols=70)
    if args.wandb:
        wandb.log({f"inference_{split}_total_batches": len(loader)})
    batch_processing_times = []
    t0 = time.time()
    for batch_count, data in enumerate(pbar):
        start_time = time.time()
        # todo this should not get hit, refactor out the if statement
        if args.model == 'BUDDY':
            data_dev = [elem.squeeze().to(device) for elem in data]
            logits = model(*data_dev[:-1])
            y_true.append(data[-1].view(-1).cpu().to(torch.float))
        else:
            data = data.to(device)
            x = data.x if args.use_feature else None
            edge_weight = data.edge_weight if args.use_edge_weight else None
            node_id = data.node_id if emb else None
            logits = model(data.z, data.edge_index, data.batch, x, edge_weight, node_id, data.src_degree,
                           data.dst_degree)
            y_true.append(data.y.view(-1).cpu().to(torch.float))
        y_pred.append(logits.view(-1).cpu())
        batch_processing_times.append(time.time() - start_time)
        if (batch_count + 1) * args.batch_size > n_samples:
            del data
            torch.cuda.empty_cache()
            break
        del data
        torch.cuda.empty_cache()
    if args.wandb:
        wandb.log({f"inference_{split}_batch_time": np.mean(batch_processing_times)})
        wandb.log({f"inference_{split}_epoch_time": time.time() - t0})

    pred, true = torch.cat(y_pred), torch.cat(y_true)
    pos_pred = pred[true == 1]
    neg_pred = pred[true == 0]
    samples_used = len(loader.dataset) if n_samples > len(loader.dataset) else n_samples
    # print(f'{len(pos_pred)} positives and {len(neg_pred)} negatives for sample of {samples_used} edges')
    return pos_pred, neg_pred, pred, true


@torch.no_grad()
def get_buddy_preds(model, loader, device, args, split=None):
    n_samples = get_split_samples(split, args, len(loader.dataset))
    t0 = time.time()
    preds = []
    data = loader.dataset
    # hydrate edges
    links = data.links
    labels = torch.tensor(data.labels)
    loader = DataLoader(range(len(links)), args.eval_batch_size,
                        shuffle=False)  # eval batch size should be the largest that fits on GPU
    if model.node_embedding is not None:
        if args.propagate_embeddings:
            emb = model.propagate_embeddings_func(data.edge_index.to(device))
        else:
            emb = model.node_embedding.weight
    else:
        emb = None
    for batch_count, indices in enumerate(tqdm(loader)):
        curr_links = links[indices]
        batch_emb = None if emb is None else emb[curr_links].to(device)
        if args.use_struct_feature:
            subgraph_features = data.subgraph_features[indices].to(device)
        else:
            subgraph_features = torch.zeros(data.subgraph_features[indices].shape).to(device)
        node_features = data.x[curr_links].to(device)
        degrees = data.degrees[curr_links].to(device)
        if args.use_RA:
            RA = data.RA[indices].to(device)
        else:
            RA = None
        logits = model(subgraph_features, node_features, degrees[:, 0], degrees[:, 1], RA, batch_emb)
        preds.append(logits.view(-1).cpu())
        if (batch_count + 1) * args.eval_batch_size > n_samples:
            break

    if args.wandb:
        wandb.log({f"inference_{split}_epoch_time": time.time() - t0})
    pred = torch.cat(preds)
    labels = labels[:len(pred)]
    pos_pred = pred[labels == 1]
    neg_pred = pred[labels == 0]
    return pos_pred, neg_pred, pred, labels


def get_split_samples(split, args, dataset_len):
    """
    get the
    :param split: train, val, test
    :param args: Namespace object
    :param dataset_len: total size of dataset
    :return:
    """
    samples = dataset_len
    if split == 'train':
        if args.dynamic_train:
            samples = get_num_samples(args.train_samples, dataset_len)
    elif split in {'val', 'valid'}:
        if args.dynamic_val:
            samples = get_num_samples(args.val_samples, dataset_len)
    elif split == 'test':
        if args.dynamic_test:
            samples = get_num_samples(args.test_samples, dataset_len)
    else:
        raise NotImplementedError(f'split: {split} is not a valid split')
    return samples


@torch.no_grad()
def get_elph_preds(model, loader, device, args, split=None):
    n_samples = get_split_samples(split, args, len(loader.dataset))
    t0 = time.time()
    preds = []
    data = loader.dataset
    # hydrate edges
    links = data.links
    labels = torch.tensor(data.labels)
    loader = DataLoader(range(len(links)), args.eval_batch_size,
                        shuffle=False)  # eval batch size should be the largest that fits on GPU
    # get node features
    if model.node_embedding is not None:
        if args.propagate_embeddings:
            emb = model.propagate_embeddings_func(data.edge_index.to(device))
        else:
            emb = model.node_embedding.weight
    else:
        emb = None
    node_features, hashes, cards = model(data.x.to(device), data.edge_index.to(device))
    for batch_count, indices in enumerate(tqdm(loader)):
        curr_links = links[indices].to(device)
        batch_emb = None if emb is None else emb[curr_links].to(device)
        if args.use_struct_feature:
            subgraph_features = model.elph_hashes.get_subgraph_features(curr_links, hashes, cards).to(device)
        else:
            subgraph_features = torch.zeros(data.subgraph_features[indices].shape).to(device)
        batch_node_features = None if node_features is None else node_features[curr_links]
        logits = model.predictor(subgraph_features, batch_node_features, batch_emb)
        preds.append(logits.view(-1).cpu())
        if (batch_count + 1) * args.eval_batch_size > n_samples:
            break

    if args.wandb:
        wandb.log({f"inference_{split}_epoch_time": time.time() - t0})
    pred = torch.cat(preds)
    labels = labels[:len(pred)]
    pos_pred = pred[labels == 1]
    neg_pred = pred[labels == 0]
    return pos_pred, neg_pred, pred, labels

</file>

<file path="lp/Buddy/src/runners/run.py">
"""
main module
"""
import argparse
import time
import warnings
from math import inf
import sys
import random

sys.path.insert(0, '..')
import os
import numpy as np
import torch
from ogb.linkproppred import Evaluator

torch.set_printoptions(precision=4)
import wandb
# when generating subgraphs the supervision edge is deleted, which triggers a SparseEfficiencyWarning, but this is
# not a performance bottleneck, so suppress for now
from scipy.sparse import SparseEfficiencyWarning

warnings.filterwarnings("ignore", category=SparseEfficiencyWarning)

from src.data import get_data, get_loaders
from src.models.elph import ELPH, BUDDY
from src.models.seal import SEALDGCNN, SEALGCN, SEALGIN, SEALSAGE
from src.utils import ROOT_DIR, print_model_params, select_embedding, str2bool
from src.wandb_setup import initialise_wandb
from src.runners.train import get_train_func
from src.runners.inference import test
from omegaconf import DictConfig, OmegaConf
import optuna
from types import SimpleNamespace
import hydra

PROJETC_DIR = os.path.dirname(os.path.realpath(__file__))
DATA_PATH = os.path.join(PROJETC_DIR, '../')
CONFIG_DIR = os.path.join(PROJETC_DIR, "configs")

# print(CONFIG_DIR)


def print_results_list(results_list):
    for idx, res in enumerate(results_list):
        print(f'repetition {idx}: test {res[0]:.2f}, val {res[1]:.2f}, train {res[2]:.2f}')


def set_seed(seed):
    """
    setting a random seed for reproducibility and in accordance with OGB rules
    @param seed: an integer seed
    @return: None
    """
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


@hydra.main(config_path=CONFIG_DIR, config_name="defaults", version_base='1.2')
def main(cfg: DictConfig):
    def dict_config_to_namespace(cfg):
        # This converts the DictConfig to a regular dictionary and then to a SimpleNamespace
        return SimpleNamespace(**OmegaConf.to_container(cfg, resolve=True))

    args = dict_config_to_namespace(cfg)
    args.train_samples = inf
    args.val_samples = inf
    args.test_samples = inf
    args.train_cache_size = inf
    # print(args)

    if args.dataset_name == 'ogbl-ddi':
        args.use_feature = 0  # dataset has no features
        assert args.sign_k > 0, '--sign_k must be set to > 0 i.e. 1,2 or 3 for ogbl-ddi'
    args = initialise_wandb(args)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    # print(f"executing on {device}")
    train_func = get_train_func(args)
    mrrs, h10s, h1s = [], [], []
    for rep in range(args.reps):
        set_seed(rep)
        dataset, splits, directed, eval_metric = get_data(args)
        train_loader, train_eval_loader, val_loader, test_loader = get_loaders(args, dataset, splits, directed)
        if args.dataset_name.startswith('ogbl'):  # then this is one of the ogb link prediction datasets
            evaluator = Evaluator(name=args.dataset_name)
        else:
            evaluator = Evaluator(name='ogbl-ppa')  # this sets HR@100 as the metric
        emb = select_embedding(args, dataset.data.num_nodes, device)
        model, optimizer = select_model(args, dataset, emb, device)
        print(f'running repetition {rep}')
        test_mrr, test_h10, test_h1 = 0, 0, 0
        val_mrr, val_h10, val_h1 = 0, 0, 0
        for epoch in range(args.epochs):
            t0 = time.time()
            loss = train_func(model, optimizer, train_loader, args, device)
            if (epoch + 1) % args.eval_steps == 0:
                results = test(model, evaluator, train_eval_loader, val_loader, test_loader, args, device,
                               eval_metric=eval_metric)

                if results['MRR'][1] > val_mrr:
                    val_mrr = results['MRR'][1]
                    test_mrr = results['MRR'][2]
                    val_h1 = results['Hits@1'][1]
                    test_h1 = results['Hits@1'][2]
                    val_h10 = results['Hits@10'][1]
                    test_h10 = results['Hits@10'][2]

                for key, result in results.items():
                    train_res, val_res, test_res = result
                    to_print = f'Epoch: {epoch:02d}, Epoch: {epoch}, Loss: {loss:.4f}, Train: {100 * train_res:.2f}%, Valid: ' \
                               f'{100 * val_res:.2f}%, Test: {100 * test_res:.2f}%, epoch time: {time.time() - t0:.1f}'
                    print(key)
                    print(to_print)

        mrrs.append(test_mrr)
        h10s.append(test_h10)
        h1s.append(test_h1)

    mean_mrr = torch.mean(torch.tensor(mrrs)).item()
    std_mrr = torch.std(torch.tensor(mrrs)).item()
    mean_h1 = torch.mean(torch.tensor(h1s)).item()
    std_h1 = torch.std(torch.tensor(h1s)).item()
    mean_h10 = torch.mean(torch.tensor(h10s)).item()
    std_h10 = torch.std(torch.tensor(h10s)).item()
    print(mean_mrr)
    print(std_mrr)
    print(mean_h1)
    print(std_h1)
    print(mean_h10)
    print(std_h10)

    if args.save_model:
        path = f'{ROOT_DIR}/saved_models/{args.dataset_name}'
        torch.save(model.state_dict(), path)

    return mean_mrr


def select_model(args, dataset, emb, device):
    if args.model == 'SEALDGCNN':
        model = SEALDGCNN(args.hidden_channels, args.num_seal_layers, args.max_z, args.sortpool_k,
                          dataset, args.dynamic_train, use_feature=args.use_feature,
                          node_embedding=emb).to(device)
    elif args.model == 'SEALSAGE':
        model = SEALSAGE(args.hidden_channels, args.num_seal_layers, args.max_z, dataset.num_features,
                         args.use_feature, node_embedding=emb, dropout=args.dropout).to(device)
    elif args.model == 'SEALGCN':
        model = SEALGCN(args.hidden_channels, args.num_seal_layers, args.max_z, dataset.num_features,
                        args.use_feature, node_embedding=emb, dropout=args.dropout, pooling=args.seal_pooling).to(
            device)
    elif args.model == 'SEALGIN':
        model = SEALGIN(args.hidden_channels, args.num_seal_layers, args.max_z, dataset.num_features,
                        args.use_feature, node_embedding=emb, dropout=args.dropout).to(device)
    elif args.model == 'BUDDY':
        model = BUDDY(args, dataset.num_features, node_embedding=emb).to(device)
    elif args.model == 'ELPH':
        model = ELPH(args, dataset.num_features, node_embedding=emb).to(device)
    else:
        raise NotImplementedError
    parameters = list(model.parameters())
    if args.train_node_embedding:
        torch.nn.init.xavier_uniform_(emb.weight)
        parameters += list(emb.parameters())
    optimizer = torch.optim.Adam(params=parameters, lr=args.lr, weight_decay=args.weight_decay)
    total_params = sum(p.numel() for param in parameters for p in param)
    # print(f'Total number of parameters is {total_params}')
    if args.model == 'DGCNN':
        print(f'SortPooling k is set to {model.k}')
    return model, optimizer


if __name__ == '__main__':
    # Data settings
    # parser = argparse.ArgumentParser(description='Efficient Link Prediction with Hashes (ELPH)')
    # parser.add_argument('--dataset_name', type=str, default='Cora',
    #                     choices=['Cora', 'Citeseer', 'Pubmed', 'ogbl-ppa', 'ogbl-collab', 'ogbl-ddi',
    #                              'ogbl-citation2', 'sports-copurchase'])
    # parser.add_argument('--val_pct', type=float, default=0.1,
    #                     help='the percentage of supervision edges to be used for validation. These edges will not appear'
    #                          ' in the training set and will only be used as message passing edges in the test set')
    # parser.add_argument('--test_pct', type=float, default=0.2,
    #                     help='the percentage of supervision edges to be used for test. These edges will not appear'
    #                          ' in the training or validation sets for either supervision or message passing')
    # parser.add_argument('--train_samples', type=float, default=inf, help='the number of training edges or % if < 1')
    # parser.add_argument('--val_samples', type=float, default=inf, help='the number of val edges or % if < 1')
    # parser.add_argument('--test_samples', type=float, default=inf, help='the number of test edges or % if < 1')
    # parser.add_argument('--preprocessing', type=str, default=None)
    # parser.add_argument('--sign_k', type=int, default=0)
    # parser.add_argument('--load_features', action='store_true', help='load node features from disk')
    # parser.add_argument('--load_hashes', action='store_true', help='load hashes from disk')
    # parser.add_argument('--cache_subgraph_features', action='store_true',
    #                     help='write / read subgraph features from disk')
    # parser.add_argument('--train_cache_size', type=int, default=inf, help='the number of training edges to cache')
    # parser.add_argument('--year', type=int, default=0, help='filter training data from before this year')
    # # GNN settings
    # parser.add_argument('--model', type=str, default='BUDDY')
    # parser.add_argument('--hidden_channels', type=int, default=1024)
    # parser.add_argument('--batch_size', type=int, default=1024)
    # parser.add_argument('--eval_batch_size', type=int, default=1000000,
    #                     help='eval batch size should be largest the GPU memory can take - the same is not necessarily true at training time')
    # parser.add_argument('--label_dropout', type=float, default=0.5)
    # parser.add_argument('--feature_dropout', type=float, default=0.5)
    # parser.add_argument('--sign_dropout', type=float, default=0.5)
    # parser.add_argument('--save_model', action='store_true', help='save the model to use later for inference')
    # parser.add_argument('--feature_prop', type=str, default='gcn',
    #                     help='how to propagate ELPH node features. Values are gcn, residual (resGCN) or cat (jumping knowledge networks)')
    # # SEAL settings
    # parser.add_argument('--dropout', type=float, default=0.5)
    # parser.add_argument('--num_seal_layers', type=int, default=3)
    # parser.add_argument('--sortpool_k', type=float, default=0.6)
    # parser.add_argument('--label_pooling', type=str, default='add', help='add or mean')
    # parser.add_argument('--seal_pooling', type=str, default='edge', help='how SEAL pools features in the subgraph')
    # # Subgraph settings
    # parser.add_argument('--num_hops', type=int, default=1)
    # parser.add_argument('--ratio_per_hop', type=float, default=1.0)
    # parser.add_argument('--max_nodes_per_hop', type=int, default=None)
    # parser.add_argument('--node_label', type=str, default='drnl')
    # parser.add_argument('--max_dist', type=int, default=4)
    # parser.add_argument('--max_z', type=int, default=1000,
    #                     help='the size of the label embedding table. ie. the maximum number of labels possible')
    # parser.add_argument('--use_feature', type=str2bool, default=True,
    #                     help="whether to use raw node features as GNN input")
    # parser.add_argument('--use_struct_feature', type=str2bool, default=True,
    #                     help="whether to use structural graph features as GNN input")
    # parser.add_argument('--use_edge_weight', action='store_true',
    #                     help="whether to consider edge weight in GNN")
    # # Training settings
    # parser.add_argument('--lr', type=float, default=0.0001)
    # parser.add_argument('--weight_decay', type=float, default=0, help='Weight decay for optimization')
    # parser.add_argument('--epochs', type=int, default=100)
    # parser.add_argument('--num_workers', type=int, default=4)
    # parser.add_argument('--num_negs', type=int, default=1, help='number of negatives for each positive')
    # parser.add_argument('--train_node_embedding', action='store_true',
    #                     help="also train free-parameter node embeddings together with GNN")
    # parser.add_argument('--propagate_embeddings', action='store_true',
    #                     help='propagate the node embeddings using the GCN diffusion operator')
    # parser.add_argument('--loss', default='bce', type=str, help='bce or auc')
    # parser.add_argument('--add_normed_features', dest='add_normed_features', type=str2bool,
    #                     help='Adds a set of features that are normalsied by sqrt(d_i*d_j) to calculate cosine sim')
    # parser.add_argument('--use_RA', type=str2bool, default=False, help='whether to add resource allocation features')
    # # SEAL specific args
    # parser.add_argument('--dynamic_train', action='store_true',
    #                     help="dynamically extract enclosing subgraphs on the fly")
    # parser.add_argument('--dynamic_val', action='store_true')
    # parser.add_argument('--dynamic_test', action='store_true')
    # parser.add_argument('--pretrained_node_embedding', type=str, default=None,
    #                     help="load pretrained node embeddings as additional node features")
    # # Testing settings
    # parser.add_argument('--reps', type=int, default=1, help='the number of repetition of the experiment to run')
    # parser.add_argument('--use_valedges_as_input', action='store_true')
    # parser.add_argument('--eval_steps', type=int, default=1)
    # parser.add_argument('--log_steps', type=int, default=1)
    # parser.add_argument('--eval_metric', type=str, default='hits',
    #                     choices=('hits', 'mrr', 'auc'))
    # parser.add_argument('--K', type=int, default=100, help='the hit rate @K')
    # # hash settings
    # parser.add_argument('--use_zero_one', type=str2bool, default=0,
    #                     help="whether to use the counts of (0,1) and (1,0) neighbors")
    # parser.add_argument('--floor_sf', type=str2bool, default=0,
    #                     help='the subgraph features represent counts, so should not be negative. If --floor_sf the min is set to 0')
    # parser.add_argument('--hll_p', type=int, default=8, help='the hyperloglog p parameter')
    # parser.add_argument('--minhash_num_perm', type=int, default=128, help='the number of minhash perms')
    # parser.add_argument('--max_hash_hops', type=int, default=2, help='the maximum number of hops to hash')
    # parser.add_argument('--subgraph_feature_batch_size', type=int, default=11000000,
    #                     help='the number of edges to use in each batch when calculating subgraph features. '
    #                          'Reduce or this or increase system RAM if seeing killed messages for large graphs')
    # # wandb settings
    # parser.add_argument('--wandb', action='store_true', help="flag if logging to wandb")
    # parser.add_argument('--wandb_offline', dest='use_wandb_offline',
    #                     action='store_true')  # https://docs.wandb.ai/guides/technical-faq
    #
    # parser.add_argument('--wandb_sweep', action='store_true',
    #                     help="flag if sweeping")  # if not it picks up params in greed_params
    # parser.add_argument('--wandb_watch_grad', action='store_true', help='allows gradient tracking in train function')
    # parser.add_argument('--wandb_track_grad_flow', action='store_true')
    #
    # parser.add_argument('--wandb_entity', default="link-prediction", type=str)
    # parser.add_argument('--wandb_project', default="link-prediction", type=str)
    # parser.add_argument('--wandb_group', default="testing", type=str, help="testing,tuning,eval")
    # parser.add_argument('--wandb_run_name', default=None, type=str)
    # parser.add_argument('--wandb_output_dir', default='./wandb_output',
    #                     help='folder to output results, images and model checkpoints')
    # parser.add_argument('--wandb_log_freq', type=int, default=1, help='Frequency to log metrics.')
    # parser.add_argument('--wandb_epoch_list', nargs='+', default=[0, 1, 2, 4, 8, 16],
    #                     help='list of epochs to log gradient flow')
    # parser.add_argument('--log_features', action='store_true', help="log feature importance")
    # args = parser.parse_args()

    main()

</file>

<file path="lp/Buddy/src/runners/run_heuristics.py">
"""
run the heuristic baselines resource allocation, common neighbours, personalised pagerank and adamic adar
"""

"""
runner for heuristic link prediction methods. Currently Adamic Adar, Common Neighbours and Personalised PageRank
"""
import time

import argparse
from argparse import Namespace
from ogb.linkproppred import Evaluator
import scipy.sparse as ssp
import torch
import wandb
import numpy as np

from evaluation import evaluate_auc, evaluate_mrr, evaluate_hits
from data import get_data
from heuristics import AA, CN, PPR, RA
from utils import DEFAULT_DIC, get_pos_neg_edges

def run(args):
  args = Namespace(**{**DEFAULT_DIC, **vars(args)})
  wandb.init(project=args.wandb_project, config=args, entity=args.wandb_entity)
  # set the correct metric for ogb
  k = 100
  if args.dataset_name == 'ogbl-collab':
    k = 50
  elif args.dataset_name == 'ogbl-ppi':
    k = 20

  for heuristic in [RA, CN, AA, PPR]:
    results_list = []
    for rep in range(args.reps):
      t0 = time.time()
      dataset, splits, directed, eval_metric = get_data(args)
      train_data, val_data, test_data = splits['train'], splits['valid'], splits['test']
      num_nodes = dataset.data.num_nodes
      if 'edge_weight' in train_data:
        train_weight = train_data.edge_weight.view(-1)
        test_weight = test_data.edge_weight.view(-1)
      else:
        train_weight = torch.ones(train_data.edge_index.size(1), dtype=int)
        test_weight = torch.ones(test_data.edge_index.size(1), dtype=int)
      train_edges, val_edges, test_edges = train_data['edge_index'], val_data['edge_index'], test_data['edge_index']
      assert torch.equal(val_edges, train_edges)
      A_train = ssp.csr_matrix((train_weight, (train_edges[0], train_edges[1])),
                               shape=(num_nodes, num_nodes))
      A_test = ssp.csr_matrix((test_weight, (test_edges[0], test_edges[1])),
                              shape=(num_nodes, num_nodes))

      # this function returns transposed edge list of shape [?,2]
      pos_train_edge, neg_train_edge = get_pos_neg_edges(splits['train'])
      pos_val_edge, neg_val_edge = get_pos_neg_edges(splits['valid'])
      pos_test_edge, neg_test_edge = get_pos_neg_edges(splits['test'])

      print(f'results for {heuristic.__name__} (val, test)')
      pos_train_pred, pos_train_edge = heuristic(A_train, pos_train_edge)
      neg_train_pred, neg_train_edge = heuristic(A_train, neg_train_edge)
      pos_val_pred, pos_val_edge = heuristic(A_train, pos_val_edge)
      neg_val_pred, neg_val_edge = heuristic(A_train, neg_val_edge)
      pos_test_pred, pos_test_edge = heuristic(A_test, pos_test_edge)
      neg_test_pred, neg_test_edge = heuristic(A_test, neg_test_edge)

      if args.dataset_name == 'ogbl-citation2':
        evaluator = Evaluator(name=args.dataset_name)
        print(f'evaluating {pos_test_pred.shape} positive samples and {neg_test_pred.shape} negative samples')
        mrr_results = evaluate_mrr(evaluator, pos_train_pred, neg_train_pred, pos_val_pred, neg_val_pred, pos_test_pred,
                                   neg_test_pred)
        print(mrr_results)
        key = 'MRR'
        train_res, val_res, test_res = mrr_results[key]
        res_dic = {f'rep{rep}_Train' + key: 100 * train_res, f'rep{rep}_Val' + key: 100 * val_res,
                   f'rep{rep}_Test' + key: 100 * test_res}
        wandb.log(res_dic)
        results_list.append(mrr_results[key])
      else:
        evaluator = Evaluator(name='ogbl-ppa')
        hit_results = evaluate_hits(evaluator, pos_train_pred, neg_train_pred, pos_val_pred, neg_val_pred,
                                    pos_test_pred,
                                    neg_test_pred, Ks=[k])
        key = f'Hits@{k}'
        train_res, val_res, test_res = hit_results[key]
        res_dic = {f'rep{rep}_Train' + key: 100 * train_res, f'rep{rep}_Val' + key: 100 * val_res,
                   f'rep{rep}_Test' + key: 100 * test_res}
        wandb.log(res_dic)
        results_list.append(hit_results[key])
        print(hit_results)

      val_pred = torch.cat([pos_val_pred, neg_val_pred])
      val_true = torch.cat([torch.ones(pos_val_pred.size(0), dtype=int),
                            torch.zeros(neg_val_pred.size(0), dtype=int)])
      test_pred = torch.cat([pos_test_pred, neg_test_pred])
      test_true = torch.cat([torch.ones(pos_test_pred.size(0), dtype=int),
                             torch.zeros(neg_test_pred.size(0), dtype=int)])
      auc_results = evaluate_auc(val_pred, val_true, test_pred, test_true)
      print(auc_results)
    if args.reps > 1:
      train_acc_mean, val_acc_mean, test_acc_mean = np.mean(results_list, axis=0) * 100
      test_acc_std = np.sqrt(np.var(results_list, axis=0)[-1]) * 100
      wandb_results = {f'{heuristic.__name__}_test_mean': test_acc_mean, f'{heuristic.__name__}_val_mean': val_acc_mean,
                       f'{heuristic.__name__}_train_mean': train_acc_mean,
                       f'{heuristic.__name__}_test_acc_std': test_acc_std}
      wandb.log(wandb_results)
      print(wandb_results)
    print(f'{heuristic.__name__} ran in {time.time() - t0:.1f} s for {args.reps} reps')
  wandb.finish()


if __name__ == '__main__':
  parser = argparse.ArgumentParser()
  parser.add_argument('--dataset_name', type=str, default='Cora',
                      choices=['Cora', 'producer', 'Citeseer', 'Pubmed', 'ogbl-ppa', 'ogbl-collab', 'ogbl-ddi',
                               'ogbl-citation2'])
  parser.add_argument('--wandb_entity', default="link-prediction", type=str)
  parser.add_argument('--wandb_project', default="link-prediction", type=str)
  parser.add_argument('--reps', type=int, default=1, help='the number of repetition of the experiment to run')
  parser.add_argument('--sample_size', type=int, default=None,
                      help='the number of training edges to sample. Currently only implemented for producer data')

  args = parser.parse_args()
  print(args)
  run(args)

</file>

<file path="lp/Buddy/src/runners/train.py">
"""
training functions
"""
import time
from math import inf

import torch
from torch.utils.data import DataLoader
from torch.nn import BCEWithLogitsLoss
from tqdm import tqdm
import wandb
import numpy as np

from src.utils import get_num_samples


def get_train_func(args):
    if args.model == 'ELPH':
        return train_elph
    elif args.model == 'BUDDY':
        train_func = train_buddy
    else:
        train_func = train
    return train_func


def train_buddy(model, optimizer, train_loader, args, device, emb=None):
    # print('starting training')
    t0 = time.time()
    model.train()
    total_loss = 0
    data = train_loader.dataset
    # hydrate edges
    links = data.links
    labels = torch.tensor(data.labels)
    # sampling
    train_samples = get_num_samples(args.train_samples, len(labels))
    sample_indices = torch.randperm(len(labels))[:train_samples]
    links = links[sample_indices]
    labels = labels[sample_indices]

    if args.wandb:
        wandb.log({"train_total_batches": len(train_loader)})
    batch_processing_times = []
    loader = DataLoader(range(len(links)), args.batch_size, shuffle=True)
    for batch_count, indices in enumerate(tqdm(loader)):
        # do node level things
        if model.node_embedding is not None:
            if args.propagate_embeddings:
                emb = model.propagate_embeddings_func(data.edge_index.to(device))
            else:
                emb = model.node_embedding.weight
        else:
            emb = None
        curr_links = links[indices]
        batch_emb = None if emb is None else emb[curr_links].to(device)

        if args.use_struct_feature:
            sf_indices = sample_indices[indices]  # need the original link indices as these correspond to sf
            subgraph_features = data.subgraph_features[sf_indices].to(device)
        else:
            subgraph_features = torch.zeros(data.subgraph_features[indices].shape).to(device)
        node_features = data.x[curr_links].to(device)
        degrees = data.degrees[curr_links].to(device)
        if args.use_RA:
            ra_indices = sample_indices[indices]
            RA = data.RA[ra_indices].to(device)
        else:
            RA = None
        start_time = time.time()
        optimizer.zero_grad()
        logits = model(subgraph_features, node_features, degrees[:, 0], degrees[:, 1], RA, batch_emb)
        loss = get_loss(args.loss)(logits, labels[indices].squeeze(0).to(device))

        loss.backward()
        optimizer.step()
        total_loss += loss.item() * args.batch_size
        batch_processing_times.append(time.time() - start_time)

    if args.wandb:
        wandb.log({"train_batch_time": np.mean(batch_processing_times)})
        wandb.log({"train_epoch_time": time.time() - t0})

    # print(f'training ran in {time.time() - t0}')

    if args.log_features:
        model.log_wandb()

    return total_loss / len(train_loader.dataset)


def train(model, optimizer, train_loader, args, device, emb=None):
    """
    Adapted version of the SEAL training function
    :param model:
    :param optimizer:
    :param train_loader:
    :param args:
    :param device:
    :param emb:
    :return:
    """

    # print('starting training')
    t0 = time.time()
    model.train()
    if args.dynamic_train:
        train_samples = get_num_samples(args.train_samples, len(train_loader.dataset))
    else:
        train_samples = inf
    total_loss = 0
    pbar = tqdm(train_loader, ncols=70)
    if args.wandb:
        wandb.log({"train_total_batches": len(train_loader)})
    batch_processing_times = []
    for batch_count, data in enumerate(pbar):
        start_time = time.time()
        optimizer.zero_grad()
        # todo this loop should no longer be hit as this function isn't called for BUDDY
        if args.model == 'BUDDY':
            data_dev = [elem.squeeze().to(device) for elem in data]
            logits = model(*data_dev[:-1])  # everything but the labels
            loss = get_loss(args.loss)(logits, data[-1].squeeze(0).to(device))
        else:
            data = data.to(device)
            x = data.x if args.use_feature else None
            edge_weight = data.edge_weight if args.use_edge_weight else None
            node_id = data.node_id if emb else None
            logits = model(data.z, data.edge_index, data.batch, x, edge_weight, node_id, data.src_degree,
                           data.dst_degree)
            loss = get_loss(args.loss)(logits, data.y)
        if args.l1 > 0:
            l1_reg = torch.tensor(0, dtype=torch.float)
            lin_params = torch.cat([x.view(-1) for x in model.lin.parameters()])
            for param in lin_params:
                l1_reg += torch.norm(param, 1) ** 2
            loss = loss + args.l1 * l1_reg
        loss.backward()
        optimizer.step()
        total_loss += loss.item() * args.batch_size
        del data
        torch.cuda.empty_cache()
        batch_processing_times.append(time.time() - start_time)
        if (batch_count + 1) * args.batch_size > train_samples:
            break
    if args.wandb:
        wandb.log({"train_batch_time": np.mean(batch_processing_times)})
        wandb.log({"train_epoch_time": time.time() - t0})

    # print(f'training ran in {time.time() - t0}')
    # if args.model in {'linear', 'pmi', 'ra', 'aa', 'one_layer'}:
    #     model.print_params()

    if args.log_features:
        model.log_wandb()

    return total_loss / len(train_loader.dataset)


def train_elph(model, optimizer, train_loader, args, device):
    """
    train a GNN that calculates hashes using message passing
    @param model:
    @param optimizer:
    @param train_loader:
    @param args:
    @param device:
    @return:
    """
    # print('starting training')
    t0 = time.time()
    model.train()
    total_loss = 0
    data = train_loader.dataset
    # hydrate edges
    links = data.links
    labels = torch.tensor(data.labels)
    # sampling
    train_samples = get_num_samples(args.train_samples, len(labels))
    sample_indices = torch.randperm(len(labels))[:train_samples]
    links = links[sample_indices]
    labels = labels[sample_indices]

    if args.wandb:
        wandb.log({"train_total_batches": len(train_loader)})
    batch_processing_times = []
    loader = DataLoader(range(len(links)), args.batch_size, shuffle=True)
    for batch_count, indices in enumerate(tqdm(loader)):
        # do node level things
        if model.node_embedding is not None:
            if args.propagate_embeddings:
                emb = model.propagate_embeddings_func(data.edge_index.to(device))
            else:
                emb = model.node_embedding.weight
        else:
            emb = None
        # get node features
        node_features, hashes, cards = model(data.x.to(device), data.edge_index.to(device))
        curr_links = links[indices].to(device)
        batch_node_features = None if node_features is None else node_features[curr_links]
        batch_emb = None if emb is None else emb[curr_links].to(device)
        # hydrate link features
        if args.use_struct_feature:
            subgraph_features = model.elph_hashes.get_subgraph_features(curr_links, hashes, cards).to(device)
        else:  # todo fix this
            subgraph_features = torch.zeros(data.subgraph_features[indices].shape).to(device)
        start_time = time.time()
        optimizer.zero_grad()
        logits = model.predictor(subgraph_features, batch_node_features, batch_emb)
        loss = get_loss(args.loss)(logits, labels[indices].squeeze(0).to(device))

        loss.backward()
        optimizer.step()
        total_loss += loss.item() * args.batch_size
        batch_processing_times.append(time.time() - start_time)

    if args.wandb:
        wandb.log({"train_batch_time": np.mean(batch_processing_times)})
        wandb.log({"train_epoch_time": time.time() - t0})

    # print(f'training ran in {time.time() - t0}')
    # if args.model in {'linear', 'pmi', 'ra', 'aa', 'one_layer'}:
    #     model.print_params()

    if args.log_features:
        model.log_wandb()

    return total_loss / len(train_loader.dataset)


def auc_loss(logits, y, num_neg=1):
    pos_out = logits[y == 1]
    neg_out = logits[y == 0]
    # hack, should really pair negative and positives in the training set
    if len(neg_out) <= len(pos_out):
        pos_out = pos_out[:len(neg_out)]
    else:
        neg_out = neg_out[:len(pos_out)]
    pos_out = torch.reshape(pos_out, (-1, 1))
    neg_out = torch.reshape(neg_out, (-1, num_neg))
    return torch.square(1 - (pos_out - neg_out)).sum()


def bce_loss(logits, y, num_neg=1):
    return BCEWithLogitsLoss()(logits.view(-1), y.to(torch.float))


def get_loss(loss_str):
    if loss_str == 'bce':
        loss = bce_loss
    elif loss_str == 'auc':
        loss = auc_loss
    else:
        raise NotImplementedError
    return loss

</file>

<file path="lp/Buddy/src/utils.py">
"""
utility functions and global variables
"""

import os
from distutils.util import strtobool
from math import inf

import torch
import numpy as np

ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))

DEFAULT_DIC = {'sample_size': None, 'dataset_name': 'Cora', 'num_hops': 2, 'max_dist': 10, 'max_nodes_per_hop': 10,
               'data_appendix': None, 'val_pct': 0.1, 'test_pct': 0.2, 'train_sample': 1, 'dynamic_train': True,
               'dynamic_val': True, 'model': 'hashing', 'sign_k': 2,
               'dynamic_test': True, 'node_label': 'drnl', 'ratio_per_hop': 1, 'use_feature': True, 'dropout': 0,
               'label_dropout': 0, 'feature_dropout': 0,
               'add_normed_features': False, 'use_RA': False, 'hidden_channels': 32, 'load_features': True,
               'load_hashes': True, 'use_zero_one': True, 'wandb': False, 'batch_size': 32, 'num_workers': 1,
               'cache_subgraph_features': False, 'eval_batch_size': 1000, 'num_negs': 1}


def print_model_params(model):
    print(model)
    for name, param in model.named_parameters():
        if param.requires_grad:
            print(name)
            print(param.data.shape)


def get_num_samples(sample_arg, dataset_len):
    """
    convert a sample arg that can be a number of % into a number of samples
    :param sample_arg: float interpreted as % if < 1 or count if >= 1
    :param dataset_len: the number of data points before sampling
    :return:
    """
    if sample_arg < 1:
        samples = int(sample_arg * dataset_len)
    else:
        samples = int(min(sample_arg, dataset_len))
    return samples


def select_embedding(args, num_nodes, device):
    """
    select a node embedding. Used by SEAL models (the E in SEAL is for Embedding)
    and needed for ogb-ddi where there are no node features
    :param args: Namespace of cmd args
    :param num_nodes: Int number of nodes to produce embeddings for
    :param device: cpu or cuda
    :return: Torch.nn.Embedding [n_nodes, args.hidden_channels]
    """
    if args.train_node_embedding:
        emb = torch.nn.Embedding(num_nodes, args.hidden_channels).to(device)
    elif args.pretrained_node_embedding:
        weight = torch.load(args.pretrained_node_embedding)
        emb = torch.nn.Embedding.from_pretrained(weight)
        emb.weight.requires_grad = False
    else:
        emb = None
    return emb


def get_pos_neg_edges(data, sample_frac=1):
    """
    extract the positive and negative supervision edges (as opposed to message passing edges) from data that has been
     transformed by RandomLinkSplit
    :param data: A train, val or test split returned by RandomLinkSplit
    :return: positive edge_index, negative edge_index.
    """
    device = data.edge_index.device
    edge_index = data['edge_label_index'].to(device)
    labels = data['edge_label'].to(device)
    pos_edges = edge_index[:, labels == 1].t()
    neg_edges = edge_index[:, labels == 0].t()
    if sample_frac != 1:
        n_pos = pos_edges.shape[0]
        np.random.seed(123)
        perm = np.random.permutation(n_pos)
        perm = perm[:int(sample_frac * n_pos)]
        pos_edges = pos_edges[perm, :]
        neg_edges = neg_edges[perm, :]
    return pos_edges.to(device), neg_edges.to(device)


def get_same_source_negs(num_nodes, num_negs_per_pos, pos_edge):
    """
    The ogb-citation datasets uses negatives with the same src, but different dst to the positives
    :param num_nodes: Int node count
    :param num_negs_per_pos: Int
    :param pos_edge: Int Tensor[2, edges]
    :return: Int Tensor[2, edges]
    """
    print(f'generating {num_negs_per_pos} single source negatives for each positive source node')
    dst_neg = torch.randint(0, num_nodes, (1, pos_edge.size(1) * num_negs_per_pos), dtype=torch.long)
    src_neg = pos_edge[0].repeat_interleave(num_negs_per_pos)
    return torch.cat([src_neg.unsqueeze(0), dst_neg], dim=0)


def neighbors(fringe, A, outgoing=True):
    """
    Retrieve neighbours of nodes within the fringe
    :param fringe: set of node IDs
    :param A: scipy CSR sparse adjacency matrix
    :param outgoing: bool
    :return:
    """
    if outgoing:
        res = set(A[list(fringe)].indices)
    else:
        res = set(A[:, list(fringe)].indices)

    return res


def get_src_dst_degree(src, dst, A, max_nodes):
    """
    Assumes undirected, unweighted graph
    :param src: Int Tensor[edges]
    :param dst: Int Tensor[edges]
    :param A: scipy CSR adjacency matrix
    :param max_nodes: cap on max node degree
    :return:
    """
    src_degree = A[src].sum() if (max_nodes is None or A[src].sum() <= max_nodes) else max_nodes
    dst_degree = A[dst].sum() if (max_nodes is None or A[src].sum() <= max_nodes) else max_nodes
    return src_degree, dst_degree


def str2bool(x):
    """
    hack to allow wandb to tune boolean cmd args
    :param x: str of bool
    :return: bool
    """
    if type(x) == bool:
        return x
    elif type(x) == str:
        return bool(strtobool(x))
    else:
        raise ValueError(f'Unrecognised type {type(x)}')

</file>

<file path="lp/Buddy/src/wandb_setup.py">
"""
configuring wandb settings
"""
import os
from copy import deepcopy

import wandb


def initialise_wandb(args, config=None):
    opt = deepcopy(vars(args))
    if config:
        opt.update(config)
    if args.wandb:
        if opt['use_wandb_offline']:
            os.environ["WANDB_MODE"] = "offline"
        else:
            os.environ["WANDB_MODE"] = "run"
        if 'wandb_run_name' in opt.keys():
            wandb.init(entity=opt['wandb_entity'], project=opt['wandb_project'], group=opt['wandb_group'],
                       name=opt['wandb_run_name'], reinit=True, config=opt)
        else:
            wandb.init(entity=opt['wandb_entity'], project=opt['wandb_project'], group=opt['wandb_group'],
                       reinit=True, config=opt)

        wandb.define_metric("epoch_step")  # Customize axes - https://docs.wandb.ai/guides/track/log
        if opt['wandb_track_grad_flow']:
            wandb.define_metric("grad_flow_step")  # Customize axes - https://docs.wandb.ai/guides/track/log
            wandb.define_metric("gf_e*", step_metric="grad_flow_step")  # grad_flow_epoch*

        return wandb.config  # access all HPs through wandb.config, so logging matches execution!

    else:
        os.environ["WANDB_MODE"] = "disabled"  # sets as NOOP, saves keep writing: if opt['wandb']:
        return args

</file>

<file path="lp/Buddy/test/__init__.py">

</file>

<file path="lp/Buddy/test/test_buddy.py">
"""
tests for the BUDDY model
"""

import unittest
from argparse import Namespace

from torch_geometric.utils import add_self_loops, to_undirected
from torch_geometric.utils.random import barabasi_albert_graph
from torch.utils.data import DataLoader
import torch
import scipy.sparse as ssp

from src.runners.inference import get_buddy_preds
from src.runners.run import run
from src.runners.train import train_buddy
from test_params import OPT, setup_seed
from src.models.elph import BUDDY
from src.datasets.elph import get_src_dst_degree
from src.data import get_data, get_hashed_train_val_test_datasets

class BUDDYTests(unittest.TestCase):
    def setUp(self):
        self.n_nodes = 30
        degree = 5
        self.num_features = 3
        self.x = torch.rand((self.n_nodes, self.num_features))
        edge_index = barabasi_albert_graph(self.n_nodes, degree)
        edge_index = to_undirected(edge_index)
        self.edge_index, _ = add_self_loops(edge_index)
        edge_weight = torch.ones(self.edge_index.size(1), dtype=int)
        self.A = ssp.csr_matrix((edge_weight, (self.edge_index[0], self.edge_index[1])),
                                shape=(self.n_nodes, self.n_nodes))
        self.args = Namespace(**OPT)
        self.args.model = 'BUDDY'
        setup_seed(0)

    def test_propagate(self):
        batch_size = 10
        num_features = self.x.shape[1]
        hash_hops = 2
        args = self.args
        args.sign_k = 0
        args.max_hash_hops = hash_hops
        sgf = torch.rand((batch_size, hash_hops * (hash_hops + 2)))
        gnn = BUDDY(args, num_features=num_features)
        x = torch.rand((batch_size, 2, self.num_features))
        x = gnn(sgf, x)
        self.assertTrue(len(x) == batch_size)
        # todo finish tests

    def test_degrees(self):
        src = 1
        dst = 3
        src_degree, dst_degree = get_src_dst_degree(src, dst, self.A, max_nodes=1000)
        degrees = torch.tensor(self.A.sum(axis=0, dtype=float)).flatten()
        self.assertTrue(src_degree == degrees[src])
        self.assertTrue(dst_degree == degrees[dst])

    def test_get_buddy_preds(self):
        opt = {'dataset_name': 'Cora', 'cache_subgraph_features': True, 'bulk_test': True}
        opt = {**OPT, **opt}
        args = Namespace(
            **opt)
        dataset, splits, _, _ = get_data(args)
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        train_data, val_data, test_data = splits['train'], splits['valid'], splits['test']
        # make a model
        model = BUDDY(args, dataset.num_features)
        # make a hashing dataset
        train_dataset, val_dataset, test_dataset = get_hashed_train_val_test_datasets(dataset, train_data, val_data,
                                                                                      test_data, args)
        test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False,
                                 num_workers=args.num_workers)
        pos_pred, neg_pred, pred, labels = get_buddy_preds(model, test_loader, device, args=args, split='test')
        self.assertTrue(len(pos_pred) + len(neg_pred) == len(test_dataset.links))

    def test_train_buddy(self):
        opt = {'dataset_name': 'Cora', 'cache_subgraph_features': True, 'use_RA': True}
        opt = {**OPT, **opt}
        args = Namespace(
            **opt)
        dataset, splits, _, _ = get_data(args)
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        train_data, val_data, test_data = splits['train'], splits['valid'], splits['test']

        # make a model
        model = BUDDY(args, dataset.num_features)
        # make a hashing dataset
        train_dataset, val_dataset, test_dataset = get_hashed_train_val_test_datasets(dataset, train_data, val_data,
                                                                                      test_data, args)
        link_loader = DataLoader(range(len(train_dataset.links)), args.batch_size, shuffle=True)
        for batch_count, indices in enumerate(link_loader):
            curr_links = torch.tensor(train_dataset.links[indices])
            subgraph_features = train_dataset.subgraph_features[indices].to(device)
            node_features = train_dataset.x[curr_links].to(device)
            degrees = train_dataset.degrees[curr_links].to(device)
            if args.use_RA:
                RA = train_dataset.RA[indices].to(device)
            else:
                RA = None
            logits = model(subgraph_features, node_features, degrees[:, 0], degrees[:, 1], RA)
            self.assertTrue(len(logits), args.batch_size)
            if (batch_count + 1) * args.batch_size >= 32:
                break

        parameters = list(model.parameters())
        optimizer = torch.optim.Adam(params=parameters, lr=args.lr, weight_decay=args.weight_decay)
        train_loader = DataLoader(train_dataset, batch_size=args.batch_size,
                          shuffle=True, num_workers=args.num_workers)
        loss = train_buddy(model, optimizer, train_loader, args, device, emb=None)
        self.assertTrue(loss > 0)

    def test_feature_forward(self):
        pass

    def test_embedding_forward(self):
        pass

    def test_forward(self):
        pass

    def test_run(self):
        # no exceptions is a pass
        self.args.train_samples = 0.1
        self.args.epochs = 1
        self.args.dataset_name = 'Cora'
        run(self.args)

</file>

<file path="lp/Buddy/test/test_data.py">
"""
testing data reader and preprocessing
"""
import unittest
import os
from argparse import Namespace

import torch
from torch import tensor
from torch_geometric.data import Data
from torch_geometric.utils import to_undirected, is_undirected, negative_sampling, from_networkx
from ogb.linkproppred import PygLinkPropPredDataset
import networkx as nx

from src.data import get_data, get_ogb_train_negs, make_obg_supervision_edges, get_ogb_data, get_loaders, \
    sample_hard_negatives
from src.utils import ROOT_DIR, get_pos_neg_edges
from test_params import OPT


class DataTests(unittest.TestCase):
    def setUp(self):
        self.edge_index = tensor([[0, 2, 2, 1], [1, 0, 1, 2]]).t()
        self.edge_weight = torch.ones(self.edge_index.size(0), dtype=int)
        self.test_edges = tensor([[0, 1], [1, 2]]).t()
        self.num_nodes = 3
        self.neg_test_edges = tensor([[0, 1], [2, 0]]).t()
        edges = 8
        self.train = {'edge': torch.randint(0, edges, size=(edges, 2))}
        self.valid = {'edge': torch.randint(0, edges, size=(edges, 2)), 'edge_neg': torch.randint(0, edges, size=(edges, 2))}
        self.test = {'edge': torch.randint(0, edges, size=(edges, 2)), 'edge_neg': torch.randint(0, edges, size=(edges, 2))}
        self.split_edge = {'train': self.train, 'valid': self.valid, 'test': self.test}

    def test_make_obg_supervision_edges(self):
        labels, edges = make_obg_supervision_edges(self.split_edge, 'test', neg_edges=None)
        n_pos, n_neg = self.test['edge'].shape[0], self.test['edge_neg'].shape[0]
        n_edges = self.train['edge'].shape[0] + self.valid['edge'].shape[0]
        self.assertTrue(labels.shape[0] == n_pos + n_neg)
        self.assertTrue(edges.shape == (2, n_edges))

    def test_get_ogb_data(self):
        ei = torch.randint(0, 10, size=(2, 20))
        ew = torch.ones(10)
        self.valid['weight'] = ew
        x = torch.rand(size=(10, 3))
        data = Data(x=x, edge_index=ei, edge_weight=ew.unsqueeze(-1))
        splits = get_ogb_data(data, self.split_edge, dataset_name='test_dataset')
        self.assertTrue('test' in splits.keys())
        self.assertTrue(len(splits.keys()) == 3)
        self.assertTrue(torch.all(torch.eq(splits['train'].edge_index, splits['valid'].edge_index)))
        self.assertTrue(torch.all(torch.eq(splits['test'].x, splits['valid'].x)))

    def test_use_val_edges(self):
        """
        to get good performance on ogb data, the val edges need to be used at test time. However, the val edges don't come
        undirected
        @return:
        """
        name = 'ogbl-collab'
        path = os.path.join(ROOT_DIR, 'dataset', name)
        dataset = PygLinkPropPredDataset(name=name, root=path)
        split_edge = dataset.get_edge_split()
        val_edges = split_edge['valid']['edge']
        self.assertTrue(not is_undirected(val_edges))  # no reason for supervision edges to be directed

    def test_get_data(self):
        """
        We use the pyg RandomLinkSplit object to create train / val / test splits. For link prediction edges play 2 roles
        1/ for message passing 2/ as supervision
        :return:
        """
        opt = {'sample_size': None, 'dataset_name': 'Cora', 'num_hops': 2, 'max_dist': 10, 'max_nodes_per_hop': 10,
               'data_appendix': None, 'val_pct': 0.1, 'test_pct': 0.2, 'train_sample': 1, 'dynamic_train': True,
               'dynamic_val': True, 'model': 'linear', 'dynamic_test': True, 'node_label': 'drnl', 'ratio_per_hop': 1}
        opt = {**OPT, **opt}
        args = Namespace(**opt)
        dataset, splits, directed, eval_metric = get_data(args)
        train, val, test = splits['train'], splits['valid'], splits['test']
        train_pos_edges, train_neg_edges = get_pos_neg_edges(train)
        # the default behaviour is 1 negative edge for each positive edge
        self.assertTrue(train_pos_edges.shape == train_neg_edges.shape)
        val_pos_edges, val_neg_edges = get_pos_neg_edges(val)
        self.assertTrue(val_pos_edges.shape == val_neg_edges.shape)
        test_pos_edges, test_neg_edges = get_pos_neg_edges(test)
        self.assertTrue(test_pos_edges.shape == test_neg_edges.shape)

    def test_get_ogb_train_negs(self):
        num_nodes = 10
        ei = torch.randint(0, 10, size=(2, 20))
        split_edge = self.split_edge
        train_negs = get_ogb_train_negs(split_edge, ei, num_nodes)
        self.assertTrue(train_negs.shape == split_edge['train']['edge'].shape)
        train_negs = get_ogb_train_negs(split_edge, ei, num_nodes, dataset_name='ogbl-citation2')
        # in this case the src node for the neg and pos edges should be the same
        self.assertTrue(torch.all(torch.eq(train_negs[:, 0], self.train['edge'][:, 0])))
        train_negs = get_ogb_train_negs(split_edge, ei, num_nodes, num_negs=2, dataset_name='ogbl-citation2')
        self.assertTrue(train_negs.shape[0] == 2 * split_edge['train']['edge'].shape[0])
        self.assertTrue(train_negs[0][0] == train_negs[1][0])

    def test_get_loaders(self):
        opt = {'sample_size': None, 'dataset_name': 'Cora', 'num_hops': 2, 'max_dist': 10, 'max_nodes_per_hop': 10,
               'data_appendix': None, 'val_pct': 0.1, 'test_pct': 0.2, 'train_sample': 1, 'dynamic_train': True,
               'dynamic_val': True, 'model': 'linear', 'dynamic_test': True, 'node_label': 'drnl', 'ratio_per_hop': 1}
        opt = {**OPT, **opt}
        args = Namespace(**opt)
        dataset, splits, directed, eval_metric = get_data(args)
        train_loader, train_eval_loader, val_loader, test_loader = get_loaders(args, dataset, splits, directed)
        # todo finish writing this test

    def test_sample_hard_negatives(self):
        grid_size = 4  # For a 4x4 grid
        G = nx.grid_2d_graph(grid_size, grid_size)
        data = from_networkx(G)
        edge_index = data.edge_index  # this is undirected
        reg_negs = negative_sampling(edge_index, num_nodes=len(G.nodes), num_neg_samples=10)
        sample_hard_negatives(edge_index, len(G.nodes))
        negs = sample_hard_negatives(self.edge_index, len(G.nodes))

</file>

<file path="lp/Buddy/test/test_elph.py">
import unittest
from argparse import Namespace

from torch_geometric.utils import add_self_loops, to_undirected
from torch_geometric.data import Data
from torch_geometric.utils.random import barabasi_albert_graph
import torch
from torch.utils.data import DataLoader
import scipy.sparse as ssp

from src.runners.train import train_elph
from src.runners.inference import get_elph_preds
from src.runners.run import run
from test_params import OPT, setup_seed
from src.models.elph import ELPH, LinkPredictor
from src.utils import ROOT_DIR, select_embedding
from src.datasets.elph import HashDataset


class ELPHTests(unittest.TestCase):
    def setUp(self):
        self.n_nodes = 30
        degree = 5
        self.num_features = 3
        self.x = torch.rand((self.n_nodes, self.num_features))
        edge_index = barabasi_albert_graph(self.n_nodes, degree)
        edge_index = to_undirected(edge_index)
        self.edge_index, _ = add_self_loops(edge_index)
        edge_weight = torch.ones(self.edge_index.size(1), dtype=int)
        self.A = ssp.csr_matrix((edge_weight, (self.edge_index[0], self.edge_index[1])),
                                shape=(self.n_nodes, self.n_nodes))
        self.args = Namespace(**OPT)
        self.args.model = 'ELPH'
        setup_seed(0)

    def test_propagate(self):
        num_features = self.x.shape[1]
        hash_hops = 2
        args = self.args
        args.max_hash_hops = hash_hops
        gnn = ELPH(args, num_features=num_features)
        x, hashes, cards = gnn(self.x, self.edge_index)
        self.assertTrue(x.shape == (self.n_nodes, args.hidden_channels))
        self.assertTrue(gnn.node_embedding is None)
        self.assertTrue(len(hashes) == hash_hops + 1)
        self.assertTrue(cards.shape == (self.n_nodes, hash_hops))
        self.assertTrue(hashes[0]['hll'].shape == (self.n_nodes, 2 ** args.hll_p))
        self.assertTrue(hashes[0]['minhash'].shape == (self.n_nodes, args.minhash_num_perm))
        args.train_node_embedding = True
        emb = select_embedding(args, self.n_nodes, self.x.device)
        gnn = ELPH(args, num_features=num_features, node_embedding=emb)
        x, hashes, cards = gnn(self.x, self.edge_index)
        self.assertTrue(gnn.node_embedding.weight.shape == (self.n_nodes, args.hidden_channels))
        # not ideal, but currently still propagate features even when we're not using them
        args.use_feature = False
        emb = select_embedding(args, self.n_nodes, self.x.device)
        gnn = ELPH(args, num_features=num_features, node_embedding=emb)
        x, hashes, cards = gnn(self.x, self.edge_index)
        self.assertTrue(x is None)
        # args.feature_prop = 'cat'
        # emb = select_embedding(args, self.n_nodes, self.x.device)
        # gnn = ELPH(args, num_features=num_features, node_embedding=emb)
        # x, hashes, cards = gnn(self.x, self.edge_index)
        # self.assertTrue(gnn.node_embedding.weight.shape == (self.n_nodes, (1 + gnn.num_layers) * args.hidden_channels))
        # args.use_feature = True
        # emb = select_embedding(args, self.n_nodes, self.x.device)
        # gnn = ELPH(args, num_features=num_features, node_embedding=emb)
        # x, hashes, cards = gnn(self.x, self.edge_index)
        # # self.assertTrue(gnn.node_embedding.weight.shape == (self.n_nodes, (1 + gnn.num_layers) * args.hidden_channels))
        # self.assertTrue(x.shape == (self.n_nodes, (1 + gnn.num_layers) * args.hidden_channels))

    def test_model_forward(self):
        n_links = 10
        num_features = self.x.shape[1]
        gnn = ELPH(self.args, num_features)
        x, hashes, cards = gnn(self.x, self.edge_index)
        links = torch.randint(self.n_nodes, (n_links, 2))
        sgf = gnn.elph_hashes.get_subgraph_features(links, hashes, cards)
        out = gnn.predictor(sgf, x[links])
        self.assertTrue(len(out) == n_links)

    def test_link_predictor(self):
        n_links = 10
        num_features = self.x.shape[1]
        args = self.args
        predictor = LinkPredictor(args)
        gnn = ELPH(args, num_features)
        x, hashes, cards = gnn(self.x, self.edge_index)
        links = torch.randint(self.n_nodes, (n_links, 2))
        sf = gnn.elph_hashes.get_subgraph_features(links, hashes, cards)
        out = gnn.predictor(sf, x[links])
        self.assertTrue(len(out) == n_links)
        out1 = predictor(sf, x[links])
        self.assertTrue(len(out1) == n_links)

    def test_train_gnn(self):
        n_links = 10
        num_features = self.x.shape[1]
        hash_hops = 2
        self.args.max_hash_hops = hash_hops
        args = self.args
        gnn = ELPH(self.args, num_features)
        parameters = list(gnn.parameters())
        device = self.x.device
        optimizer = torch.optim.Adam(params=parameters, lr=args.lr, weight_decay=args.weight_decay)
        data = Data(self.x, self.edge_index)
        root = f'{ROOT_DIR}/test/train_HashedDynamicDataset'
        split = 'train'
        pos_edges = torch.randint(self.n_nodes, (n_links, 2))
        neg_edges = torch.randint(self.n_nodes, (n_links, 2))
        hdd = HashDataset(root, split, data, pos_edges, neg_edges, args, use_coalesce=False, directed=False)
        dl = DataLoader(hdd, batch_size=1,
                        shuffle=False, num_workers=1)
        loss = train_elph(gnn, optimizer, dl, args, device)
        self.assertTrue(type(loss) == float)
        # try with embeddings
        args.train_node_embedding = True
        emb = select_embedding(args, self.n_nodes, self.x.device)
        gnn = ELPH(args, num_features, emb)
        self.assertTrue(gnn.node_embedding.weight.shape == (self.n_nodes, args.hidden_channels))
        loss = train_elph(gnn, optimizer, dl, args, device)
        self.assertTrue(type(loss) == float)
        # now check the propagation of embeddings also works
        args.propagate_embeddings = True
        gnn = ELPH(args, num_features, emb)
        loss = train_elph(gnn, optimizer, dl, args, device)
        self.assertTrue(type(loss) == float)
        # try without features
        args.use_feature = False
        gnn = ELPH(args, num_features, emb)
        loss = train_elph(gnn, optimizer, dl, args, device)
        self.assertTrue(type(loss) == float)
        # and now residual
        args.feature_prop = 'residual'
        gnn = ELPH(args, num_features, emb)
        loss = train_elph(gnn, optimizer, dl, args, device)
        self.assertTrue(type(loss) == float)
        # and jk / cat
        args.feature_prop = 'cat'
        gnn = ELPH(args, num_features, emb)
        loss = train_elph(gnn, optimizer, dl, args, device)
        self.assertTrue(type(loss) == float)

    def test_get_gnn_preds(self):
        n_links = 10
        num_features = self.x.shape[1]
        hash_hops = 2
        self.args.max_hash_hops = hash_hops
        args = self.args
        gnn = ELPH(args, num_features)
        gnn.eval()
        data = Data(self.x, self.edge_index)
        root = f'{ROOT_DIR}/test/test_HashedDynamicDataset'
        split = 'train'
        pos_edges = torch.randint(self.n_nodes, (n_links, 2))
        neg_edges = torch.randint(self.n_nodes, (n_links, 2))
        hdd = HashDataset(root, split, data, pos_edges, neg_edges, args, use_coalesce=False, directed=False)
        dl = DataLoader(hdd, batch_size=1, shuffle=False, num_workers=1)
        pos_pred, neg_pred, pred, labels = get_elph_preds(gnn, dl, self.x.device, args, split='test')
        self.assertTrue(len(pos_pred == len(pos_edges)))
        self.assertTrue(len(neg_pred == len(neg_edges)))
        self.assertTrue(len(neg_pred == len(neg_edges) + len(pos_edges)))
        self.assertTrue(torch.sum(labels) == len(pos_edges))
        # try with embeddings
        args.train_node_embedding = True
        emb = select_embedding(args, self.n_nodes, self.x.device)
        gnn = ELPH(args, num_features, emb)
        self.assertTrue(gnn.node_embedding.weight.shape == (self.n_nodes, args.hidden_channels))
        pos_pred, neg_pred, pred, labels = get_elph_preds(gnn, dl, self.x.device, args, split='test')
        self.assertTrue(len(pos_pred == len(pos_edges)))
        self.assertTrue(len(neg_pred == len(neg_edges)))
        self.assertTrue(len(neg_pred == len(neg_edges) + len(pos_edges)))
        self.assertTrue(torch.sum(labels) == len(pos_edges))
        # now check the propagation of embeddings also works
        args.propagate_embeddings = True
        gnn = ELPH(args, num_features, emb)
        pos_pred, neg_pred, pred, labels = get_elph_preds(gnn, dl, self.x.device, args, split='test')
        self.assertTrue(len(pos_pred == len(pos_edges)))
        self.assertTrue(len(neg_pred == len(neg_edges)))
        self.assertTrue(len(neg_pred == len(neg_edges) + len(pos_edges)))
        self.assertTrue(torch.sum(labels) == len(pos_edges))
        # w/o features
        args.use_feature = False
        gnn = ELPH(args, num_features, emb)
        pos_pred, neg_pred, pred, labels = get_elph_preds(gnn, dl, self.x.device, args, split='test')
        self.assertTrue(len(pos_pred == len(pos_edges)))
        self.assertTrue(len(neg_pred == len(neg_edges)))
        self.assertTrue(len(neg_pred == len(neg_edges) + len(pos_edges)))
        self.assertTrue(torch.sum(labels) == len(pos_edges))
        # residual
        args.feauture_prop = 'residual'
        gnn = ELPH(args, num_features, emb)
        pos_pred, neg_pred, pred, labels = get_elph_preds(gnn, dl, self.x.device, args, split='test')
        self.assertTrue(len(pos_pred == len(pos_edges)))
        self.assertTrue(len(neg_pred == len(neg_edges)))
        self.assertTrue(len(neg_pred == len(neg_edges) + len(pos_edges)))
        self.assertTrue(torch.sum(labels) == len(pos_edges))
        # cat / jk
        args.feauture_prop = 'cat'
        gnn = ELPH(args, num_features, emb)
        pos_pred, neg_pred, pred, labels = get_elph_preds(gnn, dl, self.x.device, args, split='test')
        self.assertTrue(len(pos_pred == len(pos_edges)))
        self.assertTrue(len(neg_pred == len(neg_edges)))
        self.assertTrue(len(neg_pred == len(neg_edges) + len(pos_edges)))
        self.assertTrue(torch.sum(labels) == len(pos_edges))

    def test_run(self):
        # no exceptions is a pass
        self.args.train_samples = 0.1
        self.args.epochs = 1
        self.args.dataset_name = 'Cora'
        run(self.args)

</file>

<file path="lp/Buddy/test/test_elph_datasets.py">
"""
Testing the subgraph sketching dataset objects
"""
import unittest
from argparse import Namespace
import os

import torch
from torch_geometric.data import Data
from torch.utils.data import DataLoader
from torch_geometric.utils.random import barabasi_albert_graph
import scipy.sparse as ssp

from src.datasets.elph import HashDataset, make_train_eval_data
from test_params import OPT
from src.utils import ROOT_DIR, get_same_source_negs
from src.hashing import ElphHashes


class ELPHDatasetTests(unittest.TestCase):
    def setUp(self):
        self.n_nodes = 30
        degree = 5
        self.n_edges = 10  # number of positive training edges
        self.x = torch.rand((self.n_nodes, 2))
        self.edge_index = barabasi_albert_graph(self.n_nodes, degree)
        self.edge_weight = torch.ones(self.edge_index.shape[1])
        self.A = ssp.csr_matrix((self.edge_weight, (self.edge_index[0], self.edge_index[1])),
                                shape=(self.n_nodes, self.n_nodes))
        self.pos_edges = torch.randint(self.n_nodes, (self.n_edges, 2))
        self.neg_edges = torch.randint(self.n_nodes, (self.n_edges, 2))

        self.args = Namespace(**OPT)

    def test_HashedDynamicDataset(self):
        torch.manual_seed(0)
        self.args.model = 'BUDDY'
        split = 'test'
        ei = self.edge_index
        data = Data(self.x, ei)
        root = f'{ROOT_DIR}/test/dataset/test_HashedDynamicDataset'
        hash_name = f'{root}{split}_hashcache.pt'
        cards_name = f'{root}{split}_cardcache.pt'
        eh = ElphHashes(self.args)
        if os.path.exists(hash_name) and os.path.exists(cards_name):
            hashes = torch.load(hash_name)
            cards = torch.load(cards_name)
        else:
            hashes, cards = eh.build_hash_tables(self.n_nodes, self.edge_index)
            torch.save(hashes, hash_name)
            torch.save(cards, cards_name)
        all_edges = torch.cat([self.pos_edges, self.neg_edges], 0)
        # construct features directly from hashes and cards
        subgraph_features = eh.get_subgraph_features(all_edges, hashes, cards)
        # construct features implicitly (hopefully) using the same hashes and cards
        hdd = HashDataset(root, split, data, self.pos_edges, self.neg_edges, self.args, use_coalesce=False,
                          directed=False)
        self.assertTrue(hdd.links.shape == (2 * self.n_edges, 2))
        self.assertTrue(len(hdd.labels) == 2 * self.n_edges)
        self.assertTrue(len(hdd.edge_weight) == self.edge_index.shape[1])

        dl = DataLoader(hdd, batch_size=1,
                        shuffle=False, num_workers=1)
        # check the dataset has the same features
        for sf, elem in zip(subgraph_features, dl):
            sf_test = elem[0]
            self.assertTrue(torch.all(torch.eq(sf, sf_test)))

    def test_get_subgraph_features_batched(self):
        batch_size = 3
        torch.manual_seed(0)
        self.args.model = 'BUDDY'
        split = 'test'
        ei = self.edge_index
        root = f'{ROOT_DIR}/test/dataset/test_HashedDynamicDataset'
        hash_name = f'{root}{split}_hashcache.pt'
        cards_name = f'{root}{split}_cardcache.pt'
        eh = ElphHashes(self.args)
        if os.path.exists(hash_name) and os.path.exists(cards_name):
            hashes = torch.load(hash_name)
            cards = torch.load(cards_name)
        else:
            hashes, cards = eh.build_hash_tables(self.n_nodes, self.edge_index)
            torch.save(hashes, hash_name)
            torch.save(cards, cards_name)
        all_edges = torch.cat([self.pos_edges, self.neg_edges], 0)
        subgraph_features = eh.get_subgraph_features(all_edges, hashes, cards, batch_size=batch_size)
        self.assertTrue(
            subgraph_features.shape == (len(all_edges), self.args.max_hash_hops * (self.args.max_hash_hops + 2)))
        sf2 = eh.get_subgraph_features(all_edges, hashes, cards)
        self.assertTrue(torch.all(torch.eq(subgraph_features, sf2)))

    def test_preprocess_features(self):
        pass

    def test_read_subgraph_features(self):
        pass

    def test_preprocess_subgraph_features(self):
        root = f'{ROOT_DIR}/test/dataset/test_HashedDynamicDataset'
        split = 'train'
        ei = self.edge_index
        data = Data(self.x, ei)
        hdd = HashDataset(root, split, data, self.pos_edges, self.neg_edges, self.args, use_coalesce=False,
                          directed=False,
                          load_features=True)

    def test_make_train_eval_dataset(self):
        self.args.max_hash_hops = 2
        torch.manual_seed(0)
        n_pos_samples = 8
        negs_per_pos = 10
        n_pos_edges = 10
        pos_edges = torch.randint(self.n_nodes, (n_pos_edges, 2))
        neg_edges = get_same_source_negs(self.n_nodes, negs_per_pos, pos_edges.t()).t()
        # neg_edges = torch.randint(self.n_nodes, (n_pos_edges * negs_per_pos, 2))
        split = 'train'
        ei = self.edge_index
        data = Data(self.x, ei)
        root = f'{ROOT_DIR}/test/dataset/test_HashedDynamicDataset'
        self.args.dataset_name = 'test_dataset'
        hdd = HashDataset(root, split, data, pos_edges, neg_edges, self.args, use_coalesce=False,
                          directed=False)
        train_eval_dataset = make_train_eval_data(self.args, hdd, self.n_nodes, n_pos_samples=n_pos_samples,
                                                  negs_per_pos=negs_per_pos)
        self.assertTrue(len(train_eval_dataset.links) == (negs_per_pos + 1) * n_pos_samples)
        self.assertTrue(len(train_eval_dataset.labels) == (negs_per_pos + 1) * n_pos_samples)
        self.assertTrue(len(train_eval_dataset.subgraph_features) == (negs_per_pos + 1) * n_pos_samples)
        self.args.use_RA = True
        hdd = HashDataset(root, split, data, pos_edges, neg_edges, self.args, use_coalesce=False,
                          directed=False)
        train_eval_dataset = make_train_eval_data(self.args, hdd, self.n_nodes, n_pos_samples=n_pos_samples,
                                                  negs_per_pos=negs_per_pos)
        self.assertTrue(len(train_eval_dataset.RA) == (negs_per_pos + 1) * n_pos_samples)
        self.args.max_hash_hops = 3
        hdd = HashDataset(root, split, data, pos_edges, neg_edges, self.args, use_coalesce=False,
                          directed=False)
        train_eval_dataset = make_train_eval_data(self.args, hdd, self.n_nodes, n_pos_samples=n_pos_samples,
                                                  negs_per_pos=negs_per_pos)
        self.assertTrue(len(train_eval_dataset.links) == (negs_per_pos + 1) * n_pos_samples)
        self.assertTrue(len(train_eval_dataset.labels) == (negs_per_pos + 1) * n_pos_samples)
        self.assertTrue(len(train_eval_dataset.subgraph_features) == (negs_per_pos + 1) * n_pos_samples)
        self.args.use_RA = True
        hdd = HashDataset(root, split, data, pos_edges, neg_edges, self.args, use_coalesce=False,
                          directed=False)
        train_eval_dataset = make_train_eval_data(self.args, hdd, self.n_nodes, n_pos_samples=n_pos_samples,
                                                  negs_per_pos=negs_per_pos)
        self.assertTrue(len(train_eval_dataset.RA) == (negs_per_pos + 1) * n_pos_samples)
        # citation eval is with mrr against same source negatives. The code checks if the name starts wiht 'ogbl-citation'
        # and if so, generates the same source negatives
        hdd = HashDataset(root, split, data, pos_edges, neg_edges, self.args, use_coalesce=False,
                          directed=True)
        train_eval_dataset = make_train_eval_data(self.args, hdd, self.n_nodes, n_pos_samples=n_pos_samples,
                                                  negs_per_pos=negs_per_pos)
        self.assertTrue(len(train_eval_dataset.links) == (negs_per_pos + 1) * n_pos_samples)
        self.assertTrue(len(train_eval_dataset.labels) == (negs_per_pos + 1) * n_pos_samples)
        self.assertTrue(len(train_eval_dataset.subgraph_features) == (negs_per_pos + 1) * n_pos_samples)

</file>

<file path="lp/Buddy/test/test_hashing.py">
import unittest
from argparse import Namespace
import random
from math import isclose

from torch_geometric.nn import MessagePassing
from torch_geometric.utils.random import barabasi_albert_graph
from torch_geometric.utils import add_self_loops, to_undirected

import numpy as np
import scipy.sparse as ssp
import torch
from datasketch import MinHash, HyperLogLogPlusPlus

from src.hashing import ElphHashes, LABEL_LOOKUP
from src.datasets.seal import neighbors
from test_params import OPT, setup_seed


class HashingTests(unittest.TestCase):
    def setUp(self):
        self.n_nodes = 30
        self.n_edges = 100
        degree = 5  # number of edges to attach to each new node, not the degree at the end of the process
        self.x = torch.rand((self.n_nodes, 2))
        edge_index = barabasi_albert_graph(self.n_nodes, degree)
        edge_index = to_undirected(edge_index)
        self.edge_index, _ = add_self_loops(edge_index)
        edge_weight = torch.ones(self.edge_index.size(1), dtype=int)
        self.A = ssp.csr_matrix((edge_weight, (self.edge_index[0], self.edge_index[1])),
                                shape=(self.n_nodes, self.n_nodes))
        self.args = Namespace(**OPT)
        setup_seed(0)

    def test_minhash(self):
        src = [0]
        dst = [1]
        src_n = neighbors(src, self.A)
        dst_n = neighbors(dst, self.A)
        m1, m2 = MinHash(), MinHash()
        for elem in src_n:
            m1.update(elem)
        for elem in dst_n:
            m2.update(elem)
        print("Estimated Jaccard for data1 and data2 is", m1.jaccard(m2))
        jaccard = len(src_n.intersection(dst_n)) / len(src_n.union(dst_n))
        print(f'true Jaccard: {jaccard}')

    def test_hyperloglog(self):
        src = [0]
        dst = [1]
        src_n = list(neighbors(src, self.A))
        dst_n = list(neighbors(dst, self.A))
        union = src_n + dst_n
        hpp = HyperLogLogPlusPlus(p=8)
        for count, elem in enumerate(union):
            hpp.update(elem)
            if count % 2 == 0:
                print(f'estimate cardinality: {hpp.count()}')
        print(f'true size is {len(set(union))}')

    def test_build_hash_tables(self):
        eh = ElphHashes(self.args)
        eh.max_hops = 2
        hash_tables, cards = eh.build_hash_tables(self.n_nodes, self.edge_index)
        self.assertTrue(len(hash_tables[0]['minhash']) == self.n_nodes)
        self.assertTrue(len(hash_tables) == eh.max_hops + 1)
        eh.max_hops = 3
        hash_tables, cards = eh.build_hash_tables(self.n_nodes, self.edge_index)
        self.assertTrue(len(hash_tables[0]['minhash']) == self.n_nodes)
        self.assertTrue(len(hash_tables) == eh.max_hops + 1)

    def test_find_intersections(self):
        max_hops = 2
        self.args.max_hash_hops = max_hops
        eh = ElphHashes(self.args)
        hash_tables, cards = eh.build_hash_tables(self.n_nodes, self.edge_index)
        node1 = 0
        node2 = 1
        features = eh._get_intersections(torch.tensor([[node1, node2]]), hash_tables)
        self.assertTrue(len(features) == max_hops ** 2)
        max_hops = 3
        self.args.max_hash_hops = max_hops
        eh = ElphHashes(self.args)
        hash_tables, cards = eh.build_hash_tables(self.n_nodes, self.edge_index)
        features = eh._get_intersections(torch.tensor([[node1, node2]]), hash_tables)
        self.assertTrue(len(features) == max_hops ** 2)

    def test_find_neighbourhood_cardinality(self):
        # max_hops = 2
        eh = ElphHashes(self.args)
        hash_table, cards = eh.build_hash_tables(self.n_nodes, self.edge_index)
        node = 0
        # cards = find_neighbourhood_cardinality(node, hash_table, max_hops=2)
        neighbors1 = neighbors([node], self.A)
        neighbors2 = neighbors(neighbors1, self.A)
        n1, n2 = len(neighbors1) + 1, len(neighbors2) + 1  # node is counted by find_neighbourhood_cardinality
        self.assertTrue((cards[node, 0] > n1 - 1) and (cards[node, 0] < n1 + 1))
        self.assertTrue((cards[node, 1] > n2 - 2) and (cards[node, 1] < n2 + 2))

    def test_get_features(self):
        """
        These tests are stochastic, sometimes we get unlucky and one fails, which is generally nothing to worry about
        @return:
        """
        setup_seed(0)
        max_hops = 3
        self.args.max_hash_hops = max_hops
        #  some of these tests fail due to negative feature values, but this doesn't seem to be a problem for larger datasets
        self.args.floor_sf = True  # make subgraph features >= 0
        self.args.hll_p = 16  # choose a high value for more accuracy
        eh = ElphHashes(self.args)
        hash_tables, cards = eh.build_hash_tables(self.n_nodes, self.edge_index)
        node1 = 0
        node2 = 1
        # get the neigbhours<node_id,num_hops>. The cards include the central nodes, so need to add them here too
        neighbors11 = neighbors([node1], self.A).union({node1})
        neighbors21 = neighbors([node2], self.A).union({node2})
        neighbors12 = neighbors(neighbors11, self.A).union(neighbors11)
        neighbors22 = neighbors(neighbors21, self.A).union(neighbors21)
        neighbors13 = neighbors(neighbors12, self.A).union(neighbors12)
        neighbors23 = neighbors(neighbors22, self.A).union(neighbors22)
        # check the cardinality estimates
        cards1 = cards[node1].numpy()
        cards2 = cards[node2].numpy()
        # cards counts the central node whereas neighbours does not, so need to add 1
        self.assertAlmostEqual(cards1[0], len(neighbors11), 1)
        self.assertAlmostEqual(cards1[1], len(neighbors12), 1)
        self.assertAlmostEqual(cards1[2], len(neighbors13), 1)
        self.assertAlmostEqual(cards2[0], len(neighbors21), 1)
        self.assertAlmostEqual(cards2[1], len(neighbors22), 1)
        self.assertAlmostEqual(cards2[2], len(neighbors23), 1)

        tmp_features = eh.get_subgraph_features(torch.tensor([[node1, node2]]), hash_tables, cards).squeeze()
        self.assertTrue(torch.all(tmp_features >= 0))
        self.assertTrue(len(tmp_features) == max_hops * (max_hops + 2))
        # this is weird, but I changed features from dict -> tensor and this was the fastest way to change the tests
        features = {eh.label_lookup[idx]: val for idx, val in enumerate(tmp_features)}
        # test (1,1) features
        int11 = neighbors11.intersection(neighbors21)
        self.assertTrue(isclose(len(int11), features[(1, 1)], abs_tol=1))
        # test (2,1) features
        int21 = neighbors12.intersection(neighbors21)
        feat21 = int21.difference(int11)
        self.assertTrue(isclose(len(feat21), features[(2, 1)], abs_tol=1))
        # test (1,2) features
        int12 = neighbors11.intersection(neighbors22)
        feat12 = int12.difference(int11)
        self.assertTrue(isclose(len(feat12), features[(1, 2)], abs_tol=1))
        # test (2,2) features
        int22 = neighbors12.intersection(neighbors22)
        feat22 = int22.difference(feat12 | feat21 | int11)
        self.assertTrue(isclose(len(feat22), features[(2, 2)], abs_tol=2))
        # TEST ORDER 3 FROM HERE
        #  (3,1)
        int31 = neighbors13.intersection(neighbors21)
        feat31 = int31.difference(int11 | feat21)
        self.assertTrue(isclose(len(feat31), features[(3, 1)], abs_tol=1))
        # (1,3)
        int13 = neighbors11.intersection(neighbors23)
        feat13 = int13.difference(int11 | feat12)
        self.assertTrue(isclose(len(feat13), features[(1, 3)], abs_tol=1.5))
        # (3,2)
        int32 = neighbors13.intersection(neighbors22)
        feat32 = int32.difference(int11 | feat21 | feat12 | feat22 | feat31)
        self.assertTrue(isclose(len(feat32), features[(3, 2)], abs_tol=2))
        # (2, 3)
        int23 = neighbors12.intersection(neighbors23)
        feat23 = int23.difference(int11 | feat21 | feat12 | feat22 | feat13)
        self.assertTrue(isclose(len(feat23), features[(2, 3)], abs_tol=2))
        # (3, 3)
        int33 = neighbors13.intersection(neighbors23)
        feat33 = int33.difference(int11 | feat21 | feat12 | feat22 | feat31 | feat13 | feat23 | feat32)
        self.assertTrue(isclose(len(feat33), features[(3, 3)], abs_tol=2))
        # (0,1)
        feat01 = neighbors21.difference(int11 | feat21 | feat31)
        self.assertTrue(isclose(len(feat01), features[(0, 1)], abs_tol=2))

    def test_get_subgraph_features(self):
        max_hops = 2
        self.args.max_hash_hops = max_hops
        eh = ElphHashes(self.args)
        hashes, cards = eh.build_hash_tables(self.n_nodes, self.edge_index)
        links = torch.randint(self.n_nodes, (10, 2))
        sf = eh.get_subgraph_features(links, hashes, cards)
        self.assertTrue(sf.shape == torch.Size((len(links), len(LABEL_LOOKUP[2]))))
        for link, features in zip(links, sf):
            features_test = eh.get_subgraph_features(link, hashes, cards)
            self.assertTrue(torch.all(torch.eq(features, features_test)))
        sf[:, [4, 5]] = 0  # knock out the zero-one features
        for link, features in zip(links, sf):
            eh.use_zero_one = False
            features_test = eh.get_subgraph_features(link, hashes, cards)
            self.assertTrue(torch.all(torch.eq(features, features_test)))

    def test_label_lookup(self):
        for key, val in LABEL_LOOKUP.items():
            self.assertTrue(len(val) == key * (key + 2))

    def test_hll_p(self):
        max_hops = 2
        node1 = 0
        node2 = 1
        self.args.hll_p = 4
        n_links = 6
        eh = ElphHashes(self.args)
        hash_table, cards = eh.build_hash_tables(self.n_nodes, self.edge_index)
        intersections = eh._get_intersections(torch.tensor([[node1, node2]]), hash_table)
        self.assertTrue(len(intersections) == max_hops ** 2)
        cards1 = cards[node1]
        self.assertTrue(len(cards1) == max_hops)
        links = torch.randint(self.n_nodes, (n_links, 2))
        sf = eh.get_subgraph_features(links, hash_table, cards)
        self.assertTrue(sf.shape == (n_links, max_hops * (max_hops + 2)))

    def test_hll_counts(self):
        self.args.hll_p = 4
        eh = ElphHashes(self.args)
        n_registers = 10
        registers = torch.randint(high=2, size=(n_registers, 16))
        counts = eh.hll_count(registers)
        self.assertTrue(len(counts) == n_registers)
        hashes, cards = eh.build_hash_tables(self.n_nodes, self.edge_index)
        one_hop_counts = eh.hll_count(hashes[1]['hll'])
        two_hop_counts = eh.hll_count(hashes[2]['hll'])
        self.assertTrue(torch.allclose(cards[:, 0], one_hop_counts, atol=1e-8))
        self.assertTrue(torch.allclose(cards[:, 1], two_hop_counts, atol=1e-8))

    def test_refine_hll_count_estimate(self):
        eh = ElphHashes(self.args)
        n_links = 10
        estimates = torch.rand(n_links) + 5 * eh.m - 0.5  # make sure we have estimates above and below the threshold
        new_estimates = eh._refine_hll_count_estimate(estimates)
        self.assertTrue(new_estimates.shape == estimates.shape)
        idx = estimates > 5 * eh.m
        self.assertTrue(torch.allclose(new_estimates[idx], estimates[idx]))

    def test_jaccard(self):
        random.seed(0)
        self.args.max_hash_hops = 2
        eh = ElphHashes(self.args)
        hashes, cards = eh.build_hash_tables(self.n_nodes, self.edge_index)
        node1 = 0
        node2 = 1
        # get the neigbhours<node_id,num_hops>. The cards include the central nodes, so need to add them here too
        neighbors11 = neighbors([node1], self.A).union({node1})
        neighbors21 = neighbors([node2], self.A).union({node2})
        jaccard = len(neighbors11.intersection(neighbors21)) / len(neighbors11.union(neighbors21))
        jaccard_est = eh.jaccard(hashes[1]['minhash'][0], hashes[1]['minhash'][1])
        self.assertTrue(isclose(jaccard, jaccard_est, abs_tol=0.1))

    def test_intersections(self):
        setup_seed(0)
        self.args.max_hash_hops = 2
        eh = ElphHashes(self.args)
        hashes, cards = eh.build_hash_tables(self.n_nodes, self.edge_index)
        node1 = 0
        node2 = 1
        links = torch.tensor([[node1, node2], [node2, node1]])
        intersections = eh._get_intersections(links, hashes)
        self.assertTrue(len(intersections) == self.args.max_hash_hops ** 2)
        neighbors11 = neighbors([node1], self.A).union({node1})
        neighbors21 = neighbors([node2], self.A).union({node2})
        neighbors12 = neighbors(neighbors11, self.A).union(neighbors11)
        neighbors22 = neighbors(neighbors21, self.A).union(neighbors21)
        int11 = len(neighbors11.intersection(neighbors21))
        int22 = len(neighbors12.intersection(neighbors22))
        int21 = len(neighbors12.intersection(neighbors21))
        int12 = len(neighbors11.intersection(neighbors22))
        # (1,1)
        self.assertTrue(isclose(int11, intersections[(1, 1)][0], abs_tol=1))
        self.assertTrue(isclose(int11, intersections[(1, 1)][1], abs_tol=1))
        # (2,1)
        self.assertTrue(isclose(int21, intersections[(2, 1)][0], abs_tol=1))
        self.assertTrue(isclose(int21, intersections[(1, 2)][1], abs_tol=1))
        # (1,2)
        self.assertTrue(isclose(int12, intersections[(1, 2)][0], abs_tol=1))
        self.assertTrue(isclose(int12, intersections[(2, 1)][1], abs_tol=1))
        # (2,2)
        self.assertTrue(isclose(int22, intersections[(2, 2)][0], abs_tol=1))
        self.assertTrue(isclose(int22, intersections[(2, 2)][1], abs_tol=1))

    def test_ElphHashes(self):
        random.seed(0)
        self.args.max_hash_hops = 3
        eh = ElphHashes(self.args)
        hashes, cards = eh.build_hash_tables(self.n_nodes, self.edge_index)
        node1 = 0
        node2 = 1
        # get the neigbhours<node_id,num_hops>. The cards include the central nodes, so need to add them here too
        neighbors11 = neighbors([node1], self.A).union({node1})
        neighbors21 = neighbors([node2], self.A).union({node2})
        neighbors12 = neighbors(neighbors11, self.A).union(neighbors11)
        neighbors22 = neighbors(neighbors21, self.A).union(neighbors21)
        neighbors13 = neighbors(neighbors12, self.A).union(neighbors12)
        neighbors23 = neighbors(neighbors22, self.A).union(neighbors22)
        count11 = len(neighbors11)
        count21 = len(neighbors21)
        count12 = len(neighbors12)
        count22 = len(neighbors22)
        count13 = len(neighbors13)
        count23 = len(neighbors23)
        cards1 = cards[node1]
        cards2 = cards[node2]
        # cards counts the central node whereas neighbours does not, so need to add 1
        self.assertTrue(isclose(cards1[0].item(), count11, abs_tol=1))
        self.assertTrue(isclose(cards1[1].item(), count12, abs_tol=1.5))
        self.assertTrue(isclose(cards1[2].item(), count13, abs_tol=2))
        self.assertTrue(isclose(cards2[0].item(), count21, abs_tol=1))
        self.assertTrue(isclose(cards2[1].item(), count22, abs_tol=1.5))
        self.assertTrue(isclose(cards2[2].item(), count23, abs_tol=2))

    def test_neighbour_merge(self):
        max_hops = 2
        self.args.max_hash_hops = max_hops
        eh = ElphHashes(self.args)
        hashes, cards = eh.build_hash_tables(self.n_nodes, self.edge_index)
        node = 0
        neighbours = list(neighbors([node], self.A))
        root_reg = hashes[1]['hll'][node]
        root_hashvalues = hashes[1]['minhash'][node]
        regs = hashes[1]['hll'][neighbours]
        hashvalues = hashes[1]['minhash'][neighbours]
        two_hop_reg = eh.hll_neighbour_merge(root_reg, regs)
        two_hop_hashes = eh.minhash_neighbour_merge(root_hashvalues, hashvalues)
        true_reg = hashes[2]['hll'][node]
        true_hashvalues = hashes[2]['minhash'][node]
        self.assertTrue(torch.allclose(two_hop_reg, true_reg))
        self.assertTrue(torch.allclose(two_hop_hashes, true_hashvalues))

    def test_bit_length(self):
        eh = ElphHashes(self.args)
        arr = np.arange(1000)
        bit_lengths = eh._np_bit_length(arr)
        for bl, elem in zip(bit_lengths, arr):
            self.assertTrue(int(elem).bit_length() == bl)

    def test_initialise_hll(self):
        eh = ElphHashes(self.args)
        regs = eh.initialise_hll(self.n_nodes).numpy()
        self.assertTrue(regs.shape == (self.n_nodes, eh.m))
        self.assertTrue(np.array_equal(np.count_nonzero(regs, axis=1), np.ones(self.n_nodes)))
        self.assertTrue(np.amin(regs) >= 0)
        self.assertTrue(np.amax(regs) <= eh.max_rank)
        for node in range(self.n_nodes):
            self.assertTrue(np.isclose(eh.hll_count(torch.tensor(regs[node])).item(), 1, atol=0.1))

    def test_initialise_minhash(self):
        eh = ElphHashes(self.args)
        hashes = eh.initialise_minhash(self.n_nodes).numpy()
        self.assertTrue(hashes.shape == (self.n_nodes, eh.num_perm))
        self.assertTrue(np.amin(hashes) >= 0)
        self.assertTrue(np.amax(hashes) <= eh._max_minhash)

    def test_propagate_minhash(self):
        class MinPropagation(MessagePassing):
            def __init__(self):
                super().__init__(aggr='max')

            def forward(self, x, edge_index):
                out = self.propagate(edge_index, x=-x)
                return -out

        mp = MinPropagation()
        x = torch.tensor([[1, 2], [3, 1]])
        edge_index = torch.tensor([[0, 1, 0, 1], [0, 1, 1, 0]])
        out = mp(x, edge_index)
        self.assertTrue(torch.all(torch.eq(out, torch.tensor([[1, 1], [1, 1]]))))
        x = torch.tensor([[1, 2], [-3, 1]])
        out = mp(x, edge_index)
        self.assertTrue(torch.all(torch.eq(out, torch.tensor([[-3, 1], [-3, 1]]))))
        self.args.minhash_num_perm = 8
        self.args.hll_p = 4
        eh = ElphHashes(self.args)
        n_nodes = 2
        hashes = eh.initialise_minhash(n_nodes)
        edge_index = torch.tensor([[0, 1], [1, 0]], dtype=torch.long)
        edge_index, _ = add_self_loops(edge_index, num_nodes=n_nodes)
        propagated_hashes = eh.minhash_prop(hashes, edge_index)
        truth = torch.min(hashes, dim=0)[0].repeat(n_nodes, 1)
        self.assertTrue(torch.all(torch.eq(propagated_hashes, truth)))
        hlls = eh.initialise_hll(n_nodes)
        propagated_hll = eh.hll_prop(hlls, edge_index)
        truth = torch.max(hlls, dim=0)[0].repeat(n_nodes, 1)
        self.assertTrue(torch.all(torch.eq(propagated_hll, truth)))

    def test_subgraph_features(self):
        setup_seed(0)
        n_links = 100
        self.args.max_hash_hops = 2
        self.args.floor_sf = True  # make subgraph features >= 0
        self.args.hll_p = 16  # choose a high value for more accuracy
        eh = ElphHashes(self.args)
        hashes, cards = eh.build_hash_tables(self.n_nodes, self.edge_index)
        links = torch.randint(self.n_nodes, (n_links, 2))
        sf = eh.get_subgraph_features(links, hashes, cards)
        self.assertTrue(sf.shape == (n_links, self.args.max_hash_hops * (self.args.max_hash_hops + 2)))
        self.args.max_hash_hops = 3
        eh = ElphHashes(self.args)
        hashes, cards = eh.build_hash_tables(self.n_nodes, self.edge_index)
        sf = eh.get_subgraph_features(links, hashes, cards)
        self.assertTrue(sf.shape == (n_links, self.args.max_hash_hops * (self.args.max_hash_hops + 2)))
        links = torch.tensor([[0, 1]])
        tmp_features = eh.get_subgraph_features(links, hashes, cards)
        node1 = 0
        node2 = 1
        # get the neigbhours<node_id,num_hops>. The cards include the central nodes, so need to add them here too
        neighbors11 = neighbors([node1], self.A).union({node1})
        neighbors21 = neighbors([node2], self.A).union({node2})
        neighbors12 = neighbors(neighbors11, self.A).union(neighbors11)
        neighbors22 = neighbors(neighbors21, self.A).union(neighbors21)
        neighbors13 = neighbors(neighbors12, self.A).union(neighbors12)
        neighbors23 = neighbors(neighbors22, self.A).union(neighbors22)

        features = {eh.label_lookup[idx]: val for idx, val in enumerate(tmp_features.flatten())}
        # test (1,1) features
        int11 = neighbors11.intersection(neighbors21)
        self.assertTrue(isclose(len(int11), features[(1, 1)], abs_tol=1))
        # test (2,1) features
        int21 = neighbors12.intersection(neighbors21)
        feat21 = int21.difference(int11)
        self.assertTrue(isclose(len(feat21), features[(2, 1)], abs_tol=1.5))
        # test (1,2) features
        int12 = neighbors11.intersection(neighbors22)
        feat12 = int12.difference(int11)
        self.assertTrue(isclose(len(feat12), features[(1, 2)], abs_tol=1))
        # test (2,2) features
        int22 = neighbors12.intersection(neighbors22)
        feat22 = int22.difference(feat12 | feat21 | int11)
        # for this test the two numbers come in around 12 and 16. looked, but can't explain the large error when
        # everything else is fine
        self.assertTrue(isclose(len(feat22), features[(2, 2)], abs_tol=4))
        # TEST ORDER 3 FROM HERE
        #  (3,1)
        int31 = neighbors13.intersection(neighbors21)
        feat31 = int31.difference(int11 | feat21)
        self.assertTrue(isclose(len(feat31), features[(3, 1)], abs_tol=1.5))
        # (1,3)
        int13 = neighbors11.intersection(neighbors23)
        feat13 = int13.difference(int11 | feat12)
        self.assertTrue(isclose(len(feat13), features[(1, 3)], abs_tol=1.5))
        # (3,2)
        int32 = neighbors13.intersection(neighbors22)
        feat32 = int32.difference(int11 | feat21 | feat12 | feat22 | feat31)
        self.assertTrue(isclose(len(feat32), features[(3, 2)], abs_tol=2))
        # (2, 3)
        int23 = neighbors12.intersection(neighbors23)
        feat23 = int23.difference(int11 | feat21 | feat12 | feat22 | feat13)
        self.assertTrue(isclose(len(feat23), features[(2, 3)], abs_tol=2))
        # (3, 3)
        int33 = neighbors13.intersection(neighbors23)
        feat33 = int33.difference(int11 | feat21 | feat12 | feat22 | feat31 | feat13 | feat23 | feat32)
        self.assertTrue(isclose(len(feat33), features[(3, 3)], abs_tol=2))
        # (0,1)
        feat01 = neighbors21.difference(int11 | feat21 | feat31)
        self.assertTrue(isclose(len(feat01), features[(0, 1)], abs_tol=2))

</file>

<file path="lp/Buddy/test/test_heuristics.py">
"""
testing the simple heuristics Personalized PageRank, Adamic Adar and Common Neighbours

This is the directed test graph
   -> 0 -> 1 <-
   |          |
    --- 2 <---
"""

import unittest
import math

import torch
from torch import tensor
import scipy.sparse as ssp
import numpy as np

from src.heuristics import AA, PPR, CN, RA


class HeuristicTests(unittest.TestCase):
    def setUp(self):
        self.edge_index = tensor([[0, 2, 2, 1], [1, 0, 1, 2]]).t()
        self.edge_weight = torch.ones(self.edge_index.size(0), dtype=torch.int)
        self.test_edges = tensor([[0, 1], [1, 2]]).t()
        self.num_nodes = 3
        self.neg_test_edges = tensor([[0, 1], [2, 0]]).t()
        self.A = ssp.csr_matrix((self.edge_weight, (self.edge_index[:, 0], self.edge_index[:, 1])),
                                shape=(self.num_nodes, self.num_nodes), dtype=float)
        # create a graph with 2 isomorphic nodes 2 & 3
        self.iso_edge_index = tensor([[2, 2, 3, 3, 4, 0], [1, 4, 1, 4, 0, 1]]).t()
        self.iso_edge_weight = torch.ones(self.iso_edge_index.size(0), dtype=int)
        self.iso_test_edges = tensor([[2, 3], [0, 0]]).t()
        self.iso_num_nodes = 5

        square1 = np.array([[0, 1, 1, 2, 2, 3, 3, 0],
                            [1, 0, 2, 1, 3, 2, 0, 3]])
        square2 = square1 + 4
        bridge = np.array([[0, 4],
                           [4, 0]])

        common_neigbours = np.array([[0, 9, 9, 4, 0, 8, 4, 8],
                                     [9, 0, 4, 9, 8, 0, 8, 4]])

        self.ei = torch.tensor(np.concatenate([square1, square2, bridge, common_neigbours], axis=1), dtype=torch.long)
        ew = torch.ones(self.ei.size(1), dtype=int)
        num_nodes = 10
        self.A1 = ssp.csr_matrix((ew, (self.ei[0], self.ei[1])), shape=(num_nodes, num_nodes))

    def test_CN(self):
        train_scores, edge_index = CN(self.A, self.edge_index)
        self.assertTrue(np.array_equal(train_scores, np.array([0, 1, 0, 0])))
        neg_scores, edge_index = CN(self.A, self.neg_test_edges)
        self.assertTrue(np.array_equal(neg_scores, np.array([1, 0])))
        pos_scores, edge_index = CN(self.A, self.test_edges)
        self.assertTrue(np.array_equal(pos_scores, np.array([0, 0])))

    def test_AA(self):
        train_scores, edge_index = AA(self.A, self.edge_index)
        print(train_scores)
        self.assertTrue(np.allclose(train_scores, np.array([0, 1 / math.log(2), 0, 0])))
        neg_scores, edge_index = AA(self.A, self.neg_test_edges)
        self.assertTrue(np.allclose(neg_scores, np.array([1 / math.log(2), 0])))
        pos_scores, edge_index = AA(self.A, self.test_edges)
        self.assertTrue(np.allclose(pos_scores, np.array([0, 0])))

    def test_RA(self):
        train_scores, edge_index = RA(self.A, self.edge_index)
        print(train_scores)
        self.assertTrue(np.allclose(train_scores, np.array([0, 1 / 2, 0, 0])))
        neg_scores, edge_index = RA(self.A, self.neg_test_edges)
        self.assertTrue(np.allclose(neg_scores, np.array([1 / 2, 0])))
        pos_scores, edge_index = RA(self.A, self.test_edges)
        self.assertTrue(np.allclose(pos_scores, np.array([0, 0])))

    def test_iso_graph(self):
        A = ssp.csr_matrix((self.iso_edge_weight, (self.iso_edge_index[:, 0], self.iso_edge_index[:, 1])),
                           shape=(self.iso_num_nodes, self.iso_num_nodes))
        aa_test_scores, edge_index = AA(A, self.iso_test_edges)
        print(aa_test_scores)
        self.assertTrue(aa_test_scores[0] == aa_test_scores[1])
        cn_test_scores, edge_index = CN(A, self.iso_test_edges)
        print(cn_test_scores)
        self.assertTrue(cn_test_scores[0] == cn_test_scores[1])
        ppr_test_scores, edge_index = PPR(A, self.iso_test_edges)
        print(ppr_test_scores)
        self.assertTrue(ppr_test_scores[0] == ppr_test_scores[1])

</file>

<file path="lp/Buddy/test/test_labelling_tricks.py">
import unittest

import torch
from torch import tensor
import scipy.sparse as ssp
import numpy as np

from src.labelling_tricks import de_plus_node_labeling, de_node_labeling, drnl_node_labeling, drnl_hash_function, \
    get_drnl_lookup


class LabelingTrickTests(unittest.TestCase):
    def setUp(self):
        num_node_features = 2
        # test graph is two connected squares
        square1 = np.array([[0, 1, 1, 2, 2, 3, 3, 0],
                            [1, 0, 2, 1, 3, 2, 0, 3]])
        square2 = square1 + 4
        bridge = np.array([[0, 4],
                           [4, 0]])
        self.edge_index = torch.tensor(np.concatenate([square1, square2, bridge], axis=1), dtype=torch.long)
        self.edge_set = {(0, 1), (2, 0), (2, 1), (1, 2)}
        self.target_edges = [(0, 1), (1, 2), (0, 2), (1, 0)]
        self.edge_weight = torch.ones(self.edge_index.size(1), dtype=int)
        self.test_edges = tensor([[0, 1], [1, 2]])
        self.num_nodes = 8
        self.A = ssp.csr_matrix((self.edge_weight, (self.edge_index[0], self.edge_index[1])),
                                shape=(self.num_nodes, self.num_nodes))
        # add one more edge connecting the squares
        bridge1 = torch.tensor([[5, 1], [1, 5]])
        ei = torch.cat([self.edge_index, bridge1], dim=1)
        ew = torch.ones(ei.size(1), dtype=int)
        self.A1 = ssp.csr_matrix((ew, (ei[0], ei[1])),
                                 shape=(self.num_nodes, self.num_nodes))
        # add one more edge connecting the squares
        bridge2 = torch.tensor([[1, 3, 5, 7], [3, 1, 7, 5]])
        ei1 = torch.cat([self.edge_index, bridge2], dim=1)
        ew1 = torch.ones(ei1.size(1), dtype=int)
        self.A2 = ssp.csr_matrix((ew1, (ei1[0], ei1[1])),
                                 shape=(self.num_nodes, self.num_nodes))

    def test_de_node_labeling(self):
        labels = de_node_labeling(self.A, 0, 4, max_dist=3)
        square1_truth = torch.tensor([[0, 1], [1, 2], [2, 3], [1, 2]])
        square2_truth = square1_truth.flip(dims=[1])
        truth = torch.cat([square1_truth, square2_truth], dim=0)
        self.assertTrue(torch.all(torch.eq(labels, truth)))
        labels = de_node_labeling(self.A1, 0, 4, max_dist=3)
        # nothing should change as DE does not mask the src / dst edges when calculating distances
        self.assertTrue(torch.all(torch.eq(labels, truth)))

    def test_drnl_node_labeling(self):
        labels = drnl_node_labeling(self.A, 0, 4, max_dist=10)
        truth = torch.tensor([1, 27, 33, 27, 1, 27, 33, 27])
        self.assertTrue(torch.all(torch.eq(labels, truth)))
        labels = drnl_node_labeling(self.A1, 0, 4, max_dist=10)
        truth1 = torch.tensor([1, 3, 7, 6, 1, 3, 7, 6])
        self.assertTrue(torch.all(torch.eq(labels, truth1)))
        labels = drnl_node_labeling(self.A2, 0, 4, max_dist=10)
        self.assertTrue(torch.all(torch.eq(labels, truth)))

    def test_de_plus_node_labeling(self):
        md = 10
        labels = de_plus_node_labeling(self.A, 0, 4, max_dist=md)
        square1_truth = torch.tensor([[0, 1], [1, md], [2, md], [1, md]])
        square2_truth = square1_truth.flip(dims=[1])
        truth = torch.cat([square1_truth, square2_truth], dim=0)
        self.assertTrue(torch.all(torch.eq(labels, truth)))
        labels = de_plus_node_labeling(self.A1, 0, 4, max_dist=md)
        square1_truth = torch.tensor([[0, 1], [1, 2], [2, 3], [1, 4]])
        square2_truth = square1_truth.flip(dims=[1])
        truth = torch.cat([square1_truth, square2_truth], dim=0)
        self.assertTrue(torch.all(torch.eq(labels, truth)))

    def test_drnl_hash_function(self):
        hash1 = drnl_hash_function(tensor([1]), tensor([1]))
        self.assertTrue(2 == hash1.item())
        hash2 = drnl_hash_function(tensor([1]), tensor([2]))
        hash3 = drnl_hash_function(tensor([2]), tensor([1]))
        self.assertTrue(hash2 == hash3)
        hash4 = drnl_hash_function(tensor([9]), tensor([9]))
        self.assertTrue(hash4.item() == 82)
        hash5 = drnl_hash_function(tensor([20]), tensor([20]))
        self.assertTrue(hash5.item() == 401)
        hash6 = drnl_hash_function(tensor([0]), tensor([0]))
        self.assertTrue(hash6.item() == 1)
        hash7 = drnl_hash_function(tensor([0]), tensor([1]))
        self.assertTrue(hash7.item() == 1)
        hash8 = drnl_hash_function(tensor([10]), tensor([0]))
        self.assertTrue(hash8.item() == 1)

    def test_get_drnl_lookup(self):
        max_dist = 10
        num_hops = 2
        z_to_idx, idx_to_dst = get_drnl_lookup(num_hops, max_dist)
        self.assertTrue(len(z_to_idx) == len(idx_to_dst))
        self.assertTrue(len(z_to_idx) == 20)
        z = torch.tensor([1, 1, 2, 3])
        z.apply_(lambda x: z_to_idx[x])
        self.assertTrue(torch.all(torch.eq(z, torch.tensor([0, 0, 1, 2]))))

</file>

<file path="lp/Buddy/test/test_params.py">
"""
testing data reader and preprocessing

Store the global parameter dictionary to be imported and modified by each test
"""
import random
import torch
import numpy as np
from math import inf

OPT = {'sample_size': None, 'dataset_name': 'Cora', 'num_hops': 2, 'max_dist': 10, 'max_nodes_per_hop': 10,
       'data_appendix': None, 'val_pct': 0.1, 'test_pct': 0.2, 'dynamic_train': True,
       'dynamic_val': True, 'model': 'BUDDY', 'sign_k': 2, 'loss': 'bce', 'log_features': False,
       'dynamic_test': True, 'node_label': 'drnl', 'ratio_per_hop': 1, 'use_feature': True, 'dropout': 0,
       'label_dropout': 0, 'feature_dropout': 0,
       'add_normed_features': False, 'use_RA': False, 'hidden_channels': 32, 'load_features': True,
       'load_hashes': True, 'use_zero_one': True, 'wandb': False, 'batch_size': 32, 'num_workers': 1,
       'cache_subgraph_features': False, 'lr': 0.1, 'weight_decay': 0, 'eval_batch_size': 100,
       'propagate_embeddings': False, 'num_negs': 1,
       'sign_dropout': 0.5, 'use_struct_feature': True, 'max_hash_hops': 2, 'hll_p': 8,
       'minhash_num_perm': 128, 'floor_sf': False, 'year': 0, 'feature_prop': 'gcn', 'train_node_embeddings': False,
       'train_samples': inf, 'val_samples': inf, 'test_samples': inf, 'reps': 1, 'train_node_embedding': False,
       'pretrained_node_embedding': False, 'max_z': 1000, 'eval_steps': 1, 'K': 100, 'save_model': False,
       'subgraph_feature_batch_size': 1000000}


def setup_seed(seed):
    np.random.seed(seed)
    random.seed(seed)
    torch.manual_seed(seed)  # cpu
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = True

</file>

<file path="lp/Buddy/test/test_seal_datasets.py">
"""
Testing the SEAL data structure
"""

import unittest
from argparse import Namespace

import torch
from torch import tensor
import scipy.sparse as ssp
from torch_geometric.data import Data

from src.datasets.seal import SEALDataset, SEALDynamicDataset, k_hop_subgraph, \
    construct_pyg_graph
from src.data import get_data
from test_params import OPT
from src.utils import get_src_dst_degree, get_pos_neg_edges


class SEALDatasetTests(unittest.TestCase):
    def setUp(self):
        self.edge_index = tensor([[0, 1, 2, 0, 2, 1, 2, 3], [1, 0, 0, 2, 1, 2, 3, 2]])
        self.edge_set = {(0, 1), (2, 0), (2, 1), (1, 2)}
        self.target_edges = [(0, 1), (1, 2), (0, 2), (1, 0)]
        self.edge_weight = torch.ones(self.edge_index.size(1), dtype=int)
        self.test_edges = tensor([[0, 1], [1, 2]])
        self.num_nodes = 4
        self.neg_test_edges = tensor([[0, 1], [2, 0]])
        self.A = ssp.csr_matrix((self.edge_weight, (self.edge_index[0], self.edge_index[1])),
                                shape=(self.num_nodes, self.num_nodes))
        # create a graph with 2 isomorphic nodes 2 & 3
        self.iso_edge_index = tensor([[2, 2, 3, 3, 4, 0], [1, 4, 1, 4, 0, 1]])
        self.iso_edge_weight = torch.ones(self.iso_edge_index.size(0), dtype=int)
        self.iso_test_edges = tensor([[2, 3], [0, 0]])
        self.iso_num_nodes = 5
        self.args = OPT

    def test_k_hop_subgraph(self):
        num_hops = ratio_per_hop = 1
        directed = False
        max_nodes_per_hop = None
        if directed:
            A_csc = self.A.tocsc()
        else:
            A_csc = None
        x = torch.rand(self.num_nodes, 2)
        src, dst = 0, 1
        nodes, subgraph, dists, node_features, y = k_hop_subgraph(src, dst, num_hops, self.A, ratio_per_hop,
                                                                  max_nodes_per_hop, node_features=x, y=1,
                                                                  directed=directed, A_csc=A_csc)
        data = construct_pyg_graph(nodes, subgraph, dists, node_features, y, node_label='drnl')
        self.assertTrue(data.edge_index.shape[1] == 4)  # 3 edges minus the 0-1 edge and undirected
        src, dst = 1, 2
        nodes, subgraph, dists, node_features, y = k_hop_subgraph(src, dst, num_hops, self.A, ratio_per_hop,
                                                                  max_nodes_per_hop, node_features=x, y=1,
                                                                  directed=directed, A_csc=A_csc)
        data = construct_pyg_graph(nodes, subgraph, dists, node_features, y, node_label='drnl')

        self.assertTrue(data.edge_index.shape[1] == 6)  # 4 edges minus 1->2 and 2->1, but undirected
        src, dst = 2, 1
        nodes, subgraph, dists, node_features, y = k_hop_subgraph(src, dst, num_hops, self.A, ratio_per_hop,
                                                                  max_nodes_per_hop, node_features=x, y=1,
                                                                  directed=directed, A_csc=A_csc)
        data = construct_pyg_graph(nodes, subgraph, dists, node_features, y, node_label='drnl')
        self.assertTrue(data.edge_index.shape[1] == 6)  # 4 edges minus 1->2 and 2->1

    def test_seal_dynamic_dataset(self):
        path = './dataset/seal_test_data'
        use_coalesce = False
        node_label = 'drnl'
        ratio_per_hop = 1.
        max_nodes_per_hop = None
        data = Data(torch.rand(self.num_nodes, 2), self.edge_index, torch.ones(self.edge_index.size(1), dtype=int))
        directed = False
        num_hops = 1
        train_dataset = SEALDynamicDataset(
            path, data, self.test_edges, self.neg_test_edges, num_hops=num_hops, percent=1.0, split='train',
            use_coalesce=use_coalesce,
            node_label=node_label,
            ratio_per_hop=ratio_per_hop,
            max_nodes_per_hop=max_nodes_per_hop,
            directed=directed,
        )
        labels = train_dataset.labels
        self.assertTrue(len(labels) == self.test_edges.size(1) + self.neg_test_edges.size(
            1))  # one prediction for each pos and neg edges
        self.assertTrue(sum(labels) == self.test_edges.size(1))  # pos edges are labelled 1 and neg edges labelled 0

    def test_seal_dataset(self):
        path = './dataset/seal_test_data'
        use_coalesce = False
        node_label = 'drnl'
        ratio_per_hop = 1.
        max_nodes_per_hop = None
        data = Data(torch.rand(self.num_nodes, 2), self.edge_index, torch.ones(self.edge_index.size(1), dtype=int))
        directed = False
        num_hops = 1
        train_dataset = SEALDataset(
            path, data, self.test_edges, self.neg_test_edges, num_hops=num_hops, percent=1.0, split='train',
            use_coalesce=use_coalesce,
            node_label=node_label,
            ratio_per_hop=ratio_per_hop,
            max_nodes_per_hop=max_nodes_per_hop,
            directed=directed,
        )
        labels = train_dataset.data.y
        self.assertTrue(labels.shape[0] == self.test_edges.size(1) + self.neg_test_edges.size(
            1))  # one prediction for each pos and neg edges
        self.assertTrue(sum(labels) == self.test_edges.size(1))  # pos edges are labelled 1 and neg edges labelled 0

    def test_get_data(self):
        """
        We use the pyg RandomLinkSplit object to create train / val / test splits. For link prediction edges play 2 roles
        1/ for message passing 2/ as supervision
        :return:
        """
        opt = {'sample_size': None, 'dataset_name': 'Cora', 'num_hops': 2, 'max_dist': 10, 'max_nodes_per_hop': 10,
               'data_appendix': None, 'val_pct': 0.1, 'test_pct': 0.2, 'train_sample': 1, 'dynamic_train': True,
               'dynamic_val': True, 'model': 'linear', 'dynamic_test': True, 'node_label': 'drnl', 'ratio_per_hop': 1}
        opt = {**OPT, **opt}
        args = Namespace(**opt)
        dataset, splits, directed, eval_metric = get_data(args)
        train, val, test = splits['train'], splits['valid'], splits['test']
        train_pos_edges, train_neg_edges = get_pos_neg_edges(train)
        # the default behaviour is 1 negative edge for each positive edge
        self.assertTrue(train_pos_edges.shape == train_neg_edges.shape)
        val_pos_edges, val_neg_edges = get_pos_neg_edges(val)
        self.assertTrue(val_pos_edges.shape == val_neg_edges.shape)
        test_pos_edges, test_neg_edges = get_pos_neg_edges(test)
        self.assertTrue(test_pos_edges.shape == test_neg_edges.shape)

    def test_get_src_dst_degree(self):
        temp1 = len(self.A[0].indices)
        self.assertTrue(temp1 == 2)
        src_deg, dst_deg = get_src_dst_degree(0, 1, self.A, 3)
        self.assertTrue(src_deg == 2)
        self.assertTrue(dst_deg == 2)
        src_deg, dst_deg = get_src_dst_degree(0, 1, self.A, 1)
        self.assertTrue(src_deg == 1)
        self.assertTrue(dst_deg == 1)
        src_deg, dst_deg = get_src_dst_degree(0, 1, self.A, None)
        self.assertTrue(src_deg == 2)
        self.assertTrue(dst_deg == 2)
        src_deg, dst_deg = get_src_dst_degree(0, 1, self.A, 0)
        self.assertTrue(src_deg == 0)
        self.assertTrue(dst_deg == 0)

</file>

<file path="lp/Buddy/test/test_sign.py">
"""
tests for the SIGN module and it's integration
"""

import unittest

import torch
from torch import tensor
from argparse import Namespace
from torch_geometric.data import Data
from torch_geometric.utils.random import barabasi_albert_graph
import scipy.sparse as ssp

from src.models.gnn import SIGNEmbedding, SIGN
from test_params import OPT
from src.datasets.elph import HashDataset


class SIGNTests(unittest.TestCase):
    def setUp(self):
        self.n_nodes = 30
        degree = 5
        self.x = torch.rand((self.n_nodes, 2))
        self.edge_index = barabasi_albert_graph(self.n_nodes, degree)
        self.edge_weight = torch.ones(self.edge_index.shape[1])
        self.A = ssp.csr_matrix((self.edge_weight, (self.edge_index[0], self.edge_index[1])),
                                shape=(self.n_nodes, self.n_nodes))
        self.test_edges = tensor([[0, 1], [1, 2]])
        self.neg_test_edges = tensor([[0, 1], [2, 0]])

    def test_generate_sign_features(self):
        sign_k = 2
        n_features = 2
        opt = {'sample_size': None, 'dataset_name': 'Cora', 'hidden_dimension': 3, 'sign_k': sign_k}
        opt = {**OPT, **opt}
        args = Namespace(**opt)
        split = 'train'
        root = ('.')
        data = Data(torch.rand(self.n_nodes, n_features), self.edge_index, self.edge_weight)
        elph_dataset = HashDataset(root, split, data, self.test_edges, self.neg_test_edges, args)
        x = elph_dataset._generate_sign_features(data, self.edge_index, self.edge_weight, sign_k)
        self.assertTrue(x.shape == (self.n_nodes, n_features * (sign_k + 1)))
        sign_k = 0
        x = elph_dataset._generate_sign_features(data, self.edge_index, self.edge_weight, sign_k)
        self.assertTrue(x.shape == data.x.shape)

    def test_sign_forward(self):
        hidden_channels = 3
        K = 2
        batch_size = 16
        # sign takes a batch of edges
        x = torch.rand((batch_size, 2, hidden_channels * (K + 1)))  # the original features + K propagated features
        xs = torch.split(x, K + 1, dim=-1)
        self.assertTrue(len(xs) == K + 1)
        self.assertTrue(xs[0].shape == xs[-1].shape)
        self.assertTrue(x.shape[0] == xs[0].shape[0])
        torch.manual_seed(0)
        sign = SIGN(hidden_channels, hidden_channels, hidden_channels, K, 0.5)
        out = sign(x)
        self.assertTrue(out.shape == (batch_size, 2, hidden_channels))

    def test_convs(self):
        hidden_channels = 5
        x = torch.rand((self.n_nodes, hidden_channels))
        torch.manual_seed(0)
        sign = SIGNEmbedding(hidden_channels, hidden_channels, hidden_channels, 2, 0.5)
        torch.manual_seed(0)
        sign_forward = sign(x, self.edge_index, self.n_nodes)
        self.assertTrue(sign_forward.shape == (self.n_nodes, hidden_channels))

</file>

<file path="lp/MGAT/DataLoad.py">
import random
import numpy as np
import torch
from tqdm import tqdm
from torch.utils.data import Dataset, DataLoader

class DataLoad(Dataset):
	def __init__(self, train_edge, num_nodes):
		super(DataLoad, self).__init__()

		src_tgt_dict = {i: [] for i in range(num_nodes)}
		for pair in tqdm(train_edge, total=train_edge.shape[0]):
			src, tgt = pair[0].item(), pair[1].item()
			if src in src_tgt_dict.keys():
				src_tgt_dict[src].append(tgt)
			else:
				src_tgt_dict[src] = [tgt]

		self.data = train_edge
		self.adj_lists = src_tgt_dict
		self.num_nodes = num_nodes

	def __getitem__(self, index):
		user, pos_item = self.data[index]
		user, pos_item = user.item(), pos_item.item()
  
		while True:
			neg_tgt = random.randint(0, self.num_nodes-1)
			if neg_tgt not in self.adj_lists[user]:
				break
		return [user, pos_item, neg_tgt]
				

	def __len__(self):
		return len(self.data)



</file>

<file path="lp/MGAT/GraphGAT.py">
import torch
import torch.nn.functional as F
from torch.nn import Parameter
from torch_geometric.nn.conv import MessagePassing
from torch_geometric.utils import remove_self_loops, add_self_loops, softmax, degree
from torch_geometric.nn.inits import uniform, glorot, zeros
class GraphGAT(MessagePassing):
	def __init__(self, in_channels, out_channels, normalize=True, bias=True, aggr='add', **kwargs):
		super(GraphGAT, self).__init__(aggr=aggr, **kwargs)
		self.in_channels = in_channels
		self.out_channels = out_channels
		self.normalize = normalize
		self.dropout = 0.2

		self.weight = Parameter(torch.Tensor(self.in_channels, out_channels))
		if bias:
			self.bias = Parameter(torch.Tensor(out_channels))
		else:
			self.register_parameter('bias', None)

		self.reset_parameters()
		self.is_get_attention = False

	def reset_parameters(self):
		uniform(self.in_channels, self.weight)
		uniform(self.in_channels, self.bias)


	def forward(self, x, edge_index, size=None):
		if size is None:
			edge_index, _ = remove_self_loops(edge_index)
		x = x.unsqueeze(-1) if x.dim() == 1 else x
		x = torch.matmul(x, self.weight)

		return self.propagate(edge_index, size=size, x=x)

	def message(self, edge_index_i, x_i, x_j, size_i, edge_index, size):
		#print(edge_index_i, x_i, x_j, size_i, edge_index, size)
      
		# Compute attention coefficients.
		x_i = x_i.view(-1, self.out_channels)
		x_j = x_j.view(-1, self.out_channels)
		inner_product = torch.mul(x_i, F.leaky_relu(x_j)).sum(dim=-1)

		# gate
		row, col = edge_index
		deg = degree(row, size[0], dtype=x_i.dtype)
		deg_inv_sqrt = deg[row].pow(-0.5)
		tmp = torch.mul(deg_inv_sqrt, inner_product)
		gate_w = torch.sigmoid(tmp)
		# gate_w = F.dropout(gate_w, p=self.dropout, training=self.training)

		# attention
		tmp = torch.mul(inner_product, gate_w)
		attention_w = softmax(tmp, edge_index_i, num_nodes=size_i)
		#attention_w = F.dropout(attention_w, p=self.dropout, training=self.training)
		return torch.mul(x_j, attention_w.view(-1, 1))

	def update(self, aggr_out):
		if self.bias is not None:
			aggr_out = aggr_out + self.bias
		if self.normalize:
			aggr_out = F.normalize(aggr_out, p=2, dim=-1)
		return aggr_out

	def __repr(self):
		return '{}({},{})'.format(self.__class__.__name__, self.in_channels, self.out_channels)

</file>

<file path="lp/MGAT/Model.py">
import math
# from tqdm import tqdm
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from GraphGAT import GraphGAT

class MGAT(torch.nn.Module):
    def __init__(self, features, edge_index, batch_size, num_nodes, num_layers: int, dim_x=64):
        super(MGAT, self).__init__()
        self.batch_size = batch_size
        self.num_nodes = num_nodes

        self.edge_index = torch.tensor(edge_index).t().contiguous().cuda()
        self.edge_index = torch.cat((self.edge_index, self.edge_index[[1,0]]), dim=1)
        
        v_feat, a_feat, t_feat = features
        self.v_feat = torch.tensor(v_feat).cuda()
        self.a_feat = None
        self.t_feat = torch.tensor(t_feat).cuda()

        self.v_gnn = GNN(self.v_feat, self.edge_index, batch_size, num_nodes, dim_x, num_layers, dim_latent=256)
        self.a_gnn = None
        self.t_gnn = GNN(self.t_feat, self.edge_index, batch_size, num_nodes, dim_x, num_layers, dim_latent=100)

        self.id_embedding = nn.Embedding(num_nodes, dim_x)
        nn.init.xavier_normal_(self.id_embedding.weight)

        #self.id_embedding = nn.init.xavier_normal_(torch.rand((num_user+num_item, dim_x), requires_grad=True)).cuda()
        self.result_embed = nn.init.xavier_normal_(torch.rand((num_nodes, dim_x))).cuda()

    def forward(self, user_nodes, pos_items, neg_items):
        v_rep = self.v_gnn(self.id_embedding)
        a_rep = None
        t_rep = self.t_gnn(self.id_embedding)
        if a_rep is None:
            representation = (v_rep + t_rep) / 2
        else:
            representation = (v_rep + a_rep + t_rep) / 3 #torch.max_pool2d((v_rep, a_rep, t_rep))#max()#torch.cat((v_rep, a_rep, t_rep), dim=1)
        self.result_embed = representation
        user_tensor = representation[user_nodes]
        pos_tensor = representation[pos_items]
        neg_tensor = representation[neg_items]
        pos_scores = torch.sum(user_tensor * pos_tensor, dim=1)
        neg_tensor = torch.sum(user_tensor * neg_tensor, dim=1)
        return pos_scores, neg_tensor


    def loss(self, data):
        user, pos_items, neg_items = data
        pos_scores, neg_scores = self.forward(user.cuda(), pos_items.cuda(), neg_items.cuda())
        loss_value = -torch.sum(torch.log2(torch.sigmoid(pos_scores - neg_scores)))
        return loss_value


class GNN(torch.nn.Module):
    def __init__(self, features, edge_index, batch_size, num_nodes, dim_id, num_layers: int, dim_latent=None):
        super(GNN, self).__init__()
        self.batch_size = batch_size
        self.num_nodes = num_nodes
        self.dim_id = dim_id
        self.dim_feat = features.size(1)
        self.dim_latent = dim_latent
        self.edge_index = edge_index
        self.features = features
        self.num_layers = num_layers

        if self.dim_latent:
            #self.preference = nn.init.xavier_normal_(torch.rand((num_user, self.dim_latent), requires_grad=True)).cuda()
            self.MLP = nn.Linear(self.dim_feat, self.dim_latent)

            self.conv_embed_1 = GraphGAT(self.dim_latent, self.dim_latent, aggr='add')
            nn.init.xavier_normal_(self.conv_embed_1.weight)
            self.linear_layer1 = nn.Linear(self.dim_latent, self.dim_id)
            nn.init.xavier_normal_(self.linear_layer1.weight)
            self.g_layer1 = nn.Linear(self.dim_latent, self.dim_id)    
            nn.init.xavier_normal_(self.g_layer1.weight) 
        else:
            #self.preference = nn.init.xavier_normal_(torch.rand((num_user, self.dim_feat), requires_grad=True)).cuda()
            self.conv_embed_1 = GraphGAT(self.dim_feat, self.dim_feat, aggr='add')
            nn.init.xavier_normal_(self.conv_embed_1.weight)
            self.linear_layer1 = nn.Linear(self.dim_feat, self.dim_id)
            nn.init.xavier_normal_(self.linear_layer1.weight)
            self.g_layer1 = nn.Linear(self.dim_feat, self.dim_id)    
            nn.init.xavier_normal_(self.g_layer1.weight)

        if num_layers >= 2:
            self.conv_embed_2 = GraphGAT(self.dim_id, self.dim_id, aggr='add')
            nn.init.xavier_normal_(self.conv_embed_2.weight)
            self.linear_layer2 = nn.Linear(self.dim_id, self.dim_id)
            nn.init.xavier_normal_(self.linear_layer2.weight)
            self.g_layer2 = nn.Linear(self.dim_id, self.dim_id)    
            nn.init.xavier_normal_(self.g_layer2.weight)
        if num_layers >= 3:
            self.conv_embed_3 = GraphGAT(self.dim_id, self.dim_id, aggr='add')
            nn.init.xavier_normal_(self.conv_embed_3.weight)
            self.linear_layer3 = nn.Linear(self.dim_id, self.dim_id)
            nn.init.xavier_normal_(self.linear_layer3.weight)
            self.g_layer3 = nn.Linear(self.dim_id, self.dim_id)    
            nn.init.xavier_normal_(self.g_layer3.weight)
            

    def forward(self, id_embedding):
        temp_features = torch.tanh(self.MLP(self.features)) if self.dim_latent else self.features
        x = temp_features
        x = F.normalize(x).cuda()

        #layer-1
        h = F.leaky_relu(self.conv_embed_1(x, self.edge_index, None))
        x_hat = F.leaky_relu(self.linear_layer1(x)) + id_embedding.weight
        x_1 = F.leaky_relu(self.g_layer1(h)+x_hat)
        
        if self.num_layers == 1:
            x = x_1
            return x_1
        
        # layer-2
        h = F.leaky_relu(self.conv_embed_2(x_1, self.edge_index, None))
        x_hat = F.leaky_relu(self.linear_layer2(x_1)) + id_embedding.weight
        x_2 = F.leaky_relu(self.g_layer2(h)+x_hat)

        if self.num_layers == 2:
            x = torch.cat((x_1, x_2), dim=1)
            return x
            
        h = F.leaky_relu(self.conv_embed_2(x_2, self.edge_index, None))
        x_hat = F.leaky_relu(self.linear_layer2(x_2)) + id_embedding.weight
        x_3 = F.leaky_relu(self.g_layer3(h)+x_hat)
        
        if self.num_layers == 3:
            x = torch.cat((x_1, x_2, x_3), dim=1)
            return x
        
</file>

<file path="lp/MGAT/README.md">
# MGAT
This is our Pytorch implementation for our paper- Multimodal Graph Attention Network(MGAT):

>	Zhulin Tao, Yinwei Wei, Xiang Wang, Xiangnan He, Xianglin Huang, Tat-Seng Chua:
MGAT: Multimodal Graph Attention Network for Recommendation. Inf. Process. Manag. 57(5): 102277 (2020)

## Introduction
In this work, we propose a new Multimodal Graph Attention Network, short for MGAT, which disentangles personal interests at the granularity of modality. In particular, built upon multimodal interaction graphs, MGAT conducts information propagation within individual graphs, while leveraging the gated attention mechanism to identify varying importance scores of different modalities to user preference.
## Environment Requirement
The code has been tested running under Python 3.6.5. The required packages are as follows:
* torch==1.7.0
* numpy==1.16.1
* torch_geometric==1.6.1

## run
```python
CUDA_VISIBLE_DEVICES=0 python  -u train.py --num_epoch 200 --batch_size 2048 --weight_decay 0.1 --l_r 3e-5
```
# Citation
@article{DBLP:journals/ipm/TaoWWHHC20,
  author    = {Zhulin Tao and
               Yinwei Wei and
               Xiang Wang and
               Xiangnan He and
               Xianglin Huang and
               Tat{-}Seng Chua},
  title     = {{MGAT:} Multimodal Graph Attention Network for Recommendation},
  journal   = {Inf. Process. Manag.},
  volume    = {57},
  number    = {5},
  pages     = {102277},
  year      = {2020}
}

</file>

<file path="lp/MGAT/mmgat.sh">

python run.py \
    --l_r 0.002 \
    --weight_decay 1e-1 \
    --batch_size 2048 \
    --num_epoch 100 \
    --edge_split_path /nfs/turbo/coe-dkoutra/jing/Next-GraphGPT/Patton/cloth/lp-edge-split-random.pt \
    --feat_path /nfs/turbo/coe-dkoutra/jing/Next-GraphGPT/Patton/cloth/t5vit_feat.pt \
    --project_name mmgat_cloth \
    --wandb_run_name random_t5vit \
    --num_layers 2 \
    --PATH_best_weight_save './mmgat_cloth_ckpt.pt' \
    --PATH_best_metrics_save './mmgat_cloth_best_metrics.txt' \
    --PATH_best_hyperparameter_save './mmgat_cloth_best_hyperparameter.txt' \

</file>

<file path="lp/MGAT/run.py">
import os
import sys
import argparse
import numpy as np
from tqdm import tqdm
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from DataLoad import DataLoad
from Model import MGAT
import wandb
import pickle
import optuna


def batch_evaluate_model(representation: torch.Tensor, triple_data, batch_size=256, verbose=False):
    dataset = TensorDataset(
        torch.tensor(triple_data['source_node']),
        torch.tensor(triple_data['target_node']),
        torch.tensor(triple_data['target_node_neg'], dtype=torch.long))

    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)

    rank_ls = []
    for src, pos_tgt, neg_tgt_ls in tqdm(dataloader, disable=not verbose):
        src_ls = src.repeat_interleave(neg_tgt_ls.size(1) + 1, dim=0)
        tgt_ls = torch.cat((pos_tgt.unsqueeze(1), neg_tgt_ls), dim=1).view(-1)

        src_score = representation[src_ls]
        tgt_score = representation[tgt_ls]

        pred = torch.sum(src_score * tgt_score, dim=1).detach().cpu()
        rank = get_rank_2d(pred.view(src.size(0), -1))
        rank_ls.extend(rank.numpy().tolist())

    hit_10 = calculate_hit(rank_ls, 10)
    hit_1 = calculate_hit(rank_ls, 1)
    mrr = calculate_mrr(rank_ls)
    return mrr, hit_1, hit_10


def train(dataloader, model, optimizer, max_step=None):
    model.train()
    sum_loss = 0.0
    for idx, data in enumerate(tqdm(dataloader)):
        optimizer.zero_grad()
        loss = model.loss(data)
        loss.backward()
        optimizer.step()
        sum_loss += loss.cpu().item()

        if max_step is not None and idx > max_step:
            break
    return sum_loss / idx


def get_rank(pred, n=0, descending=True):
    """
    Calculate the rank of the nth element in pred
    descending=True means large values ranks higher,
    descending=False means small values ranks higher.
    """
    arg = torch.argsort(torch.tensor(pred), descending=descending)
    rank = torch.where(arg == n)[0] + 1
    return rank.tolist()[0]


def get_rank_2d(pred, n=0, descending=True):
    """
    Calculate the rank of the nth element in each row of pred.
    `pred` is a 2D tensor where each row contains scores for a single sample.
    `n` specifies the index of the element to find the rank for in each row.
    `descending` controls the order of ranking.
    """
    batch_size, num_elements = pred.shape
    n_tensor = torch.full_like(pred, n, dtype=torch.long)
    sorted_indices = torch.argsort(pred, dim=1, descending=descending)
    ranks = torch.nonzero(sorted_indices == n_tensor, as_tuple=False)[:, 1] + 1  # Adding 1 to make ranks start from 1

    return ranks


def calculate_mrr(rank_ls):
    """
    Return the MRR (Mean Reciprocal Rank) of a list of ranks.
    """
    if type(rank_ls) is list:
        rk = np.array(rank_ls)
    else:
        rk = rank_ls
    return (1 / rk).mean()


def calculate_hit(rank_ls, n):
    """
    Return the Hit@n of a list of ranks.
    """
    if type(rank_ls) is list:
        rk = np.array(rank_ls)
    else:
        rk = rank_ls
    rk = np.array(rank_ls)
    return rk[rk <= n].shape[0] / rk.shape[0]


def hyperparameter_search(train_dataloader, args, features, edge_index, num_nodes, val_triple_data):
    def objective(trial):
        lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)
        num_layers = trial.suggest_int('num_layers', 1, 3)

        model = MGAT(features, edge_index, args.batch_size, num_nodes, num_layers,
                     args.dim_latent).cuda()
        optimizer = torch.optim.Adam([{'params': model.parameters(), 'lr': lr}],
                                     weight_decay=args.weight_decay)
        val_max_mrr = 0

        for epoch in range(args.num_epoch):
            train(train_dataloader, model, optimizer, max_step=None)
            torch.cuda.empty_cache()

            if (epoch + 1) % 5 == 0:
                with torch.no_grad():
                    v_rep = model.v_gnn(model.id_embedding)
                    t_rep = model.t_gnn(model.id_embedding)
                    representation = (v_rep + t_rep) / 2

                    val_mrr, val_hit_1, val_hit_10 = batch_evaluate_model(representation, val_triple_data,
                                                                          verbose=False)

                    if val_mrr > val_max_mrr:
                        val_max_mrr = val_mrr
                    print(trial)
                    print(f"Epoch: {epoch}, MRR: {val_max_mrr}, Hit@1: {val_hit_1}, Hit@10: {val_hit_10}")

        return val_max_mrr

    study = optuna.create_study(direction='maximize')
    study.optimize(objective, n_trials=args.n_trials)

    print(study.best_params)

    if args.PATH_best_hyperparameter_save is not None:
        with open(args.PATH_best_hyperparameter_save, 'w') as f:
            f.write(study.best_params)
            f.write('\n')
            f.write(study.best_value)


def main():
    parser = argparse.ArgumentParser()

    # using default values is recommended
    parser.add_argument('--no-cuda', action='store_true', default=False, help='Disables CUDA training.')
    parser.add_argument('--model_name', default='MGAT', help='Model name.')
    parser.add_argument('--PATH_weight_load', default=None, help='Loading weight filename.')
    parser.add_argument('--dim_latent', type=int, default=32, help='Latent dimension.')
    parser.add_argument('--num_workers', type=int, default=4, help='Workers number')

    # your data here
    parser.add_argument('--edge_split_path', required=True)
    parser.add_argument('--v_feat_path', default=None)
    parser.add_argument('--t_feat_path', default=None)
    parser.add_argument('--feat_path', default=None)

    # your setting here
    parser.add_argument('--num_layers', default=2, type=int)
    parser.add_argument('--num_epoch', type=int, default=200, help='Epoch number')
    parser.add_argument('--l_r', type=float, default=1e-4, help='Learning rate.')
    parser.add_argument('--weight_decay', type=float, default=0, help='Weight decay.')
    parser.add_argument('--batch_size', type=int, default=2048, help='Batch size.')
    parser.add_argument('--repeat_times', type=int, default=3, help='Repeat times.')

    parser.add_argument('--PATH_best_weight_save', default=None, help='Writing weight filename.')
    parser.add_argument('--PATH_best_metrics_save', default=None)

    # if want to do hyperparameter search
    parser.add_argument('--do_hyperparameter_search', action="store_true")
    parser.add_argument('--PATH_best_hyperparameter_save', default=None)
    parser.add_argument('--n_trials', type=int, default=20)
    parser.add_argument('--hyperparameter_search_max_step', type=int, default=10000)

    parser.add_argument('--project_name', default='MGAT')
    parser.add_argument('--wandb_run_name', default='untitled_run')
    parser.add_argument('--wandb_key', default=None)
    parser.add_argument('--report_to', default=None)

    args = parser.parse_args()

    # wandb_key = 'ab1e2fd95c62273341f8fe8bbe29b9a6ee33725a'
    # if args.wandb_key is not None and args.report_to == 'wandb':
    #     wandb.login(key=wandb_key)

    print('Data loading ...')
    edge_split = torch.load(args.edge_split_path)
    if args.feat_path is None:
        v_feat = torch.load(args.v_feat_path).to('cuda')
        t_feat = torch.load(args.t_feat_path).to('cuda')
    else:
        feat = torch.load(args.feat_path).to('cuda')
        v_feat = feat[:, int(feat.shape[1] / 2):]
        t_feat = feat[:, :int(feat.shape[1] / 2)]
    a_feat = None
    features = [v_feat, a_feat, t_feat]
    num_nodes = t_feat.shape[0]
    print(f"number of nodes: {num_nodes}")
    train_edge = torch.concat(
        [edge_split['train']['source_node'].reshape(-1, 1), edge_split['train']['target_node'].reshape(-1, 1)], dim=1)
    edge_index = train_edge
    train_dataset = DataLoad(train_edge, num_nodes)
    train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True,
                                  num_workers=args.num_workers)

    val_triple_data = edge_split['valid']
    test_triple_data = edge_split['test']
    print('Data has been loaded.')

    if args.do_hyperparameter_search:
        hyperparameter_search(train_dataloader, args, features, edge_index, num_nodes, val_triple_data)
        return

    num_epoch = args.num_epoch
    learning_rate = args.l_r
    weight_decay = args.weight_decay

    if args.report_to == 'wandb':
        wandb.init(
            project=args.project_name,
            name=args.wandb_run_name,
            config={
                "learning_rate": learning_rate,
                "epochs": num_epoch,
            }
        )

    global_cur_epoch = 0
    mrrs, h1s, h10s = [], [], []

    for _ in range(args.repeat_times):
        model = MGAT(features, edge_index, args.batch_size, num_nodes, args.num_layers,
                     args.dim_latent).cuda()
        optimizer = torch.optim.Adam([{'params': model.parameters(), 'lr': learning_rate}],
                                     weight_decay=weight_decay)

        max_mrr = 0.0
        max_hit_1 = 0.0
        max_hit_10 = 0.0
        val_max_mrr = 0.0
        num_decreases = 0

        for epoch in range(num_epoch):
            loss = train(train_dataloader, model, optimizer, max_step=None)

            torch.cuda.empty_cache()

            # print({"loss": loss})

            with torch.no_grad():
                v_rep = model.v_gnn(model.id_embedding)
                t_rep = model.t_gnn(model.id_embedding)
                representation = (v_rep + t_rep) / 2

                val_mrr, val_hit_1, val_hit_10 = batch_evaluate_model(representation, val_triple_data, verbose=False)
                test_mrr, test_hit_1, test_hit_10 = batch_evaluate_model(representation, test_triple_data,
                                                                         verbose=False)

                if val_mrr > val_max_mrr:
                    val_max_mrr = val_mrr
                    max_mrr = test_mrr
                    max_hit_1 = test_hit_1
                    max_hit_10 = test_hit_10
                    num_decreases = 0

                    if args.PATH_best_weight_save is not None:
                        torch.save(model.state_dict(), args.PATH_best_weight_save)

                    if args.PATH_best_metrics_save is not None:
                        with open(args.PATH_best_metrics_save, 'w') as f:
                            f.write(f"MRR: {test_mrr}")
                            f.write(f"Hit@1: {test_hit_1}")
                            f.write(f"Hit@10: {test_hit_10}")

            global_cur_epoch += 1

        print({
            "val_mrr": val_mrr,
            'val_hit_1': val_hit_1,
            'val_hit_10': val_hit_10,

            'test_mrr': test_mrr,
            'test_hit_1': test_hit_1,
            'test_hit_10': test_hit_10,
        })
        mrrs.append(test_mrr)
        h1s.append(test_hit_1)
        h10s.append(test_hit_10)

    mean_mrr = torch.mean(torch.tensor(mrrs)).item()
    std_mrr = torch.std(torch.tensor(mrrs)).item()
    mean_h1 = torch.mean(torch.tensor(h1s)).item()
    std_h1 = torch.std(torch.tensor(h1s)).item()
    mean_h10 = torch.mean(torch.tensor(h10s)).item()
    std_h10 = torch.std(torch.tensor(h10s)).item()
    print(mean_mrr)
    print(std_mrr)
    print(mean_h1)
    print(std_h1)
    print(mean_h10)
    print(std_h10)


if __name__ == '__main__':
    main()

</file>

<file path="lp/MMGCN/BaseModel.py">
import torch
import torch.nn.functional as F
from torch.nn import Parameter
from torch_geometric.nn.conv import MessagePassing
from torch_geometric.utils import remove_self_loops, add_self_loops, degree
from torch_geometric.nn.inits import uniform

class BaseModel(MessagePassing):
	def __init__(self, in_channels, out_channels, normalize=True, bias=True, aggr='add', **kwargs):
		super(BaseModel, self).__init__(aggr=aggr, **kwargs)
		self.aggr = aggr
		self.in_channels = in_channels
		self.out_channels = out_channels
		self.normalize = normalize
		self.weight = Parameter(torch.Tensor(self.in_channels, out_channels))

		self.reset_parameters()

	def reset_parameters(self):
		uniform(self.in_channels, self.weight)

	def forward(self, x, edge_index, size=None):
		x = torch.matmul(x, self.weight)
		return self.propagate(edge_index, size=(x.size(0), x.size(0)), x=x)

	def message(self, x_j, edge_index, size):
		return x_j

	def update(self, aggr_out):
		return aggr_out

	def __repr(self):
		return '{}({},{})'.format(self.__class__.__name__, self.in_channels, self.out_channels)

</file>

<file path="lp/MMGCN/Dataset.py">
import time
import pickle
import random
import numpy as np
from tqdm import tqdm
import os
import torch
import torch.nn as nn
from torch.utils.data import Dataset
from torch.utils.data import DataLoader

def data_load(train_edge, feat_path):
    
    v_t_feat = torch.load(feat_path, map_location='cpu')
    t_feat = v_t_feat[:, :int(v_t_feat.shape[1] / 2)]
    v_feat = v_t_feat[:, int(v_t_feat.shape[1] / 2):]
    a_feat = None
    
    num_nodes = v_feat.shape[0]
    
    src_tgt_dict = {i: [] for i in range(num_nodes)}
    for pair in tqdm(train_edge, total=train_edge.shape[0]):
        src, tgt = pair[0].item(), pair[1].item()
        if src in src_tgt_dict.keys():
            src_tgt_dict[src].append(tgt)
        else:
            src_tgt_dict[src] = [tgt]
            
    
    return num_nodes, train_edge, src_tgt_dict, v_feat, a_feat, t_feat

def data_load_old(train_edge, v_feat_path, t_feat_path):
    
    v_feat = torch.load(v_feat_path).to('cuda')
    t_feat = torch.load(t_feat_path).to('cuda')
    a_feat = None
    
    num_nodes = v_feat.shape[0]
    
    src_tgt_dict = {i: [] for i in range(num_nodes)}
    for pair in tqdm(train_edge, total=train_edge.shape[0]):
        src, tgt = pair[0].item(), pair[1].item()
        if src in src_tgt_dict.keys():
            src_tgt_dict[src].append(tgt)
        else:
            src_tgt_dict[src] = [tgt]
            
    
    return num_nodes, train_edge, src_tgt_dict, v_feat, a_feat, t_feat


class TrainingDataset(Dataset):
    def __init__(self, num_nodes, src_tgt_dict, edge_index):
        self.edge_index = edge_index
        self.num_nodes = num_nodes
        self.src_tgt_dict = src_tgt_dict
        self.all_set = set(range(num_nodes))

    def __len__(self):
        return len(self.edge_index)

    def __getitem__(self, index):
        src, pos_tgt = self.edge_index[index]
        src = src.item()
        pos_tgt = pos_tgt.item()
        while True:
            neg_tgt = random.randint(0, self.num_nodes-1)
            if neg_tgt not in self.src_tgt_dict[src]:
                break
        return torch.LongTensor([src, src]), torch.LongTensor([pos_tgt, neg_tgt])

</file>

<file path="lp/MMGCN/Full_vt.py">
from tqdm import tqdm
import torch
import torch.nn as nn
from torch.autograd import no_grad
import numpy as np

def full_vt(epoch, model, data, prefix, writer=None):   
    print(prefix+' start...')
    model.eval()

    with no_grad():
        precision, recall, ndcg_score = model.full_accuracy(data)
        print('---------------------------------{0}-th Precition:{1:.4f} Recall:{2:.4f} NDCG:{3:.4f}---------------------------------'.format(
            epoch, precision, recall, ndcg_score))
        if writer is not None:
            writer.add_scalar(prefix+'_Precition', precision, epoch)
            writer.add_scalar(prefix+'_Recall', recall, epoch)
            writer.add_scalar(prefix+'_NDCG', ndcg_score, epoch)

            writer.add_histogram(prefix+'_visual_distribution', model.v_rep, epoch)
            writer.add_histogram(prefix+'_acoustic_distribution', model.a_rep, epoch)
            writer.add_histogram(prefix+'_textual_distribution', model.t_rep, epoch)
            
            writer.add_histogram(prefix+'_user_visual_distribution', model.user_preferences[:,:44], epoch)
            writer.add_histogram(prefix+'_user_acoustic_distribution', model.user_preferences[:, 44:-44], epoch)
            writer.add_histogram(prefix+'_user_textual_distribution', model.user_preferences[:, -44:], epoch)

            writer.add_embedding(model.v_rep)
            writer.add_embedding(model.a_rep)
            writer.add_embedding(model.t_rep)
            
        return precision, recall, ndcg_score




</file>

<file path="lp/MMGCN/Model_MMGCN.py">
import math
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import Parameter
from BaseModel import BaseModel

class GCN(torch.nn.Module):
    def __init__(self, edge_index, batch_size, num_node, dim_feat, dim_id, aggr_mode, concate, has_id, num_layers, dim_latent=None):
        super(GCN, self).__init__()
        self.batch_size = batch_size
        self.num_node = num_node
        self.dim_id = dim_id
        self.dim_feat = dim_feat
        self.dim_latent = dim_latent
        self.edge_index = edge_index
        self.aggr_mode = aggr_mode
        self.concate = concate
        self.has_id = has_id
        self.num_layers = num_layers

        if self.dim_latent:
            self.MLP = nn.Linear(self.dim_feat, self.dim_latent)
            self.conv_embed_1 = BaseModel(self.dim_latent, self.dim_latent, aggr=self.aggr_mode)
            nn.init.xavier_normal_(self.conv_embed_1.weight)
            self.linear_layer1 = nn.Linear(self.dim_latent, self.dim_id)
            nn.init.xavier_normal_(self.linear_layer1.weight)
            self.g_layer1 = nn.Linear(self.dim_latent+self.dim_id, self.dim_id) if self.concate else nn.Linear(self.dim_latent, self.dim_id)    
            nn.init.xavier_normal_(self.g_layer1.weight) 

        else:
            self.conv_embed_1 = BaseModel(self.dim_feat, self.dim_feat, aggr=self.aggr_mode)
            nn.init.xavier_normal_(self.conv_embed_1.weight)
            self.linear_layer1 = nn.Linear(self.dim_feat, self.dim_id)
            nn.init.xavier_normal_(self.linear_layer1.weight)
            self.g_layer1 = nn.Linear(self.dim_feat+self.dim_id, self.dim_id) if self.concate else nn.Linear(self.dim_feat, self.dim_id)     
            nn.init.xavier_normal_(self.g_layer1.weight)              
          
        if num_layers >= 2:
            self.conv_embed_2 = BaseModel(self.dim_id, self.dim_id, aggr=self.aggr_mode)
            nn.init.xavier_normal_(self.conv_embed_2.weight)
            self.linear_layer2 = nn.Linear(self.dim_id, self.dim_id)
            nn.init.xavier_normal_(self.linear_layer2.weight)
            self.g_layer2 = nn.Linear(self.dim_id+self.dim_id, self.dim_id) if self.concate else nn.Linear(self.dim_id, self.dim_id)

        if num_layers >= 3:
            self.conv_embed_3 = BaseModel(self.dim_id, self.dim_id, aggr=self.aggr_mode)
            nn.init.xavier_normal_(self.conv_embed_3.weight)
            self.linear_layer3 = nn.Linear(self.dim_id, self.dim_id)
            nn.init.xavier_normal_(self.linear_layer3.weight)
            self.g_layer3 = nn.Linear(self.dim_id+self.dim_id, self.dim_id) if self.concate else nn.Linear(self.dim_id, self.dim_id)
        
        if num_layers >= 4:
            self.conv_embed_4 = BaseModel(self.dim_id, self.dim_id, aggr=self.aggr_mode)
            nn.init.xavier_normal_(self.conv_embed_4.weight)
            self.linear_layer4 = nn.Linear(self.dim_id, self.dim_id)
            nn.init.xavier_normal_(self.linear_layer4.weight)
            self.g_layer4 = nn.Linear(self.dim_id+self.dim_id, self.dim_id) if self.concate else nn.Linear(self.dim_id, self.dim_id)
        
        if num_layers >= 5:
            self.conv_embed_5 = BaseModel(self.dim_id, self.dim_id, aggr=self.aggr_mode)
            nn.init.xavier_normal_(self.conv_embed_5.weight)
            self.linear_layer5 = nn.Linear(self.dim_id, self.dim_id)
            nn.init.xavier_normal_(self.linear_layer5.weight)
            self.g_layer5 = nn.Linear(self.dim_id+self.dim_id, self.dim_id) if self.concate else nn.Linear(self.dim_id, self.dim_id)

    def forward(self, features, id_embedding):
        temp_features = self.MLP(features) if self.dim_latent else features

        x = temp_features
        x = F.normalize(x).cuda()

        h = F.leaky_relu(self.conv_embed_1(x, self.edge_index))#equation 1
        x_hat = F.leaky_relu(self.linear_layer1(x)) + id_embedding if self.has_id else F.leaky_relu(self.linear_layer1(x))#equation 5 
        x = F.leaky_relu(self.g_layer1(torch.cat((h, x_hat), dim=1))) if self.concate else F.leaky_relu(self.g_layer1(h)+x_hat)

        if self.num_layers >= 2:
            h = F.leaky_relu(self.conv_embed_2(x, self.edge_index))#equation 1
            x_hat = F.leaky_relu(self.linear_layer2(x)) + id_embedding if self.has_id else F.leaky_relu(self.linear_layer2(x))#equation 5
            x = F.leaky_relu(self.g_layer2(torch.cat((h, x_hat), dim=1))) if self.concate else F.leaky_relu(self.g_layer2(h)+x_hat)

        if self.num_layers >= 3:
            h = F.leaky_relu(self.conv_embed_3(x, self.edge_index))#equation 1
            x_hat = F.leaky_relu(self.linear_layer3(x)) + id_embedding if self.has_id else F.leaky_relu(self.linear_layer3(x))#equation 5
            x = F.leaky_relu(self.g_layer3(torch.cat((h, x_hat), dim=1))) if self.concate else F.leaky_relu(self.g_layer3(h)+x_hat)
        
        if self.num_layers >= 4:
            h = F.leaky_relu(self.conv_embed_4(x, self.edge_index))#equation 1
            x_hat = F.leaky_relu(self.linear_layer4(x)) + id_embedding if self.has_id else F.leaky_relu(self.linear_layer4(x))#equation 6
            x = F.leaky_relu(self.g_layer4(torch.cat((h, x_hat), dim=1))) if self.concate else F.leaky_relu(self.g_layer4(h)+x_hat)
        
        if self.num_layers >= 5:
            h = F.leaky_relu(self.conv_embed_5(x, self.edge_index))#equation 1
            x_hat = F.leaky_relu(self.linear_layer5(x)) + id_embedding if self.has_id else F.leaky_relu(self.linear_layer5(x))#equation 7
            x = F.leaky_relu(self.g_layer5(torch.cat((h, x_hat), dim=1))) if self.concate else F.leaky_relu(self.g_layer5(h)+x_hat)

        return x


class Net(torch.nn.Module):
    def __init__(self, v_feat, a_feat, t_feat, words_tensor, edge_index, batch_size, num_node, aggr_mode, concate, num_layers, has_id, src_tgt_dict, reg_weight, dim_x):
        super(Net, self).__init__()
        self.batch_size = batch_size
        self.num_node = num_node
        self.aggr_mode = aggr_mode
        self.concate = concate
        self.src_tgt_dict = src_tgt_dict
        self.weight = torch.tensor([[1.0],[-1.0]]).cuda()
        self.reg_weight = reg_weight
        
        self.edge_index = torch.tensor(edge_index).t().contiguous().cuda()
        self.edge_index = torch.cat((self.edge_index, self.edge_index[[1,0]]), dim=1)
        self.num_modal = 0

        self.v_feat = torch.tensor(v_feat,dtype=torch.float).cuda()
        self.v_gcn = GCN(self.edge_index, batch_size, num_node, self.v_feat.size(1), dim_x, self.aggr_mode, self.concate, num_layers=num_layers, has_id=has_id, dim_latent=256)

        self.a_feat = None
        self.a_gcn = None

        self.t_feat = torch.tensor(t_feat,dtype=torch.float).cuda()
        self.t_gcn = GCN(self.edge_index, batch_size, num_node, self.t_feat.size(1), dim_x, self.aggr_mode, self.concate, num_layers=num_layers, has_id=has_id)

        self.id_embedding = nn.init.xavier_normal_(torch.rand((num_node, dim_x), requires_grad=True)).cuda()
        self.result = nn.init.xavier_normal_(torch.rand((num_node, dim_x))).cuda()


    def forward(self):
        v_rep = self.v_gcn(self.v_feat, self.id_embedding)
        a_rep = None

        # # self.t_feat = torch.tensor(scatter_('mean', self.word_embedding(self.words_tensor[1]), self.words_tensor[0])).cuda()
        t_rep = self.t_gcn(self.t_feat, self.id_embedding)
        
        if a_rep is not None:
            representation = (v_rep+a_rep+t_rep)/3
        else:
            representation = (v_rep+t_rep)/2

        self.result = representation
        return representation

    def loss(self, src_tensor, tgt_tensor):
        src_tensor = src_tensor.view(-1)
        tgt_tensor = tgt_tensor.view(-1)
        out = self.forward()
        src_score = out[src_tensor]
        tgt_score = out[tgt_tensor]
        score = torch.sum(src_score*tgt_score, dim=1).view(-1, 2)
        loss = -torch.mean(torch.log(torch.sigmoid(torch.matmul(score, self.weight))))
        reg_embedding_loss = (self.id_embedding[src_tensor]**2 + self.id_embedding[tgt_tensor]**2).mean()
        reg_loss = self.reg_weight * (reg_embedding_loss)
        return loss+reg_loss, reg_loss, loss, reg_embedding_loss, reg_embedding_loss

</file>

<file path="lp/MMGCN/main.py">
import argparse
import os
import time
import numpy as np
import torch
from Dataset import TrainingDataset, data_load
from Model_MMGCN import Net
from torch.utils.data import DataLoader
from Train import train
from Full_vt import full_vt
# from torch.utils.tensorboard import SummaryWriter
###############################248###########################################
if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--seed', type=int, default=1, help='Seed init.')
    parser.add_argument('--no-cuda', action='store_true', default=False, help='Disables CUDA training.')
    parser.add_argument('--data_path', default='movielens', help='Dataset path')
    parser.add_argument('--save_file', default='', help='Filename')

    parser.add_argument('--PATH_weight_load', default=None, help='Loading weight filename.')
    parser.add_argument('--PATH_weight_save', default=None, help='Writing weight filename.')

    parser.add_argument('--l_r', type=float, default=1e-4, help='Learning rate.')
    parser.add_argument('--weight_decay', type=float, default=1e-5, help='Weight decay.')

    parser.add_argument('--batch_size', type=int, default=1024, help='Batch size.')
    parser.add_argument('--num_epoch', type=int, default=1000, help='Epoch number.')
    parser.add_argument('--num_workers', type=int, default=1, help='Workers number.')

    parser.add_argument('--dim_E', type=int, default=64, help='Embedding dimension.')
    parser.add_argument('--prefix', default='', help='Prefix of save_file.')
    parser.add_argument('--aggr_mode', default='add', help='Aggregation Mode.')
    parser.add_argument('--topK', type=int, default=10, help='Workers number.')

    parser.add_argument('--has_entropy_loss', default='False', help='Has Cross Entropy loss.')
    parser.add_argument('--has_weight_loss', default='False', help='Has Weight Loss.')
    parser.add_argument('--has_v', default='True', help='Has Visual Features.')
    parser.add_argument('--has_a', default='True', help='Has Acoustic Features.')
    parser.add_argument('--has_t', default='True', help='Has Textual Features.')

    args = parser.parse_args()
    
    seed = args.seed
    np.random.seed(seed)
    device = torch.device("cuda:0" if torch.cuda.is_available() and not args.no_cuda else "cpu")
    ##########################################################################################################################################
    data_path = args.data_path
    save_file = args.save_file

    learning_rate = args.l_r
    weight_decay = args.weight_decay
    batch_size = args.batch_size
    num_workers = args.num_workers
    num_epoch = args.num_epoch
    num_routing = args.num_routing
    topK = args.topK
    prefix = args.prefix
    aggr_mode = args.aggr_mode

    has_v = True if args.has_v == 'True' else False
    has_a = True if args.has_a == 'True' else False
    has_t = True if args.has_t == 'True' else False
    has_entropy_loss = True if args.has_entropy_loss == 'True' else False
    has_weight_loss = True if args.has_weight_loss == 'True' else False
    dim_E = args.dim_E
    writer = None#SummaryWriter()
    # with open(data_path+'/result/result{0}_{1}.txt'.format(l_r, weight_decay), 'w') as save_file:
    #     save_file.write('---------------------------------lr: {0} \t Weight_decay:{1} ---------------------------------\r\n'.format(l_r, weight_decay))
    ##########################################################################################################################################
    print('Data loading ...')

    num_user, num_item, train_edge, user_item_dict, v_feat, a_feat, t_feat = data_load(data_path)

    v_feat = torch.tensor(v_feat, dtype=torch.float).cuda() if has_v else None
    a_feat = torch.tensor(a_feat, dtype=torch.float).cuda() if has_a else None
    t_feat = torch.tensor(t_feat, dtype=torch.float).cuda() if has_t else None

    train_dataset = TrainingDataset(num_user, num_item, user_item_dict, train_edge)
    train_dataloader = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=num_workers)

    val_data = np.load('./Data/'+data_path+'/val_full.npy', allow_pickle=True)
    test_data = np.load('./Data/'+data_path+'/test_full.npy', allow_pickle=True)
    print('Data has been loaded.')
    ##########################################################################################################################################
    model = Net(v_feat, a_feat, t_feat, None, train_edge, batch_size, num_user, num_item, 'mean', 'False', 2, True, user_item_dict, weight_decay, dim_E).cuda()
    ##########################################################################################################################################
    optimizer = torch.optim.Adam([{'params': model.parameters(), 'lr': learning_rate}])
    ##########################################################################################################################################
    max_precision = 0.0
    max_recall = 0.0
    max_NDCG = 0.0
    val_max_recall = 0.0
    num_decreases = 0 
    for epoch in range(num_epoch):
        loss = train(epoch, len(train_dataset), train_dataloader, model, optimizer, batch_size, writer)
        if torch.isnan(loss):
            with open('./Data/'+data_path+'/result_{0}.txt'.format(save_file), 'a') as save_file:
                    save_file.write('lr: {0} \t Weight_decay:{1} is Nan\r\n'.format(learning_rate, weight_decay))
            break
        torch.cuda.empty_cache()

        val_precision, val_recall, val_ndcg = full_vt(epoch, model, val_data, 'Val', writer)
        test_precision, test_recall, test_ndcg = full_vt(epoch, model, test_data, 'Test', writer)

        if val_recall > val_max_recall:
            val_max_recall = val_recall
            max_precision = test_precision
            max_recall = test_recall
            max_NDCG = test_ndcg
            num_decreases = 0
        else:
            if num_decreases > 20:
                with open('./Data/'+data_path+'/result_{0}.txt'.format(save_file), 'a') as save_file:
                    save_file.write('lr: {0} \t Weight_decay:{1} =====> Precision:{2} \t Recall:{3} \t NDCG:{4}\r\n'.
                                    format(learning_rate, weight_decay, max_precision, max_recall, max_NDCG))
                break
            else:
                num_decreases += 1

</file>

<file path="lp/MMGCN/mmgcn.sh">

python run.py \
    --l_r 0.002 \
    --weight_decay 1e-5 \
    --batch_size 1024 \
    --num_epoch 100 \
    --num_workers 4 \
    --aggr_mode mean \
    --num_layers 2 \
    --has_a False \
    --edge_split_path /nfs/turbo/coe-dkoutra/jing/Next-GraphGPT/Patton/sports/lp-edge-split.pt \
    --feat_path /nfs/turbo/coe-dkoutra/jing/Next-GraphGPT/Patton/cloth/clip_feat.pt \
    --project_name mmgcn_cloth \
    --wandb_run_name hard_clip\
    --PATH_best_weight_save './mmgcn_cloth_ckpt.pt' \
    --PATH_best_metrics_save './mmgcn_cloth_best_metrics.txt' \
    --PATH_best_hyperparameter_save './mmgcn_cloth_best_hyperparameter.txt'

</file>

<file path="lp/MMGCN/run.py">
import argparse
import sys
import os
import time
import random
import numpy as np
import torch
from Dataset import TrainingDataset, data_load, data_load_old
from Model_MMGCN import Net
from torch.utils.data import DataLoader, TensorDataset
from Train import train
from Full_vt import full_vt
import wandb
from tqdm import tqdm
import optuna


def get_rank(pred, n=0, descending=True):
    """
    Calculate the rank of the nth element in pred
    descending=True means large values ranks higher,
    descending=False means small values ranks higher.
    """
    arg = torch.argsort(torch.tensor(pred), descending=descending)
    rank = torch.where(arg == n)[0] + 1
    return rank.tolist()[0]


def get_rank_2d(pred, n=0, descending=True):
    """
    Calculate the rank of the nth element in each row of pred.
    `pred` is a 2D tensor where each row contains scores for a single sample.
    `n` specifies the index of the element to find the rank for in each row.
    `descending` controls the order of ranking.
    """
    batch_size, num_elements = pred.shape
    # Create a tensor of the same shape as pred filled with the index n
    n_tensor = torch.full_like(pred, n, dtype=torch.long)

    # Sort pred in the specified order
    sorted_indices = torch.argsort(pred, dim=1, descending=descending)

    # Find the rank (position) of the nth element in each row
    ranks = torch.nonzero(sorted_indices == n_tensor, as_tuple=False)[:, 1] + 1  # Adding 1 to make ranks start from 1

    return ranks


def calculate_mrr(rank_ls):
    """
    Return the MRR (Mean Reciprocal Rank) of a list of ranks.
    """
    if type(rank_ls) is list:
        rk = np.array(rank_ls)
    else:
        rk = rank_ls
    return (1 / rk).mean()


def calculate_hit(rank_ls, n):
    """
    Return the Hit@n of a list of ranks.
    """
    if type(rank_ls) is list:
        rk = np.array(rank_ls)
    else:
        rk = rank_ls
    rk = np.array(rank_ls)
    return rk[rk <= n].shape[0] / rk.shape[0]


def batch_evaluate_model(representation, triple_data, batch_size=128, verbose=False):
    dataset = TensorDataset(
        torch.tensor(triple_data['source_node']),
        torch.tensor(triple_data['target_node']),
        torch.tensor(triple_data['target_node_neg'], dtype=torch.long))

    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)

    rank_ls = []
    for src, pos_tgt, neg_tgt_ls in tqdm(dataloader, disable=not verbose):
        src_ls = src.repeat_interleave(neg_tgt_ls.size(1) + 1, dim=0)
        tgt_ls = torch.cat((pos_tgt.unsqueeze(1), neg_tgt_ls), dim=1).view(-1)

        src_score = representation[src_ls]
        tgt_score = representation[tgt_ls]

        pred = torch.sum(src_score * tgt_score, dim=1).detach().cpu()
        rank = get_rank_2d(pred.view(src.size(0), -1))
        rank_ls.extend(rank.numpy().tolist())

    hit_10 = calculate_hit(rank_ls, 10)
    hit_1 = calculate_hit(rank_ls, 1)
    mrr = calculate_mrr(rank_ls)
    return mrr, hit_1, hit_10


def train(dataloader, model, optimizer, batch_size, max_step=None):
    model.train()
    sum_loss = 0.0
    sum_model_loss = 0.0
    sum_reg_loss = 0.0
    sum_ent_loss = 0.0
    sum_weight_loss = 0.0
    step = 0.0
    num_pbar = 0
    for idx, batch in enumerate(tqdm(dataloader)):
        src_tensor, tgt_tensor = batch
        optimizer.zero_grad()
        loss, model_loss, reg_loss, weight_loss, entropy_loss = model.loss(src_tensor, tgt_tensor)
        loss.backward(retain_graph=True)
        optimizer.step()
        num_pbar += batch_size
        sum_loss += loss.cpu().item()
        sum_model_loss += model_loss.cpu().item()
        sum_reg_loss += reg_loss.cpu().item()
        sum_ent_loss += entropy_loss.cpu().item()
        sum_weight_loss += weight_loss.cpu().item()
        step += 1.0
        if max_step is not None and idx > max_step:
            break

    return loss


def hyperparameter_search(train_dataloader, args, v_feat, a_feat, t_feat, train_edge, num_nodes, src_tgt_dict,
                          weight_decay, dim_E, val_triple_data):
    def objective(trial):
        lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)
        num_layers = trial.suggest_int('num_layers', 1, 3)

        model = Net(v_feat, a_feat, t_feat, None, train_edge, args.batch_size, num_nodes, 'mean', 'False', num_layers,
                    True, src_tgt_dict, weight_decay, dim_E).cuda()
        optimizer = torch.optim.Adam([{'params': model.parameters(), 'lr': lr}])

        # loss = train(train_dataloader, model, optimizer, args.batch_size, max_step=args.hyperparameter_search_max_step)
        #
        # with torch.no_grad():
        #     model.eval()
        #     representation = model.forward()
        #     val_mrr, val_hit_1, val_hit_10 = batch_evaluate_model(representation, val_triple_data, verbose=False)
        #
        # return val_mrr
        val_max_mrr = 0
        for epoch in range(args.num_epoch):
            train(train_dataloader, model, optimizer, args.batch_size, max_step=None)
            torch.cuda.empty_cache()

            if (epoch + 1) % 5 == 0:
                with torch.no_grad():
                    representation = model.forward()
                    val_mrr, val_hit_1, val_hit_10 = batch_evaluate_model(representation, val_triple_data,
                                                                          verbose=False)
                    if val_mrr > val_max_mrr:
                        val_max_mrr = val_mrr
                    print(f"{lr}, {num_layers}")
                    print(f"Epoch: {epoch}, MRR: {val_max_mrr}, Hit@1: {val_hit_1}, Hit@10: {val_hit_10}")
        return val_max_mrr

    study = optuna.create_study(direction='maximize')
    study.optimize(objective, n_trials=args.n_trials)

    print(study.best_params)

    if args.PATH_best_hyperparameter_save is not None:
        with open(args.PATH_best_hyperparameter_save, 'w') as f:
            f.write(study.best_params)
            f.write('\n')
            f.write(study.best_value)


def main():
    parser = argparse.ArgumentParser()

    # using default values is recommended
    parser.add_argument('--seed', type=int, default=1, help='Seed init.')
    parser.add_argument('--no-cuda', action='store_true', default=False, help='Disables CUDA training.')
    parser.add_argument('--data_path', default='movielens', help='Dataset path')
    parser.add_argument('--save_file', default='', help='Filename')
    parser.add_argument('--PATH_weight_load', default=None, help='Loading weight filename.')
    parser.add_argument('--num_workers', type=int, default=1, help='Workers number.')
    parser.add_argument('--dim_E', type=int, default=64, help='Embedding dimension.')
    parser.add_argument('--prefix', default='', help='Prefix of save_file.')
    parser.add_argument('--aggr_mode', default='add', help='Aggregation Mode.')
    parser.add_argument('--topK', type=int, default=10, help='Workers number.')
    parser.add_argument('--has_entropy_loss', default='False', help='Has Cross Entropy loss.')
    parser.add_argument('--has_weight_loss', default='False', help='Has Weight Loss.')
    parser.add_argument('--has_v', default='True', help='Has Visual Features.')
    parser.add_argument('--has_a', default='True', help='Has Acoustic Features.')
    parser.add_argument('--has_t', default='True', help='Has Textual Features.')
    parser.add_argument('--weight_decay', type=float, default=1e-5, help='Weight decay.')

    # your data here
    parser.add_argument('--edge_split_path', required=True)
    parser.add_argument('--v_feat_path', default=None)
    parser.add_argument('--t_feat_path', default=None)
    parser.add_argument('--feat_path', default=None)

    # your setting here
    parser.add_argument('--l_r', type=float, default=1e-4, help='Learning rate.')
    parser.add_argument('--num_layers', default=3, type=int)
    parser.add_argument('--batch_size', type=int, default=1024, help='Batch size.')
    parser.add_argument('--num_epoch', type=int, default=1000, help='Epoch number.')
    parser.add_argument('--repeat_times', type=int, default=3, help='Repeat times.')

    parser.add_argument('--PATH_best_weight_save', default=None, help='Writing weight filename.')
    parser.add_argument('--PATH_best_metrics_save', default=None)

    # if want to do hyperparameter search
    parser.add_argument('--do_hyperparameter_search', action="store_true")
    parser.add_argument('--PATH_best_hyperparameter_save', default=None)
    parser.add_argument('--n_trials', type=int, default=20)
    parser.add_argument('--hyperparameter_search_max_step', type=int, default=10000)

    parser.add_argument('--project_name', default='MGAT')
    parser.add_argument('--wandb_run_name', default='untitled_run')
    parser.add_argument('--wandb_key', default=None)
    parser.add_argument('--report_to', default=None)

    args = parser.parse_args()

    if args.wandb_key is not None and args.report_to == 'wandb':
        wandb_key = 'ab1e2fd95c62273341f8fe8bbe29b9a6ee33725a'
        wandb.login(key=wandb_key)

    seed = args.seed
    np.random.seed(seed)
    device = torch.device("cuda:0" if torch.cuda.is_available() and not args.no_cuda else "cpu")
    ##########################################################################################################################################
    data_path = args.data_path
    save_file = args.save_file

    learning_rate = args.l_r
    weight_decay = args.weight_decay
    batch_size = args.batch_size
    num_workers = args.num_workers
    num_epoch = args.num_epoch
    topK = args.topK
    prefix = args.prefix
    aggr_mode = args.aggr_mode

    has_v = True if args.has_v == 'True' else False
    has_a = True if args.has_a == 'True' else False
    has_t = True if args.has_t == 'True' else False

    has_entropy_loss = True if args.has_entropy_loss == 'True' else False
    has_weight_loss = True if args.has_weight_loss == 'True' else False
    dim_E = args.dim_E
    writer = None

    edge_split = torch.load(args.edge_split_path)
    train_edge = torch.concat(
        [edge_split['train']['source_node'].reshape(-1, 1), edge_split['train']['target_node'].reshape(-1, 1)], dim=1)
    train_edge.shape

    ##########################################################################################################################################
    print('Data loading ...')
    if args.feat_path is None:
        num_nodes, train_edge, src_tgt_dict, v_feat, a_feat, t_feat = data_load_old(train_edge, args.v_feat_path,
                                                                                    args.t_feat_path)
    else:
        num_nodes, train_edge, src_tgt_dict, v_feat, a_feat, t_feat = data_load(train_edge, args.feat_path)

    train_dataset = TrainingDataset(num_nodes, src_tgt_dict, train_edge)

    val_triple_data = edge_split['valid']
    test_triple_data = edge_split['test']

    print('Data has been loaded.')

    if args.do_hyperparameter_search:
        train_dataloader = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=4)
        hyperparameter_search(train_dataloader, args, v_feat, a_feat, t_feat, train_edge, num_nodes, src_tgt_dict,
                              weight_decay, dim_E, val_triple_data)
        return

    num_layers = args.num_layers
    learning_rate = args.l_r
    num_epoch = args.num_epoch
    batch_size = args.batch_size

    global_step = 0
    mrrs, h1s, h10s = [], [], []
    for try_time in range(args.repeat_times):
        model = Net(v_feat, a_feat, t_feat, None, train_edge, batch_size, num_nodes, 'mean', 'False', num_layers, True,
                    src_tgt_dict, weight_decay, dim_E).cuda()
        optimizer = torch.optim.Adam([{'params': model.parameters(), 'lr': learning_rate}])

        if args.report_to == 'wandb':
            wandb.init(
                project=args.project_name,
                name=args.wandb_run_name,
                config={
                    "learning_rate": learning_rate,
                    'num_layers': num_layers,
                    "epochs": num_epoch,
                }
            )

        max_mrr = 0.0
        max_hit_1 = 0.0
        max_hit_10 = 0.0
        val_max_mrr = 0.0
        num_decreases = 0
        for epoch in range(num_epoch):
            train_dataloader = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=4)
            loss = train(train_dataloader, model, optimizer, batch_size, max_step=None)
            torch.cuda.empty_cache()

            with torch.no_grad():
                representation = model.forward()
                val_mrr, val_hit_1, val_hit_10 = batch_evaluate_model(representation, val_triple_data, verbose=False)
                test_mrr, test_hit_1, test_hit_10 = batch_evaluate_model(representation, test_triple_data,
                                                                         verbose=False)

            print({
                'loss': loss,
                "val_mrr": val_mrr,
                'val_hit_1': val_hit_1,
                'val_hit_10': val_hit_10,
                'test_mrr': test_mrr,
                'test_hit_1': test_hit_1,
                'test_hit_10': test_hit_10,
            })

            if val_mrr > val_max_mrr:
                val_max_mrr = val_mrr
                max_mrr = test_mrr
                max_hit_1 = test_hit_1
                max_hit_10 = test_hit_10
                num_decreases = 0

                if args.PATH_best_weight_save is not None:
                    torch.save(model.state_dict(), args.PATH_best_weight_save)

                if args.PATH_best_metrics_save is not None:
                    with open(args.PATH_best_metrics_save, 'w') as f:
                        f.write(f"MRR: {test_mrr}")
                        f.write(f"Hit@1: {test_hit_1}")
                        f.write(f"Hit@10: {test_hit_10}")
            global_step += 1

        mrrs.append(test_mrr)
        h1s.append(test_hit_1)
        h10s.append(test_hit_10)

    mean_mrr = torch.mean(torch.tensor(mrrs)).item()
    std_mrr = torch.std(torch.tensor(mrrs)).item()
    mean_h1 = torch.mean(torch.tensor(h1s)).item()
    std_h1 = torch.std(torch.tensor(h1s)).item()
    mean_h10 = torch.mean(torch.tensor(h10s)).item()
    std_h10 = torch.std(torch.tensor(h10s)).item()
    print(mean_mrr)
    print(std_mrr)
    print(mean_h1)
    print(std_h1)
    print(mean_h10)
    print(std_h10)


if __name__ == '__main__':
    main()

</file>

<file path="lp/configs/defaults.yaml">
defaults:
  # - override hydra/launcher: submitit_slurm
  - _self_

mode: 'puregpu'
model_name: 'SAGE'
checkpoint_folder: 'output/'
load_checkpoint_folder: None
dataset: ogbn-arxiv
dataset_finetuned: False
nodes_image_ratio: 0.1
overlap_ratio: 0.1
n_epochs: 30
runs: 3
log_steps: 1
accum_iter_number: 1
lr: 5e-4
hidden_dim: 256
num_layers: 3
batch_size: 512
exclude_target_degree: 0
num_of_neighbors: 15
full_neighbor: False
inference_mode: 'train'
preload_node_emb: True
add_self_loop: False
eval_dir: ''
use_feature: 'original'
feat: 'clip'
use_concat: False
no_eval: False
lr_scheduler_step_size: 5
lr_scheduler_gamma: 0.1
patton_dir: /nfs/turbo/coe-dkoutra/jing/Next-GraphGPT/Patton/cloth-coview
hydra:
  job:
    chdir: True
  # launcher:
  #   name: 'gnn'
  #   timeout_min: 4320
  #   cpus_per_task: 4
  #   gres: 'gpu:1'
  #   tasks_per_node: 1
  #   mem_per_cpu: 10000
  #   partition: 'spgpu'
  #   account: 'dkoutra1'
  #   array_parallelism: 3 # limit the number of jobs can be run at the same time
  #   exclude: 'gl1507, gl1510, gl1513'
</file>

<file path="lp/configs/optuna.yaml">
defaults:
  - defaults
  - override hydra/sweeper: optuna
  - _self_


hydra:
  sweeper:
    sampler:
      seed: 123
    direction: maximize
    study_name: gnn
    storage: null
    n_trials: 20
    n_jobs: 1
    # max_failure_rate: 0.0
    params:
      lr: tag(log, interval(1e-4, 1e-2))
      num_layers: choice(1, 2, 3)


</file>

<file path="lp/lp_dataset.py">
import os
import sys
import pickle

import pandas as pd
import numpy as np
import torch
import dgl

class LinkPredictionDataset(object):
    def __init__(self, root: str, feat_name: str, edge_split_type: str, verbose: bool=True, device: str='cpu'):
        """
        Args:
            root (str): root directory to store the dataset folder.
            feat_name (str): the name of the node features, e.g., "t5vit".
            edge_split_type (str): the type of edge split, can be "random" or "hard".
            verbose (bool): whether to print the information.
            device (str): device to use.
        """
        root = os.path.normpath(root)
        self.name = os.path.basename(root)
        self.verbose = verbose
        self.root = root
        self.feat_name = feat_name
        self.edge_split_type = edge_split_type
        self.device = device
        if self.verbose:
            print(f"Dataset name: {self.name}")
            print(f'Feature name: {self.feat_name}')
            print(f'Edge split type: {self.edge_split_type}')
            print(f'Device: {self.device}')
        
        edge_split_path = os.path.join(root, f'lp-edge-split.pt')
        self.edge_split = torch.load(edge_split_path, map_location=self.device)
        feat_path = os.path.join(root, f'{self.feat_name}_feat.pt')
        feat = torch.load(feat_path, map_location='cpu')
        self.num_nodes = feat.shape[0]
        self.graph = dgl.graph((
            self.edge_split['train']['source_node'],
            self.edge_split['train']['target_node'],
        ), num_nodes=self.num_nodes).to('cpu')
        self.graph.ndata['feat'] = feat

    def get_edge_split(self):
        return self.edge_split

    def __getitem__(self, idx: int):
        assert idx == 0, 'This dataset has only one graph'
        return self.graph

    def __len__(self):
        return 1
    
    def __repr__(self):
        return '{}({})'.format(self.__class__.__name__, len(self))
    

# borrowed from OGB
class LinkPredictionEvaluator(object):
    def __init__(self):
        """
        Calculate MRR, H@1, H@3, and H@10 
        """
        return
    
    def _parse_and_check_input(self, input_dict):
        if not 'y_pred_pos' in input_dict:
            raise RuntimeError('Missing key of y_pred_pos')
        if not 'y_pred_neg' in input_dict:
            raise RuntimeError('Missing key of y_pred_neg')

        y_pred_pos, y_pred_neg = input_dict['y_pred_pos'], input_dict['y_pred_neg']

        '''
            y_pred_pos: numpy ndarray or torch tensor of shape (num_edges, )
            y_pred_neg: numpy ndarray or torch tensor of shape (num_edges, num_nodes_negative)
        '''

        # convert y_pred_pos, y_pred_neg into either torch tensor or both numpy array
        # type_info stores information whether torch or numpy is used

        type_info = None

        # check the raw tyep of y_pred_pos
        if not (isinstance(y_pred_pos, np.ndarray) or (torch is not None and isinstance(y_pred_pos, torch.Tensor))):
            raise ValueError('y_pred_pos needs to be either numpy ndarray or torch tensor')

        # check the raw type of y_pred_neg
        if not (isinstance(y_pred_neg, np.ndarray) or (torch is not None and isinstance(y_pred_neg, torch.Tensor))):
            raise ValueError('y_pred_neg needs to be either numpy ndarray or torch tensor')

        # if either y_pred_pos or y_pred_neg is torch tensor, use torch tensor
        if torch is not None and (isinstance(y_pred_pos, torch.Tensor) or isinstance(y_pred_neg, torch.Tensor)):
            # converting to torch.Tensor to numpy on cpu
            if isinstance(y_pred_pos, np.ndarray):
                y_pred_pos = torch.from_numpy(y_pred_pos)

            if isinstance(y_pred_neg, np.ndarray):
                y_pred_neg = torch.from_numpy(y_pred_neg)

            # put both y_pred_pos and y_pred_neg on the same device
            y_pred_pos = y_pred_pos.to(y_pred_neg.device)

            type_info = 'torch'


        else:
            # both y_pred_pos and y_pred_neg are numpy ndarray

            type_info = 'numpy'


        if not y_pred_pos.ndim == 1:
            raise RuntimeError('y_pred_pos must to 1-dim arrray, {}-dim array given'.format(y_pred_pos.ndim))

        if not y_pred_neg.ndim == 2:
            raise RuntimeError('y_pred_neg must to 2-dim arrray, {}-dim array given'.format(y_pred_neg.ndim))

        return y_pred_pos, y_pred_neg, type_info

    def eval(self, input_dict):
        y_pred_pos, y_pred_neg, type_info = self._parse_and_check_input(input_dict)
        return self._eval_mrr(y_pred_pos, y_pred_neg, type_info)

    @property
    def expected_input_format(self):
        desc = '==== Expected input format of Evaluator\n'
        desc += '{\'y_pred_pos\': y_pred_pos, \'y_pred_neg\': y_pred_neg}\n'
        desc += '- y_pred_pos: numpy ndarray or torch tensor of shape (num_edges, ). Torch tensor on GPU is recommended for efficiency.\n'
        desc += '- y_pred_neg: numpy ndarray or torch tensor of shape (num_edges, num_nodes_neg). Torch tensor on GPU is recommended for efficiency.\n'
        desc += 'y_pred_pos is the predicted scores for positive edges.\n'
        desc += 'y_pred_neg is the predicted scores for negative edges. It needs to be a 2d matrix.\n'
        desc += 'y_pred_pos[i] is ranked among y_pred_neg[i].\n'
        desc += 'Note: As the evaluation metric is ranking-based, the predicted scores need to be different for different edges.'

        return desc

    @property
    def expected_output_format(self):
        desc = '==== Expected output format of Evaluator\n'
        desc += '{' + '\'hits@1_list\': hits@1_list, \'hits@3_list\': hits@3_list, \n\'hits@10_list\': hits@10_list, \'mrr_list\': mrr_list}\n'
        desc += '- mrr_list (list of float): list of scores for calculating MRR \n'
        desc += '- hits@1_list (list of float): list of scores for calculating Hits@1 \n'
        desc += '- hits@3_list (list of float): list of scores to calculating Hits@3\n'
        desc += '- hits@10_list (list of float): list of scores to calculating Hits@10\n'
        desc += 'Note: i-th element corresponds to the prediction score for the i-th edge.\n'
        desc += 'Note: To obtain the final score, you need to concatenate the lists of scores and take average over the concatenated list.'

        return desc

    def _eval_mrr(self, y_pred_pos, y_pred_neg, type_info):
        '''
            compute mrr
            y_pred_neg is an array with shape (batch size, num_entities_neg).
            y_pred_pos is an array with shape (batch size, )
        '''

        if type_info == 'torch':
            # calculate ranks
            y_pred_pos = y_pred_pos.view(-1, 1)
            # optimistic rank: "how many negatives have a larger score than the positive?"
            # ~> the positive is ranked first among those with equal score
            optimistic_rank = (y_pred_neg > y_pred_pos).sum(dim=1)
            # pessimistic rank: "how many negatives have at least the positive score?"
            # ~> the positive is ranked last among those with equal score
            pessimistic_rank = (y_pred_neg >= y_pred_pos).sum(dim=1)
            ranking_list = 0.5 * (optimistic_rank + pessimistic_rank) + 1
            hits1_list = (ranking_list <= 1).to(torch.float)
            hits3_list = (ranking_list <= 3).to(torch.float)
            hits10_list = (ranking_list <= 10).to(torch.float)
            mrr_list = 1./ranking_list.to(torch.float)

            return {'hits@1_list': hits1_list,
                     'hits@3_list': hits3_list,
                     'hits@10_list': hits10_list,
                     'mrr_list': mrr_list}

        else:
            y_pred_pos = y_pred_pos.reshape(-1, 1)
            optimistic_rank = (y_pred_neg >= y_pred_pos).sum(dim=1)
            pessimistic_rank = (y_pred_neg > y_pred_pos).sum(dim=1)
            ranking_list = 0.5 * (optimistic_rank + pessimistic_rank) + 1
            hits1_list = (ranking_list <= 1).astype(np.float32)
            hits3_list = (ranking_list <= 3).astype(np.float32)
            hits10_list = (ranking_list <= 10).astype(np.float32)
            mrr_list = 1./ranking_list.astype(np.float32)

            return {'hits@1_list': hits1_list,
                     'hits@3_list': hits3_list,
                     'hits@10_list': hits10_list,
                     'mrr_list': mrr_list}

</file>

<file path="lp/main.py">
import torch
from utils import Logger, to_bidirected_with_reverse_mapping, load_usair_dataset, load_esci_dataset, \
    remove_collab_dissimilar_edges
import torch.nn.functional as F
import dgl
from dgl.dataloading import DataLoader, NeighborSampler, MultiLayerFullNeighborSampler, as_edge_prediction_sampler, \
    negative_sampler
import tqdm
from models import SAGE, GCN, MLP, Dot, GATv2
import os
import numpy as np
from sklearn import metrics
import pickle
import pdb
import hydra
import logging
from omegaconf import DictConfig, OmegaConf
import time
import optuna
from lp_dataset import LinkPredictionDataset, LinkPredictionEvaluator

PROJETC_DIR = os.path.dirname(os.path.realpath(__file__))
DATA_PATH = os.path.join(PROJETC_DIR, '../')
CONFIG_DIR = os.path.join(PROJETC_DIR, "configs")
log = logging.getLogger(__name__)

def compute_mrr_esci(
        model, 
        node_emb, 
        src, 
        dst, 
        neg_dst, 
        device, 
        batch_size=500, 
        preload_node_emb=True,
        use_concat = False, 
        use_dot = False
    ):
    """Compute Mean Reciprocal Rank (MRR) in batches in esci dataset."""

    # gpu may be out of memory for large datasets
    if preload_node_emb:
        node_emb = node_emb.to(device)

    rr = torch.zeros(src.shape[0])
    hits_at_10 = torch.zeros(src.shape[0])
    hits_at_1 = torch.zeros(src.shape[0])
    for start in tqdm.trange(0, src.shape[0], batch_size, desc='Evaluate'):
        end = min(start + batch_size, src.shape[0])
        all_dst = torch.cat([dst[start:end, None], neg_dst[start:end]], 1)
        h_src = node_emb[src[start:end]][:, None, :].to(device)
        h_dst = node_emb[all_dst.view(-1)].view(*all_dst.shape, -1).to(device)
        if use_concat:
            h_src = h_src.repeat(1, 1001, 1)
            pred = model.predictor(torch.cat((h_src, h_dst), dim=2)).squeeze(-1)
        elif use_dot:
            pred = model.decoder(h_src * h_dst).squeeze(-1)
        else:
            pred = model.predictor(h_src * h_dst).squeeze(-1)
        #import pdb; pdb.set_trace()
        y_pred_pos = pred[:, 0]
        y_pred_neg = pred[:, 1:]
        y_pred_pos = y_pred_pos.view(-1, 1)
        # optimistic rank: "how many negatives have at least the positive score?"
        # ~> the positive is ranked first among those with equal score
        optimistic_rank = (y_pred_neg >= y_pred_pos).sum(dim=1)
        # pessimistic rank: "how many negatives have a larger score than the positive?"
        # ~> the positive is ranked last among those with equal score
        pessimistic_rank = (y_pred_neg > y_pred_pos).sum(dim=1)
        ranking_list = 0.5 * (optimistic_rank + pessimistic_rank) + 1
        hits_at_10[start:end] = ranking_list<=10
        hits_at_1[start:end] = ranking_list<=1
        mrr_list = 1. / ranking_list.to(torch.float)
        rr[start:end] = mrr_list
    MRR = rr.mean()
    Hits_10 = hits_at_10.sum()/src.shape[0]
    Hits_1 = hits_at_1.sum()/src.shape[0]

    return MRR, Hits_10, Hits_1




def train(cfg, device, g, reverse_eids, seed_edges, model, edge_split, logger, run, eval_batch_size=1000):
    # create sampler & dataloader
    total_it = 1000 * 512 / cfg.batch_size
    if not os.path.exists(cfg.checkpoint_folder):
        os.makedirs(cfg.checkpoint_folder)
    checkpoint_path = cfg.checkpoint_folder + cfg.model_name + "_" + cfg.dataset + "_" + "batch_size_" + str(
        cfg.batch_size) + "_n_layers_" + str(cfg.num_layers) + "_hidden_dim_" + str(cfg.hidden_dim) + "_lr_" + str(
        cfg.lr) + "_exclude_degree_" + str(cfg.exclude_target_degree) + "_full_neighbor_" + str(
        cfg.full_neighbor) + "_accu_num_" + str(cfg.accum_iter_number) + "_trail_" + str(run) + "_best.pth"
    if cfg.full_neighbor:
        log.info("We use the full neighbor of the target node to train the models. ")
        sampler = MultiLayerFullNeighborSampler(num_layers=cfg.num_layers, prefetch_node_feats=['feat'])
    else:
        log.info("We sample the neighbor node of the target node to train the models. ")
        sampler = NeighborSampler([cfg.num_of_neighbors] * cfg.num_layers, prefetch_node_feats=['feat'])
    log.info("We exclude the training target. ")
    sampler = as_edge_prediction_sampler(
        sampler, exclude="reverse_id", reverse_eids=reverse_eids, negative_sampler=negative_sampler.Uniform(1))
    use_uva = (cfg.mode == 'mixed')
    dataloader = DataLoader(
        g, seed_edges, sampler,
        device=device, batch_size=cfg.batch_size, shuffle=True,
        drop_last=False, num_workers=0, use_uva=use_uva)
    opt = torch.optim.Adam(model.parameters(), lr=cfg.lr)
    lr_scheduler = torch.optim.lr_scheduler.StepLR(
        opt, 
        step_size=cfg.lr_scheduler_step_size,
        gamma=cfg.lr_scheduler_gamma,
    )
    optuna_acc = 0
    for epoch in range(cfg.n_epochs):
        model.train()
        total_loss = 0
        # batch accumulation parameter
        accum_iter = cfg.accum_iter_number

        log.info('Training...')
        for it, (input_nodes, pair_graph, neg_pair_graph, blocks) in enumerate(dataloader):
            # pair_graph: all positive edge pairs in this batch, stored  as a graph
            # neg_pair_graph: all negative edge pairs in this batch, stored as a graph
            # blocks: each block is the aggregated graph as input for each layer
            x = blocks[0].srcdata['feat'].float()
            pos_score, neg_score = model(pair_graph, neg_pair_graph, blocks, x)
            score = torch.cat([pos_score, neg_score])
            pos_label = torch.ones_like(pos_score)
            neg_label = torch.zeros_like(neg_score)
            labels = torch.cat([pos_label, neg_label])
            loss = F.binary_cross_entropy_with_logits(score, labels)
            (loss / accum_iter).backward()
            if ((it + 1) % accum_iter == 0) or (it + 1 == len(dataloader)) or (it + 1 == total_it):
                # Update Optimizer
                opt.step()
                opt.zero_grad()
            total_loss += loss.item()
            if (it + 1) == total_it:
                break

        lr_scheduler.step()

        log.info("Epoch {:05d} | Loss {:.4f}".format(epoch, total_loss / (it + 1)))
        if (epoch + 1) % cfg.log_steps == 0:
            model.eval()
            log.info('Validation/Testing...')
            with torch.no_grad():
                node_emb = model.inference(g, device, eval_batch_size)
                results = []

                log.info("do evaluation on training examples: check if can be overfitted")
                torch.manual_seed(12345)
                num_sampled_nodes = edge_split['valid']['target_node_neg'].size(dim=0)
                idx = torch.randperm(edge_split['train']['source_node'].numel())[:num_sampled_nodes]
                edge_split['eval_train'] = {
                    'source_node': edge_split['train']['source_node'][idx],
                    'target_node': edge_split['train']['target_node'][idx],
                    'target_node_neg': edge_split['valid']['target_node_neg'],
                }

                src = edge_split['eval_train']['source_node'].to(node_emb.device)
                dst = edge_split['eval_train']['target_node'].to(node_emb.device)
                neg_dst = edge_split['eval_train']['target_node_neg'].to(node_emb.device)

                use_concat = cfg.use_concat
                use_dot = cfg.model_name == "Dot"
                mrr, hits_at_10, hits_at_1 = compute_mrr_esci(model, node_emb, src, dst, neg_dst, device, preload_node_emb=cfg.preload_node_emb, use_concat=use_concat, use_dot=use_dot)
                log.info('Train MRR {:.4f} '.format(mrr.item()))
                if cfg.no_eval is False:
                    valid_mrr = []
                    valid_h_10 = []
                    valid_h_1 = []
                    test_mrr = []
                    test_h_10 = []
                    test_h_1 = []
                    for split in ['valid', 'test']:
                        if cfg.dataset == "ogbl-citation2":
                            evaluator = Evaluator(name=cfg.dataset)
                            src = edge_split[split]['source_node'].to(node_emb.device)
                            dst = edge_split[split]['target_node'].to(node_emb.device)
                            neg_dst = edge_split[split]['target_node_neg'].to(node_emb.device)
                            results.append(compute_mrr(model, evaluator, node_emb, src, dst, neg_dst, device))
                        else:
                            src = edge_split[split]['source_node'].to(node_emb.device)
                            dst = edge_split[split]['target_node'].to(node_emb.device)
                            neg_dst = edge_split[split]['target_node_neg'].to(node_emb.device)
                            results.append(
                                compute_mrr_esci(model, node_emb, src, dst, neg_dst, device, preload_node_emb=cfg.preload_node_emb, use_concat=use_concat, use_dot=use_dot)
                            )
                    valid_mrr.append(results[0][0].item())
                    valid_h_10.append(results[0][1].item())
                    valid_h_1.append(results[0][2].item())
                    test_mrr.append(results[1][0].item())
                    test_h_10.append(results[1][1].item())
                    test_h_1.append(results[1][2].item())

                    # save best checkpoint
                    valid_result, test_result = results[0][0].item(), results[1][0].item()
                    
                    # we want to find the best previous checkpoint
                    # if there is no previous checkpoint, set it to 0
                    # Warning: it only works for MRR and Hit@N.
                    if len(logger.results[run]) > 0:
                        previous_best_valid_result = torch.tensor(logger.results[run])[:, 0].max().item()
                    else:  # length = 0
                        previous_best_valid_result = 0.0
                    
                    if valid_result > previous_best_valid_result:
                        log.info("Saving checkpoint. ")
                        torch.save({
                            'epoch': epoch + 1,
                            'model_state_dict': model.state_dict(),
                        }, checkpoint_path)
                        optuna_acc = test_result

                    logger.add_result(run, [valid_result, test_result])
                    log.info('Validation MRR {:.4f}, Test MRR {:.4f}'.format(valid_result, test_result))
                    log.info('Validation Hits@10 {:.4f}, Test Hits@10 {:.4f}'.format(results[0][1].item(), results[1][1].item()))
                    log.info('Validation Hits@1 {:.4f}, Test Hits@1 {:.4f}'.format(results[0][2].item(), results[1][2].item()))
    logger.print_statistics(run)
    return results[1][0].item(), results[1][1].item(), results[1][2].item()


@hydra.main(config_path=CONFIG_DIR, config_name="defaults", version_base='1.2')
def main(cfg: DictConfig):
    log.info('Loading data')
    data_path = '/nfs/turbo/coe-dkoutra/jing/Multimodal-Graph-Completed-Graph' # replace this with the path where you save the datasets
    dataset_name = 'sports-copurchase'
    feat_name = 't5vit'
    edge_split_type = 'hard'
    verbose = True
    device = ('cuda' if cfg.mode == 'puregpu' else 'cpu') # use 'cuda' if GPU is available

    dataset = LinkPredictionDataset(
        root=os.path.join(data_path, dataset_name),
        feat_name=feat_name,
        edge_split_type=edge_split_type,
        verbose=verbose,
        device=device
    )

    g = dataset.graph
    # type(graph) would be dgl.DGLGraph
    # use graph.ndata['feat'] to get the features

    edge_split = dataset.get_edge_split()
    g = dgl.remove_self_loop(g)
    log.info("remove isolated nodes")
    g, reverse_eids = to_bidirected_with_reverse_mapping(g)
    g = g.to('cuda' if cfg.mode == 'puregpu' else 'cpu')
    num_nodes = g.number_of_nodes()
    reverse_eids = reverse_eids.to(device)
    seed_edges = torch.arange(g.num_edges()).to(device)
    
    in_size = g.ndata['feat'].shape[1]
    logger = Logger(cfg.runs)

    mrrs = []
    h1s = []
    h10s = []
    for run in range(cfg.runs):
        log.info("Run {}/{}".format(run + 1, cfg.runs))
        if cfg.model_name == "SAGE":
            model = SAGE(in_size, cfg.hidden_dim, cfg.num_layers).to(device)
        elif cfg.model_name == "GCN":
            model = GCN(in_size, cfg.hidden_dim, cfg.num_layers).to(device)
            if cfg.add_self_loop:
                g = dgl.add_self_loop(g)
        elif cfg.model_name == "MLP":
            model = MLP(in_size, cfg.hidden_dim, cfg.num_layers).to(device)
        elif cfg.model_name == "Dot":
            model = Dot(in_size, cfg.hidden_dim, cfg.num_layers).to(device)
        elif cfg.model_name == "GAT":
            g = dgl.remove_self_loop(g)
            g = dgl.add_self_loop(g)
            if cfg.dataset == "ogbl-citation2":
                num_heads = 8
            else:
                num_heads = 8
            num_out_heads = 1
            heads = ([num_heads] * (cfg.num_layers - 1)) + [num_out_heads]
            activation = F.elu
            feat_drop = 0
            attn_drop = 0
            negative_slope = 0.2
            residual = True
            model = GATv2(in_size, cfg.hidden_dim, cfg.num_layers, heads, activation, feat_drop, attn_drop,
                        negative_slope, residual).to(device)
        elif cfg.model_name == "GIN":
            g = dgl.remove_self_loop(g)
            g = dgl.add_self_loop(g)
            print("Add self loop")
            model = GIN(in_size, cfg.hidden_dim, cfg.num_layers).to(device)
        else:
            raise ValueError(f"Model '{cfg.model_name}' is not supported")
        # model training
        log.info('Training...')
        # log.info(edge_split['test'].keys())
        mrr, h10, h1 = train(cfg, device, g, reverse_eids, seed_edges, model, edge_split, logger, run)
        mrrs.append(mrr)
        h10s.append(h10)
        h1s.append(h1)
    logger.print_statistics()
    mean_mrr = torch.mean(torch.tensor(mrrs)).item()
    std_mrr = torch.std(torch.tensor(mrrs)).item()
    mean_h1 = torch.mean(torch.tensor(h1s)).item()
    std_h1 = torch.std(torch.tensor(h1s)).item()
    mean_h10 = torch.mean(torch.tensor(h10s)).item()
    std_h10 = torch.std(torch.tensor(h10s)).item()
    print(mean_mrr)
    print(std_mrr)
    print(mean_h1)
    print(std_h1)
    print(mean_h10)
    print(std_h10)    
    return mean_mrr


if __name__=='__main__':
    main()
</file>

<file path="lp/models.py">
import torch
import torch.nn.functional as F
import dgl.nn as dglnn
import tqdm
import torch.nn as nn
from dgl.dataloading import DataLoader, NeighborSampler, MultiLayerFullNeighborSampler
from dgl.nn import GATv2Conv
from dgl.nn.pytorch.conv import GINConv
from torch.nn import Linear

class SAGE(nn.Module):
    def __init__(self, in_size, hid_size, num_layers=3):
        super().__init__()
        self.layers = nn.ModuleList()
        self.layers.append(dglnn.SAGEConv(in_size, hid_size, 'mean'))
        # n-layer GraphSAGE-mean
        for i in range(num_layers - 1):
            self.layers.append(dglnn.SAGEConv(hid_size, hid_size, 'mean'))
        self.hid_size = hid_size
        self.predictor = nn.Sequential(
            nn.Linear(hid_size, hid_size),
            nn.ReLU(),
            nn.Linear(hid_size, hid_size),
            nn.ReLU(),
            nn.Linear(hid_size, 1))

    def forward(self, pair_graph, neg_pair_graph, blocks, x):
        h = x
        for l, (layer, block) in enumerate(zip(self.layers, blocks)):
            h = layer(block, h)
            if l != len(self.layers) - 1:
                h = F.relu(h)
        pos_src, pos_dst = pair_graph.edges()
        neg_src, neg_dst = neg_pair_graph.edges()
        h_pos = self.predictor(h[pos_src] * h[pos_dst])
        h_neg = self.predictor(h[neg_src] * h[neg_dst])
        return h_pos, h_neg

    def inference(self, g, device, batch_size):
        """Layer-wise inference algorithm to compute GNN node embeddings."""
        feat = g.ndata['feat'].float()
        sampler = MultiLayerFullNeighborSampler(1, prefetch_node_feats=['feat'])
        dataloader = DataLoader(
            g, torch.arange(g.num_nodes()).to(g.device), sampler, device=device,
            batch_size=batch_size, shuffle=False, drop_last=False,
            num_workers=0)
        buffer_device = torch.device('cpu')
        pin_memory = (buffer_device != device)
        for l, layer in enumerate(self.layers):
            y = torch.empty(g.num_nodes(), self.hid_size, device=buffer_device,
                            pin_memory=pin_memory)
            feat = feat.to(device)
            for input_nodes, output_nodes, blocks in tqdm.tqdm(dataloader, desc='Inference'):
                x = feat[input_nodes]
                h = layer(blocks[0], x)
                if l != len(self.layers) - 1:
                    h = F.relu(h)
                y[output_nodes] = h.to(buffer_device)
            feat = y
        return y


class GCN(nn.Module):
    def __init__(self, in_size, hid_size, num_layers=3):
        super().__init__()
        self.layers = nn.ModuleList()
        self.layers.append(
            dglnn.GraphConv(in_size, hid_size, norm='both', weight=True, bias=True, allow_zero_in_degree=True))
        # n-layer GraphConv
        for i in range(num_layers - 1):
            self.layers.append(
                dglnn.GraphConv(hid_size, hid_size, norm='both', weight=True, bias=True, allow_zero_in_degree=True))
        self.hid_size = hid_size
        self.predictor = nn.Sequential(
            nn.Linear(hid_size, hid_size),
            nn.ReLU(),
            nn.Linear(hid_size, hid_size),
            nn.ReLU(),
            nn.Linear(hid_size, 1))

    def forward(self, pair_graph, neg_pair_graph, blocks, x):
        h = x
        for l, (layer, block) in enumerate(zip(self.layers, blocks)):
            h = layer(block, h)
            if l != len(self.layers) - 1:
                h = F.relu(h)
        pos_src, pos_dst = pair_graph.edges()
        neg_src, neg_dst = neg_pair_graph.edges()
        h_pos = self.predictor(h[pos_src] * h[pos_dst])
        h_neg = self.predictor(h[neg_src] * h[neg_dst])
        return h_pos, h_neg

    def inference(self, g, device, batch_size):
        """Layer-wise inference algorithm to compute GNN node embeddings."""
        feat = g.ndata['feat'].float()
        sampler = MultiLayerFullNeighborSampler(1, prefetch_node_feats=['feat'])
        dataloader = DataLoader(
            g, torch.arange(g.num_nodes()).to(g.device), sampler, device=device,
            batch_size=batch_size, shuffle=False, drop_last=False,
            num_workers=0)
        buffer_device = torch.device('cpu')
        pin_memory = (buffer_device != device)
        for l, layer in enumerate(self.layers):
            y = torch.empty(g.num_nodes(), self.hid_size, device=buffer_device,
                            pin_memory=pin_memory)
            feat = feat.to(device)
            for input_nodes, output_nodes, blocks in tqdm.tqdm(dataloader, desc='Inference'):
                x = feat[input_nodes]
                h = layer(blocks[0], x)
                if l != len(self.layers) - 1:
                    h = F.relu(h)
                y[output_nodes] = h.to(buffer_device)
            feat = y
        return y


class MLP(nn.Module):
    def __init__(self, in_size, hid_size, num_layers=3):
        super().__init__()
        self.hid_size = hid_size
        self.layers = nn.ModuleList()
        self.layers.append(nn.Linear(in_size, hid_size))
        # n-layer GraphSAGE-mean
        for i in range(num_layers - 1):
            self.layers.append(nn.Linear(hid_size, hid_size))
        self.hid_size = hid_size
        self.predictor = nn.Sequential(
            nn.Linear(hid_size, hid_size),
            nn.LayerNorm(hid_size),
            nn.ReLU(),
            nn.Linear(hid_size, hid_size),
            nn.LayerNorm(hid_size),
            nn.ReLU(),
            nn.Linear(hid_size, 1))

    def forward(self, pair_graph, neg_pair_graph, blocks, x):
        h = x
        for l, (layer, block) in enumerate(zip(self.layers, blocks)):
            h = layer(h)
            if l != len(self.layers) - 1:
                h = F.relu(h)
        pos_src, pos_dst = pair_graph.edges()
        neg_src, neg_dst = neg_pair_graph.edges()
        h_pos = self.predictor(h[pos_src] * h[pos_dst])
        h_neg = self.predictor(h[neg_src] * h[neg_dst])
        return h_pos, h_neg

        h_pos = self.predictor(torch.cat((h[pos_src], h[pos_dst]), dim=1))
        h_neg = self.predictor(torch.cat((h[neg_src], h[neg_dst]), dim=1))
        return h_pos, h_neg

    def inference(self, g, device, batch_size):
        """Layer-wise inference algorithm to compute GNN node embeddings."""
        feat = g.ndata['feat'].float().to(device)
        sampler = MultiLayerFullNeighborSampler(1, prefetch_node_feats=['feat'])
        dataloader = DataLoader(
            g, torch.arange(g.num_nodes()).to(g.device), sampler, device=device,
            batch_size=batch_size, shuffle=False, drop_last=False,
            num_workers=0)
        buffer_device = torch.device('cpu')
        pin_memory = (buffer_device != device)
        for l, layer in enumerate(self.layers):
            y = torch.empty(g.num_nodes(), self.hid_size, device=buffer_device,
                            pin_memory=pin_memory)
            feat = feat.to(device)
            for input_nodes, output_nodes, blocks in tqdm.tqdm(dataloader, desc='Inference'):
                x = feat[output_nodes]
                h = layer(x)
                if l != len(self.layers) - 1:
                    h = F.relu(h)
                y[output_nodes] = h.to(buffer_device)
            feat = y
        return y


class Dot(nn.Module):
    ### no encoder because the finetuned feature should be robust enough
    ### directly decoder using an elementwise product + 3 layer MLP
    def __init__(self, in_size, hid_size, num_layers=3):
        super().__init__()
        # self.encoder = nn.Sequential(
        #     nn.Linear(in_size, hid_size),
        #     nn.LayerNorm(hid_size),
        #     nn.ReLU(),
        #     nn.Linear(hid_size, hid_size))

        # self.decoder = nn.Linear(hid_size, 1)
        self.decoder = nn.Sequential(
            nn.Linear(in_size, hid_size),
            nn.LayerNorm(hid_size),
            nn.ReLU(),
            nn.Linear(in_size, hid_size),
            nn.LayerNorm(hid_size),
            nn.ReLU(),
            nn.Linear(in_size, hid_size),
            nn.LayerNorm(hid_size),
            nn.ReLU(),
            nn.Linear(in_size, hid_size),
            nn.LayerNorm(hid_size),
            nn.ReLU(),
            nn.Linear(in_size, hid_size),
            nn.LayerNorm(hid_size),
            nn.ReLU(),
            nn.Linear(in_size, hid_size),
            nn.LayerNorm(hid_size),
            nn.ReLU(),
            nn.Linear(in_size, hid_size),
            nn.LayerNorm(hid_size),
            nn.ReLU(),
            nn.Linear(in_size, hid_size),
            nn.LayerNorm(hid_size),
            nn.ReLU(),
            nn.Linear(hid_size, 1),
        )

    def forward(self, pair_graph, neg_pair_graph, blocks, x):
        # h = self.encoder(x)
        h = x
        pos_src, pos_dst = pair_graph.edges()
        neg_src, neg_dst = neg_pair_graph.edges()
        # h_pos = self.predictor(h[pos_src] * h[pos_dst])
        h_pos = self.decoder(h[pos_src] * h[pos_dst])
        h_neg = self.decoder(h[neg_src] * h[neg_dst])
        return h_pos, h_neg

    def inference(self, g, device, batch_size):
        """Layer-wise inference algorithm to compute GNN node embeddings."""
        feat = g.ndata['feat'].float().to(device)
        buffer_device = torch.device('cpu')
        # h = self.encoder(feat).to(buffer_device)
        h = feat
        return h



class GATv2(nn.Module):
    def __init__(self, in_size, hid_size, num_layers, heads, activation, feat_drop, attn_drop,
                 negative_slope, residual):
        super(GATv2, self).__init__()
        self.num_layers = num_layers
        self.gatv2_layers = nn.ModuleList()
        self.activation = activation
        self.heads = heads
        self.hid_size = hid_size
        self.layer_norms = torch.nn.ModuleList()
        # input projection (no residual)
        self.gatv2_layers.append(
            GATv2Conv(in_size, hid_size, heads[0], feat_drop, attn_drop, negative_slope, False, self.activation,
                      bias=False, share_weights=True)
        )
        # hidden layers
        for l in range(num_layers - 1):
            # due to multi-head, the in_dim = num_hidden * num_heads
            self.layer_norms.append(nn.LayerNorm(hid_size * heads[l]))
            self.gatv2_layers.append(
                GATv2Conv(hid_size * heads[l], hid_size, heads[l + 1], feat_drop, attn_drop, negative_slope, residual,
                          self.activation, bias=False, share_weights=True)
            )
        # output projection
        self.predictor = nn.Sequential(
            nn.Linear(hid_size * heads[-1], hid_size),
            nn.ReLU(),
            nn.Linear(hid_size, hid_size),
            nn.ReLU(),
            nn.Linear(hid_size, 1))

    def forward(self, pair_graph, neg_pair_graph, blocks, x):
        h = x
        for l, (layer, block) in enumerate(zip(self.gatv2_layers, blocks)):
            h = layer(block, h).flatten(1)
        pos_src, pos_dst = pair_graph.edges()
        neg_src, neg_dst = neg_pair_graph.edges()
        h_pos = self.predictor(h[pos_src] * h[pos_dst])
        h_neg = self.predictor(h[neg_src] * h[neg_dst])
        return h_pos, h_neg

    def inference(self, g, device, batch_size):
        """Layer-wise inference algorithm to compute GNN node embeddings."""
        feat = g.ndata['feat'].float()
        sampler = MultiLayerFullNeighborSampler(1, prefetch_node_feats=['feat'])
        dataloader = DataLoader(
            g, torch.arange(g.num_nodes()).to(g.device), sampler, device=device,
            batch_size=batch_size, shuffle=False, drop_last=False,
            num_workers=0)
        buffer_device = torch.device('cpu')
        pin_memory = (buffer_device != device)
        for l, layer in enumerate(self.gatv2_layers):
            y = torch.empty(g.num_nodes(), self.hid_size * self.heads[l], device=buffer_device,
                            pin_memory=pin_memory)
            feat = feat.to(device)
            for input_nodes, output_nodes, blocks in tqdm.tqdm(dataloader, desc='Inference'):
                x = feat[input_nodes]
                h = layer(blocks[0], x).flatten(1)
                y[output_nodes] = h.to(buffer_device)
            feat = y
        return y


class GINMLP(nn.Module):
    """Construct two-layer MLP-type aggreator for GIN model"""

    def __init__(self, input_dim, hidden_dim, output_dim):
        super().__init__()
        self.linears = nn.ModuleList()
        # two-layer MLP
        self.linears.append(nn.Linear(input_dim, hidden_dim, bias=False))
        self.linears.append(nn.Linear(hidden_dim, output_dim, bias=False))
        self.batch_norm = nn.BatchNorm1d((hidden_dim))

    def forward(self, x):
        h = x
        h = F.relu(self.batch_norm(self.linears[0](h)))
        return self.linears[1](h)


class GIN(nn.Module):
    def __init__(self, in_size, hid_size, num_layers=3):
        super().__init__()
        self.ginlayers = nn.ModuleList()
        self.hid_size = hid_size
        self.batch_norms = nn.ModuleList()
        # five-layer GCN with two-layer MLP aggregator and sum-neighbor-pooling scheme
        for layer in range(num_layers - 1):  # excluding the input layer
            if layer == 0:
                mlp = GINMLP(in_size, hid_size, hid_size)
            else:
                mlp = GINMLP(hid_size, hid_size, hid_size)
            self.ginlayers.append(
                GINConv(mlp, learn_eps=False)
            )  # set to True if learning epsilon
            self.batch_norms.append(nn.BatchNorm1d(hid_size))

        self.predictor = nn.Sequential(
            nn.Linear(hid_size, hid_size),
            nn.ReLU(),
            nn.Linear(hid_size, hid_size),
            nn.ReLU(),
            nn.Linear(hid_size, 1))

    def forward(self, pair_graph, neg_pair_graph, blocks, x):
        # list of hidden representation at each layer (including the input layer)
        h = x
        for l, (layer, block) in enumerate(zip(self.ginlayers, blocks)):
            h = layer(block, h)
            h = self.batch_norms[l](h)
            if l != len(self.ginlayers) - 1:
                h = F.relu(h)

        pos_src, pos_dst = pair_graph.edges()
        neg_src, neg_dst = neg_pair_graph.edges()
        h_pos = self.predictor(h[pos_src] * h[pos_dst])
        h_neg = self.predictor(h[neg_src] * h[neg_dst])
        return h_pos, h_neg

    def inference(self, g, device, batch_size):
        """Layer-wise inference algorithm to compute GNN node embeddings."""
        feat = g.ndata['feat'].float()
        sampler = MultiLayerFullNeighborSampler(1, prefetch_node_feats=['feat'])
        dataloader = DataLoader(
            g, torch.arange(g.num_nodes()).to(g.device), sampler, device=device,
            batch_size=batch_size, shuffle=False, drop_last=False,
            num_workers=0)
        buffer_device = torch.device('cpu')
        pin_memory = (buffer_device != device)
        for l, layer in enumerate(self.ginlayers):
            y = torch.empty(g.num_nodes(), self.hid_size, device=buffer_device,
                            pin_memory=pin_memory)
            feat = feat.to(device)
            for input_nodes, output_nodes, blocks in tqdm.tqdm(dataloader, desc='Inference'):
                x = feat[input_nodes]
                h = layer(blocks[0], x)
                h = self.batch_norms[l](h)
                if l != len(self.ginlayers) - 1:
                    h = F.relu(h)
                y[output_nodes] = h.to(buffer_device)
            feat = y
        return y
</file>

<file path="lp/utils.py">
import torch
import numpy as np
import scipy.sparse as ssp
import torch.nn.functional as F
import dgl
import os
from tqdm import tqdm
import logging


log = logging.getLogger(__name__)


def to_bidirected_with_reverse_mapping(g):
    """Makes a graph bidirectional, and returns a mapping array ``mapping`` where ``mapping[i]``
    is the reverse edge of edge ID ``i``. Does not work with graphs that have self-loops.
    """
    g_simple, mapping = dgl.to_simple(
        dgl.add_reverse_edges(g), return_counts='count', writeback_mapping=True)
    c = g_simple.edata['count']
    num_edges = g.num_edges()
    mapping_offset = torch.zeros(g_simple.num_edges() + 1, dtype=g_simple.idtype)
    mapping_offset[1:] = c.cumsum(0)
    idx = mapping.argsort()
    idx_uniq = idx[mapping_offset[:-1]]
    reverse_idx = torch.where(idx_uniq >= num_edges, idx_uniq - num_edges, idx_uniq + num_edges)
    reverse_mapping = mapping[reverse_idx]
    # sanity check
    src1, dst1 = g_simple.edges()
    src2, dst2 = g_simple.find_edges(reverse_mapping)
    assert torch.equal(src1, dst2)
    assert torch.equal(src2, dst1)
    return g_simple, reverse_mapping


def load_usair_dataset(device, dataset_root):
    dataset, label_dict = dgl.load_graphs(os.path.join(dataset_root, "USAir.dgl"))
    g = dataset[0]
    g = g.to('cpu')
    g, reverse_eids = to_bidirected_with_reverse_mapping(g)
    reverse_eids = reverse_eids.to(device)
    seed_edges = torch.arange(g.num_edges()).to(device)
    load_split = np.load(os.path.join(dataset_root, "USAir.npz"), allow_pickle=True)['current'].item()
    test_neg_i, test_neg_j, _ = ssp.find(ssp.triu(load_split['orginal_A'] == 0, k=1))
    test_neg_i = torch.Tensor(test_neg_i).unsqueeze(1)
    test_neg_j = torch.Tensor(test_neg_j).unsqueeze(1)
    negatives = torch.cat((test_neg_i, test_neg_j), dim=1)
    edge_split = {}
    train_idx = torch.cat((torch.Tensor(load_split['train_pos'][0]).unsqueeze(1),
                           torch.Tensor(load_split['train_pos'][1]).unsqueeze(1)), dim=1)
    valid = torch.cat((torch.Tensor(load_split['valid_pos'][0]).unsqueeze(1),
                       torch.Tensor(load_split['valid_pos'][1]).unsqueeze(1)), dim=1)
    test = torch.cat((torch.Tensor(load_split['test_pos'][0]).unsqueeze(1),
                      torch.Tensor(load_split['test_pos'][1]).unsqueeze(1)), dim=1)
    edge_split['train'] = {}
    edge_split['train']['edge'] = train_idx.long()
    edge_split['valid'] = {}
    edge_split['valid']['edge'] = valid.long()
    edge_split['valid']['edge_neg'] = negatives.long()
    edge_split['test'] = {}
    edge_split['test']['edge'] = test.long()
    edge_split['test']['edge_neg'] = negatives.long()
    g.ndata['feat'] = F.one_hot(torch.arange(0, g.number_of_nodes()))
    return g, reverse_eids, seed_edges, edge_split


def load_esci_dataset(device, dataset_root):
    dataset, label_dict = dgl.load_graphs(os.path.join(dataset_root, "esci_train.dgl"))
    g = dataset[0]
    g = g.to('cpu')
    # since negative sampler regards reverse edges as non-exists and might sample them
    # it's essential to add reverse edges to the graph first
    g, reverse_eids = to_bidirected_with_reverse_mapping(g)
    reverse_eids = reverse_eids.to(device)
    seed_edges = torch.arange(g.num_edges()).to(device)
    load_split = np.load(os.path.join(dataset_root, "esci.npz"), allow_pickle=True)['current'].item()
    # generate validation and test negatives
    num_negative_edges = 1100
    # negative_sampler = dgl.dataloading.negative_sampler.Uniform(num_negative_edges)
    orig_g = dgl.load_graphs(os.path.join(dataset_root, "esci.dgl"))[0][0]
    # get valid, test edge id
    asins_to_sample = torch.arange(33804, orig_g.number_of_nodes())
    num_asins = g.number_of_nodes() - 33804
    sampled_negatives = []
    for i in tqdm(range(load_split['valid_pos'].shape[0])):
        u = load_split['valid_pos'][i][0]
        v = load_split['valid_pos'][i][1]
        num_negatives = torch.randperm(num_asins)[:num_negative_edges]
        sampled_negative = asins_to_sample[num_negatives]
        repeat_u = torch.full((num_negative_edges, 1), u)[:, 0]
        check_valid = orig_g.has_edges_between(repeat_u, sampled_negative)
        choose_valid = check_valid == False
        choose_negative = sampled_negative[choose_valid]
        if torch.sum(choose_valid) > 1000:
            sampled_negatives.append(choose_negative[:1000])
        else:
            log.info("insufficient negatives sampled")
            import pdb
            pdb.set_trace()
    neg_valid_edges = torch.stack((sampled_negatives))
    sampled_negatives = []
    for i in tqdm(range(load_split['test_pos'].shape[0])):
        u = load_split['test_pos'][i][0]
        v = load_split['test_pos'][i][1]
        num_negatives = torch.randperm(num_asins)[:num_negative_edges]
        sampled_negative = asins_to_sample[num_negatives]
        repeat_u = torch.full((num_negative_edges, 1), u)[:, 0]
        check_test = orig_g.has_edges_between(repeat_u, sampled_negative)
        choose_test = check_test == False
        choose_negative = sampled_negative[choose_test]
        if torch.sum(choose_test) > 1000:
            sampled_negatives.append(choose_negative[:1000])
        else:
            log.info("insufficient negatives sampled")
            import pdb
            pdb.set_trace()
    neg_test_edges = torch.stack((sampled_negatives))
    # valid_eids = orig_g.edge_ids(load_split['valid_pos'][:, 0], load_split['valid_pos'][:, 1])
    # test_eids = orig_g.edge_ids(load_split['test_pos'][:, 0], load_split['test_pos'][:, 1])
    # negative_valid_edges = negative_sampler(g, valid_eids)
    # negative_test_edges = negative_sampler(g, test_eids)
    # import pdb 
    # pdb.set_trace()
    # double check that negative edges does not exist in the graph
    # check_valid = orig_g.has_edges_between(negative_valid_edges[0], negative_valid_edges[1])
    # check_test = orig_g.has_edges_between(negative_test_edges[0], negative_test_edges[1])
    # choose_valid = check_valid == False
    # choose_test = check_test == False
    # log.info(torch.sum(check_valid))
    # log.info(torch.sum(check_test))
    # negative_valid_edges_u = negative_valid_edges[0][choose_valid].unsqueeze(1)
    # negative_valid_edges_v = negative_valid_edges[1][choose_valid].unsqueeze(1)
    # negative_test_edges_u = negative_test_edges[0][choose_test].unsqueeze(1)
    # negative_test_edges_v = negative_test_edges[1][choose_test].unsqueeze(1)
    # neg_valid_edges = torch.cat((negative_valid_edges_u, negative_valid_edges_v), dim=1)
    # neg_test_edges = torch.cat((negative_test_edges_u, negative_test_edges_v), dim=1)
    # check_valid = g.has_edges_between(neg_valid_edges[:, 0], neg_valid_edges[:, 1])
    # check_test = g.has_edges_between(neg_test_edges[:, 0], neg_test_edges[:, 1])
    # log.info(torch.sum(check_valid))
    # log.info(torch.sum(check_test))
    edge_split = {}
    edge_split['train'] = {}
    edge_split['train']['source_node'] = load_split['train_pos'][:, 0].long()
    edge_split['train']['target_node'] = load_split['train_pos'][:, 1].long()
    edge_split['valid'] = {}
    edge_split['valid']['source_node'] = load_split['valid_pos'][:, 0].long()
    edge_split['valid']['target_node'] = load_split['valid_pos'][:, 1].long()
    edge_split['valid']['target_node_neg'] = neg_valid_edges.long()
    edge_split['test'] = {}
    edge_split['test']['source_node'] = load_split['test_pos'][:, 0].long()
    edge_split['test']['target_node'] = load_split['test_pos'][:, 1].long()
    edge_split['test']['target_node_neg'] = neg_test_edges.long()
    g.ndata['feat'] = g.ndata['feats']
    return g, reverse_eids, seed_edges, edge_split


def remove_collab_dissimilar_edges(dataset):
    """
    Remove the edges in edge_split()
    :param dataset:
    :return: processed collab dataset
    """
    edge_split = dataset.get_edge_split()
    dataset, label_dict = dgl.load_graphs('ogbl-collab_low_degree.dgl')
    g = dataset[0]
    new_edge_split = {
        'train': {'edge': [], 'weight': [], 'year': []},
        'valid': {'edge': edge_split['valid']['edge'], 'weight': edge_split['valid']['weight'],
                  'year': edge_split['valid']['year'], 'edge_neg': edge_split['valid']['edge_neg']},
        'test': {'edge': edge_split['test']['edge'], 'weight': edge_split['test']['weight'],
                 'year': edge_split['test']['year'], 'edge_neg': edge_split['test']['edge_neg']}
    }
    for split in ['train']:
        edge_tuple = edge_split[split]['edge']
        weight_list = edge_split[split]['weight']
        year_list = edge_split[split]['year']
        for index, edge in enumerate(edge_tuple):
            if g.has_edges_between(edge[0], edge[1]):
                new_edge_split[split]['edge'].append([edge[0], edge[1]])
                new_edge_split[split]['weight'].append(weight_list[index])
                new_edge_split[split]['year'].append(year_list[index])
        new_edge_split[split]['edge'] = torch.tensor(new_edge_split[split]['edge'], dtype=torch.int64)
        new_edge_split[split]['weight'] = torch.tensor(new_edge_split[split]['weight'], dtype=torch.int64)
        new_edge_split[split]['year'] = torch.tensor(new_edge_split[split]['year'], dtype=torch.int64)

    import pickle
    with open('ogbl-collab_low_degree.pickle', 'wb') as handle:
        pickle.dump(new_edge_split, handle, protocol=pickle.HIGHEST_PROTOCOL)
    return g, new_edge_split


def construct_low_degree_collab(dataset):
    """
    Construct the low-degree collab, for each node, remove the edge connected with most dissimilar nodes
    """
    g = dataset[0]
    node_feature = g.ndata['feat']
    cosi = torch.nn.CosineSimilarity(dim=0)
    num_of_nodes = g.num_nodes()
    index = 0
    for head_node in range(num_of_nodes):
        if index % 100 == 0:
            log.info(index)
        index += 1
        head_degree = g.in_degrees(head_node)
        if head_degree > 1:
            _, dst_node_list = g.out_edges(head_node)
            simi_vector = []
            for dst_node in dst_node_list:
                simi_vector.append(cosi(node_feature[head_node], node_feature[dst_node]))
            simi_vector = torch.tensor(simi_vector)
            dissimi_dst_node = dst_node_list[torch.argmin(simi_vector)]
            tail_degree = g.in_degrees(dissimi_dst_node)
            if tail_degree > 1 and g.has_edges_between(head_node, dissimi_dst_node):
                remove_edge_id = g.edge_ids(head_node, dissimi_dst_node)
                g.remove_edges(remove_edge_id)

    dgl.save_graphs('ogbl-collab_low_degree.dgl', [g])
    log.info("Finish removing edges")


class Logger(object):
    def __init__(self, runs):
        self.results = [[] for _ in range(runs)]

    def add_result(self, run, result):
        assert 0 <= run < len(self.results)
        self.results[run].append(result)

    def print_statistics(self, run=None):
        if run is not None:
            result = 100 * torch.tensor(self.results[run])
            argmax = result[:, 0].argmax().item()
            log.info(f'Run {run + 1:02d}:')
            log.info(f'Highest Valid: {result[:, 0].max():.2f}')
            log.info(f'   Final Test: {result[argmax, 1]:.2f}')
        else:
            result = 100 * torch.tensor(self.results)

            best_results = []
            for r in result:
                valid = r[:, 0].max().item()
                test = r[r[:, 0].argmax(), 1].item()
                best_results.append((valid, test))

            best_result = torch.tensor(best_results)

            log.info(f'All runs:')
            r = best_result[:, 0]
            log.info(f'Highest Valid: {r.mean():.2f} ± {r.std():.2f}')
            r = best_result[:, 1]
            log.info(f'   Final Test: {r.mean():.2f} ± {r.std():.2f}')
</file>

<file path="nc/MGAT/DataLoad.py">
import random
import numpy as np
import torch
from tqdm import tqdm
from torch.utils.data import Dataset, DataLoader

class DataLoad(Dataset):
	def __init__(self, num_nodes, train_nodes, labels):
		super(DataLoad, self).__init__()

		self.num_nodes = num_nodes
		self.train_nodes = torch.LongTensor(train_nodes)
		self.labels = labels

	def __getitem__(self, index):
		src = self.train_nodes[index].item()
		label = self.labels[src].item()
		return torch.LongTensor([src]), torch.LongTensor([label])


	def __len__(self):
		return len(self.train_nodes)




</file>

<file path="nc/MGAT/GraphGAT.py">
import torch
import torch.nn.functional as F
from torch.nn import Parameter
from torch_geometric.nn.conv import MessagePassing
from torch_geometric.utils import remove_self_loops, add_self_loops, softmax, degree
from torch_geometric.nn.inits import uniform, glorot, zeros
class GraphGAT(MessagePassing):
	def __init__(self, in_channels, out_channels, normalize=True, bias=True, aggr='add', **kwargs):
		super(GraphGAT, self).__init__(aggr=aggr, **kwargs)
		self.in_channels = in_channels
		self.out_channels = out_channels
		self.normalize = normalize
		self.dropout = 0.2

		self.weight = Parameter(torch.Tensor(self.in_channels, out_channels))
		if bias:
			self.bias = Parameter(torch.Tensor(out_channels))
		else:
			self.register_parameter('bias', None)

		self.reset_parameters()
		self.is_get_attention = False

	def reset_parameters(self):
		uniform(self.in_channels, self.weight)
		uniform(self.in_channels, self.bias)


	def forward(self, x, edge_index, size=None):
		if size is None:
			edge_index, _ = remove_self_loops(edge_index)
		x = x.unsqueeze(-1) if x.dim() == 1 else x
		x = torch.matmul(x, self.weight)

		return self.propagate(edge_index, size=size, x=x)

	def message(self, edge_index_i, x_i, x_j, size_i, edge_index, size):
		#print(edge_index_i, x_i, x_j, size_i, edge_index, size)
      
		# Compute attention coefficients.
		x_i = x_i.view(-1, self.out_channels)
		x_j = x_j.view(-1, self.out_channels)
		inner_product = torch.mul(x_i, F.leaky_relu(x_j)).sum(dim=-1)

		# gate
		row, col = edge_index
		deg = degree(row, size[0], dtype=x_i.dtype)
		deg_inv_sqrt = deg[row].pow(-0.5)
		tmp = torch.mul(deg_inv_sqrt, inner_product)
		gate_w = torch.sigmoid(tmp)
		# gate_w = F.dropout(gate_w, p=self.dropout, training=self.training)

		# attention
		tmp = torch.mul(inner_product, gate_w)
		attention_w = softmax(tmp, edge_index_i, num_nodes=size_i)
		#attention_w = F.dropout(attention_w, p=self.dropout, training=self.training)
		return torch.mul(x_j, attention_w.view(-1, 1))

	def update(self, aggr_out):
		if self.bias is not None:
			aggr_out = aggr_out + self.bias
		if self.normalize:
			aggr_out = F.normalize(aggr_out, p=2, dim=-1)
		return aggr_out

	def __repr(self):
		return '{}({},{})'.format(self.__class__.__name__, self.in_channels, self.out_channels)

</file>

<file path="nc/MGAT/Model.py">
import math
# from tqdm import tqdm
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from GraphGAT import GraphGAT

class MGAT(torch.nn.Module):
    def __init__(self, features, edge_index, batch_size, num_nodes, num_layers: int, dim_x=64):
        super(MGAT, self).__init__()
        self.batch_size = batch_size
        self.num_nodes = num_nodes

        self.edge_index = torch.tensor(edge_index).t().contiguous().cuda()
        self.edge_index = torch.cat((self.edge_index, self.edge_index[[1,0]]), dim=1)
        
        v_feat, a_feat, t_feat = features
        self.v_feat = torch.tensor(v_feat).cuda()
        self.a_feat = None
        self.t_feat = torch.tensor(t_feat).cuda()

        self.v_gnn = GNN(self.v_feat, self.edge_index, batch_size, num_nodes, dim_x, num_layers, dim_latent=256)
        self.a_gnn = None
        self.t_gnn = GNN(self.t_feat, self.edge_index, batch_size, num_nodes, dim_x, num_layers, dim_latent=100)

        self.id_embedding = nn.Embedding(num_nodes, dim_x)
        nn.init.xavier_normal_(self.id_embedding.weight)

        #self.id_embedding = nn.init.xavier_normal_(torch.rand((num_user+num_item, dim_x), requires_grad=True)).cuda()
        self.result_embed = nn.init.xavier_normal_(torch.rand((num_nodes, dim_x))).cuda()

        self.transform = nn.Sequential(
            nn.Linear(dim_x*num_layers, dim_x),
            nn.ReLU(),
            nn.Linear(dim_x, dim_x),
            nn.ReLU(),
            nn.Linear(dim_x, 12))

    def forward(self):
        v_rep = self.v_gnn(self.id_embedding)
        a_rep = None
        t_rep = self.t_gnn(self.id_embedding)
        if a_rep is None:
            representation = (v_rep + t_rep) / 2
        else:
            representation = (v_rep + a_rep + t_rep) / 3 #torch.max_pool2d((v_rep, a_rep, t_rep))#max()#torch.cat((v_rep, a_rep, t_rep), dim=1)
        representation = self.transform(representation)
        return representation


    def loss(self, data):
        node, label = data
        label = label.cuda()
        node = node.view(-1)
        out = self.forward()
        y_hat = out[node]
        loss = F.cross_entropy(y_hat, label[:,0])
        return loss


class GNN(torch.nn.Module):
    def __init__(self, features, edge_index, batch_size, num_nodes, dim_id, num_layers: int, dim_latent=None):
        super(GNN, self).__init__()
        self.batch_size = batch_size
        self.num_nodes = num_nodes
        self.dim_id = dim_id
        self.dim_feat = features.size(1)
        self.dim_latent = dim_latent
        self.edge_index = edge_index
        self.features = features
        self.num_layers = num_layers

        if self.dim_latent:
            #self.preference = nn.init.xavier_normal_(torch.rand((num_user, self.dim_latent), requires_grad=True)).cuda()
            self.MLP = nn.Linear(self.dim_feat, self.dim_latent)

            self.conv_embed_1 = GraphGAT(self.dim_latent, self.dim_latent, aggr='add')
            nn.init.xavier_normal_(self.conv_embed_1.weight)
            self.linear_layer1 = nn.Linear(self.dim_latent, self.dim_id)
            nn.init.xavier_normal_(self.linear_layer1.weight)
            self.g_layer1 = nn.Linear(self.dim_latent, self.dim_id)    
            nn.init.xavier_normal_(self.g_layer1.weight) 
        else:
            #self.preference = nn.init.xavier_normal_(torch.rand((num_user, self.dim_feat), requires_grad=True)).cuda()
            self.conv_embed_1 = GraphGAT(self.dim_feat, self.dim_feat, aggr='add')
            nn.init.xavier_normal_(self.conv_embed_1.weight)
            self.linear_layer1 = nn.Linear(self.dim_feat, self.dim_id)
            nn.init.xavier_normal_(self.linear_layer1.weight)
            self.g_layer1 = nn.Linear(self.dim_feat, self.dim_id)    
            nn.init.xavier_normal_(self.g_layer1.weight)

        if num_layers >= 2:
            self.conv_embed_2 = GraphGAT(self.dim_id, self.dim_id, aggr='add')
            nn.init.xavier_normal_(self.conv_embed_2.weight)
            self.linear_layer2 = nn.Linear(self.dim_id, self.dim_id)
            nn.init.xavier_normal_(self.linear_layer2.weight)
            self.g_layer2 = nn.Linear(self.dim_id, self.dim_id)    
            nn.init.xavier_normal_(self.g_layer2.weight)
        if num_layers >= 3:
            self.conv_embed_3 = GraphGAT(self.dim_id, self.dim_id, aggr='add')
            nn.init.xavier_normal_(self.conv_embed_3.weight)
            self.linear_layer3 = nn.Linear(self.dim_id, self.dim_id)
            nn.init.xavier_normal_(self.linear_layer3.weight)
            self.g_layer3 = nn.Linear(self.dim_id, self.dim_id)    
            nn.init.xavier_normal_(self.g_layer3.weight)
            

    def forward(self, id_embedding):
        temp_features = torch.tanh(self.MLP(self.features)) if self.dim_latent else self.features
        x = temp_features
        x = F.normalize(x).cuda()

        #layer-1
        h = F.leaky_relu(self.conv_embed_1(x, self.edge_index, None))
        x_hat = F.leaky_relu(self.linear_layer1(x)) + id_embedding.weight
        x_1 = F.leaky_relu(self.g_layer1(h)+x_hat)
        
        if self.num_layers == 1:
            x = x_1
            return x_1
        
        # layer-2
        h = F.leaky_relu(self.conv_embed_2(x_1, self.edge_index, None))
        x_hat = F.leaky_relu(self.linear_layer2(x_1)) + id_embedding.weight
        x_2 = F.leaky_relu(self.g_layer2(h)+x_hat)

        if self.num_layers == 2:
            x = torch.cat((x_1, x_2), dim=1)
            return x
            
        h = F.leaky_relu(self.conv_embed_2(x_2, self.edge_index, None))
        x_hat = F.leaky_relu(self.linear_layer2(x_2)) + id_embedding.weight
        x_3 = F.leaky_relu(self.g_layer3(h)+x_hat)
        
        if self.num_layers == 3:
            x = torch.cat((x_1, x_2, x_3), dim=1)
            return x
        
</file>

<file path="nc/MGAT/README.md">
# MGAT
This is our Pytorch implementation for our paper- Multimodal Graph Attention Network(MGAT):

>	Zhulin Tao, Yinwei Wei, Xiang Wang, Xiangnan He, Xianglin Huang, Tat-Seng Chua:
MGAT: Multimodal Graph Attention Network for Recommendation. Inf. Process. Manag. 57(5): 102277 (2020)

## Introduction
In this work, we propose a new Multimodal Graph Attention Network, short for MGAT, which disentangles personal interests at the granularity of modality. In particular, built upon multimodal interaction graphs, MGAT conducts information propagation within individual graphs, while leveraging the gated attention mechanism to identify varying importance scores of different modalities to user preference.
## Environment Requirement
The code has been tested running under Python 3.6.5. The required packages are as follows:
* torch==1.7.0
* numpy==1.16.1
* torch_geometric==1.6.1

## run
```python
CUDA_VISIBLE_DEVICES=0 python  -u train.py --num_epoch 200 --batch_size 2048 --weight_decay 0.1 --l_r 3e-5
```
# Citation
@article{DBLP:journals/ipm/TaoWWHHC20,
  author    = {Zhulin Tao and
               Yinwei Wei and
               Xiang Wang and
               Xiangnan He and
               Xianglin Huang and
               Tat{-}Seng Chua},
  title     = {{MGAT:} Multimodal Graph Attention Network for Recommendation},
  journal   = {Inf. Process. Manag.},
  volume    = {57},
  number    = {5},
  pages     = {102277},
  year      = {2020}
}

</file>

<file path="nc/MGAT/mmgat.sh">



python run.py \
    --l_r 0.0004549756183905287 \
    --weight_decay 1e-5 \
    --batch_size 2048 \
    --num_epoch 30 \
    --num_workers 4 \
    --num_layers 2 \
    --feat_path /nfs/turbo/coe-dkoutra/jing/Next-GraphGPT/amazon_nc/fashion/t5vit_feat.pt \
    --project_name mgat_nc \
    --wandb_run_name hard_clip\
    --PATH_best_weight_save './mgat_amazon_nc_ckpt.pt' \
    --PATH_best_metrics_save './mgat_amazon_nc_best_metrics.txt' \
    --PATH_best_hyperparameter_save './mgat_amazon_nc_best_hyperparameter.txt' \
    # --do_hyperparameter_search



</file>

<file path="nc/MGAT/run.py">
import os
import sys
import argparse
import numpy as np
from tqdm import tqdm
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from DataLoad import DataLoad
from Model import MGAT
import wandb
import pickle
import optuna


def batch_evaluate_model(representation: torch.Tensor, triple_data, batch_size=256, verbose=False):
    dataset = TensorDataset(
        torch.tensor(triple_data['source_node']),
        torch.tensor(triple_data['target_node']),
        torch.tensor(triple_data['target_node_neg'], dtype=torch.long))

    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)

    rank_ls = []
    for src, pos_tgt, neg_tgt_ls in tqdm(dataloader, disable=not verbose):
        src_ls = src.repeat_interleave(neg_tgt_ls.size(1) + 1, dim=0)
        tgt_ls = torch.cat((pos_tgt.unsqueeze(1), neg_tgt_ls), dim=1).view(-1)

        src_score = representation[src_ls]
        tgt_score = representation[tgt_ls]

        pred = torch.sum(src_score * tgt_score, dim=1).detach().cpu()
        rank = get_rank_2d(pred.view(src.size(0), -1))
        rank_ls.extend(rank.numpy().tolist())

    hit_10 = calculate_hit(rank_ls, 10)
    hit_1 = calculate_hit(rank_ls, 1)
    mrr = calculate_mrr(rank_ls)
    return mrr, hit_1, hit_10


def train(dataloader, model, optimizer, max_step=None):
    model.train()
    sum_loss = 0.0
    for idx, data in enumerate(tqdm(dataloader)):
        optimizer.zero_grad()
        loss = model.loss(data)
        loss.backward()
        optimizer.step()
        sum_loss += loss.cpu().item()

        if max_step is not None and idx > max_step:
            break
    return sum_loss / idx



def hyperparameter_search(train_dataloader, args, features, edge_index, num_nodes, valid_nodes, test_nodes, labels):
    def objective(trial):
        lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)
        num_layers = trial.suggest_int('num_layers', 1, 3)

        model = MGAT(features, edge_index, args.batch_size, num_nodes, num_layers,
                     args.dim_latent).cuda()
        optimizer = torch.optim.Adam([{'params': model.parameters(), 'lr': lr}],
                                     weight_decay=args.weight_decay)
        val_max_mrr = 0

        for epoch in range(args.num_epoch):
            loss = train(train_dataloader, model, optimizer, max_step=None)
            torch.cuda.empty_cache()

            print({"loss": loss})
            if (epoch + 1) % 5 == 0:
                with torch.no_grad():
                    v_rep = model.v_gnn(model.id_embedding)
                    t_rep = model.t_gnn(model.id_embedding)
                    representation = (v_rep + t_rep) / 2
                    representation = model.forward()
                    val_rep = representation[valid_nodes].cpu()
                    test_rep = representation[test_nodes].cpu()
                    val_label = labels[valid_nodes]
                    val_pred = torch.argmax(val_rep, dim=1)
                    val_acc = (val_pred == val_label).sum()/val_label.shape[0]
                    test_label = labels[test_nodes]
                    test_pred = torch.argmax(test_rep, dim=1)
                    test_acc = (test_pred == test_label).sum()/test_label.shape[0]

                    if val_acc > val_max_mrr:
                        val_max_mrr = val_acc
                    print(f"{lr}, {num_layers}")
                    print(f"Epoch: {epoch}, Val Acc: {val_max_mrr}, Test Acc: {test_acc}")

        return val_max_mrr

    study = optuna.create_study(direction='maximize')
    study.optimize(objective, n_trials=args.n_trials)

    print(study.best_params)

    if args.PATH_best_hyperparameter_save is not None:
        with open(args.PATH_best_hyperparameter_save, 'w') as f:
            f.write(study.best_params)
            f.write('\n')
            f.write(study.best_value)


def main():
    parser = argparse.ArgumentParser()

    # using default values is recommended
    parser.add_argument('--no-cuda', action='store_true', default=False, help='Disables CUDA training.')
    parser.add_argument('--model_name', default='MGAT', help='Model name.')
    parser.add_argument('--PATH_weight_load', default=None, help='Loading weight filename.')
    parser.add_argument('--dim_latent', type=int, default=32, help='Latent dimension.')
    parser.add_argument('--num_workers', type=int, default=4, help='Workers number')

    # your data here
    parser.add_argument('--v_feat_path', default=None)
    parser.add_argument('--t_feat_path', default=None)
    parser.add_argument('--feat_path', default=None)

    # your setting here
    parser.add_argument('--num_layers', default=2, type=int)
    parser.add_argument('--num_epoch', type=int, default=200, help='Epoch number')
    parser.add_argument('--l_r', type=float, default=1e-4, help='Learning rate.')
    parser.add_argument('--weight_decay', type=float, default=0, help='Weight decay.')
    parser.add_argument('--batch_size', type=int, default=2048, help='Batch size.')
    parser.add_argument('--repeat_times', type=int, default=3, help='Repeat times.')

    parser.add_argument('--PATH_best_weight_save', default=None, help='Writing weight filename.')
    parser.add_argument('--PATH_best_metrics_save', default=None)

    # if want to do hyperparameter search
    parser.add_argument('--do_hyperparameter_search', action="store_true")
    parser.add_argument('--PATH_best_hyperparameter_save', default=None)
    parser.add_argument('--n_trials', type=int, default=20)
    parser.add_argument('--hyperparameter_search_max_step', type=int, default=10000)

    parser.add_argument('--project_name', default='MGAT')
    parser.add_argument('--wandb_run_name', default='untitled_run')
    parser.add_argument('--wandb_key', default=None)
    parser.add_argument('--report_to', default=None)

    args = parser.parse_args()

    print('Data loading ...')
    BOOKS_PATH = "/nfs/turbo/coe-dkoutra/jing/Next-GraphGPT/amazon_nc/fashion"
    edges = torch.load(os.path.join(BOOKS_PATH, "nc_edges-nodeid.pt"))
    labels = torch.LongTensor(torch.load(os.path.join(BOOKS_PATH, "labels-w-missing.pt")))
    splits = torch.load(os.path.join(BOOKS_PATH, "split.pt"))
    train_edge = torch.LongTensor(edges)
    num_classes = 12
    num_nodes = labels.shape[0]
    if args.feat_path is None:
        v_feat = torch.load(args.v_feat_path).to('cuda')
        t_feat = torch.load(args.t_feat_path).to('cuda')
    else:
        feat = torch.load(args.feat_path).to('cuda')
        v_feat = feat[:, int(feat.shape[1] / 2):]
        t_feat = feat[:, :int(feat.shape[1] / 2)]
    a_feat = None
    features = [v_feat, a_feat, t_feat]
    print(f"number of nodes: {num_nodes}")
    edge_index = train_edge
    train_dataset = DataLoad(num_nodes, splits['train_idx'], labels)
    train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True,
                                  num_workers=args.num_workers)

    valid_nodes = splits['val_idx']                
    test_nodes = splits['test_idx']
    print('Data has been loaded.')

    if args.do_hyperparameter_search:
        hyperparameter_search(train_dataloader, args, features, edge_index, num_nodes, valid_nodes, test_nodes, labels)
        return

    num_epoch = args.num_epoch
    learning_rate = args.l_r
    weight_decay = args.weight_decay

    if args.report_to == 'wandb':
        wandb.init(
            project=args.project_name,
            name=args.wandb_run_name,
            config={
                "learning_rate": learning_rate,
                "epochs": num_epoch,
            }
        )

    global_cur_epoch = 0
    mrrs, h1s, h10s = [], [], []

    for _ in range(args.repeat_times):
        model = MGAT(features, edge_index, args.batch_size, num_nodes, args.num_layers,
                     args.dim_latent).cuda()
        optimizer = torch.optim.Adam([{'params': model.parameters(), 'lr': learning_rate}],
                                     weight_decay=weight_decay)

        max_acc = 0.0
        max_hit_1 = 0.0
        max_hit_10 = 0.0
        val_max_acc = 0.0
        num_decreases = 0

        for epoch in range(num_epoch):
            loss = train(train_dataloader, model, optimizer, max_step=None)

            torch.cuda.empty_cache()

            # print({"loss": loss})

            with torch.no_grad():
                v_rep = model.v_gnn(model.id_embedding)
                t_rep = model.t_gnn(model.id_embedding)
                representation = (v_rep + t_rep) / 2

                representation = model.forward()
                valid_nodes = splits['val_idx']
                val_rep = representation[valid_nodes].cpu()
                test_nodes = splits['test_idx']
                test_rep = representation[test_nodes].cpu()
                val_label = labels[valid_nodes]
                val_pred = torch.argmax(val_rep, dim=1)
                val_acc = (val_pred == val_label).sum()/val_label.shape[0]
                test_label = labels[test_nodes]
                test_pred = torch.argmax(test_rep, dim=1)
                test_acc = (test_pred == test_label).sum()/test_label.shape[0]

                if val_acc > val_max_acc:
                    val_max_acc = val_acc
                    max_acc = test_acc

                    num_decreases = 0

                    if args.PATH_best_weight_save is not None:
                        torch.save(model.state_dict(), args.PATH_best_weight_save)

                    if args.PATH_best_metrics_save is not None:
                        with open(args.PATH_best_metrics_save, 'w') as f:
                            f.write(f"MRR: {max_acc}")

            global_cur_epoch += 1

            print({
                'loss': loss,
                "val_acc": val_acc,
                "test_acc": test_acc,
            })
        mrrs.append(max_acc)

    mean_mrr = torch.mean(torch.tensor(mrrs)).item()
    std_mrr = torch.std(torch.tensor(mrrs)).item()
    print(mean_mrr)
    print(std_mrr)

if __name__ == '__main__':
    main()

</file>

<file path="nc/MMGCN/BaseModel.py">
import torch
import torch.nn.functional as F
from torch.nn import Parameter
from torch_geometric.nn.conv import MessagePassing
from torch_geometric.utils import remove_self_loops, add_self_loops, degree
from torch_geometric.nn.inits import uniform

class BaseModel(MessagePassing):
	def __init__(self, in_channels, out_channels, normalize=True, bias=True, aggr='add', **kwargs):
		super(BaseModel, self).__init__(aggr=aggr, **kwargs)
		self.aggr = aggr
		self.in_channels = in_channels
		self.out_channels = out_channels
		self.normalize = normalize
		self.weight = Parameter(torch.Tensor(self.in_channels, out_channels))

		self.reset_parameters()

	def reset_parameters(self):
		uniform(self.in_channels, self.weight)

	def forward(self, x, edge_index, size=None):
		x = torch.matmul(x, self.weight)
		return self.propagate(edge_index, size=(x.size(0), x.size(0)), x=x)

	def message(self, x_j, edge_index, size):
		return x_j

	def update(self, aggr_out):
		return aggr_out

	def __repr(self):
		return '{}({},{})'.format(self.__class__.__name__, self.in_channels, self.out_channels)

</file>

<file path="nc/MMGCN/Dataset.py">
import time
import pickle
import random
import numpy as np
from tqdm import tqdm
import os
import torch
import torch.nn as nn
from torch.utils.data import Dataset
from torch.utils.data import DataLoader

def data_load(train_edge, feat_path):
    
    v_t_feat = torch.load(feat_path, map_location='cpu')
    t_feat = v_t_feat[:, :int(v_t_feat.shape[1] / 2)]
    v_feat = v_t_feat[:, int(v_t_feat.shape[1] / 2):]
    a_feat = None
    
    num_nodes = v_feat.shape[0]
    
    src_tgt_dict = {i: [] for i in range(num_nodes)}
    for pair in tqdm(train_edge, total=train_edge.shape[0]):
        src, tgt = pair[0].item(), pair[1].item()
        if src in src_tgt_dict.keys():
            src_tgt_dict[src].append(tgt)
        else:
            src_tgt_dict[src] = [tgt]
            
    
    return num_nodes, train_edge, src_tgt_dict, v_feat, a_feat, t_feat

def data_load_old(train_edge, v_feat_path, t_feat_path):
    
    v_feat = torch.load(v_feat_path).to('cuda')
    t_feat = torch.load(t_feat_path).to('cuda')
    a_feat = None
    
    num_nodes = v_feat.shape[0]
    
    src_tgt_dict = {i: [] for i in range(num_nodes)}
    for pair in tqdm(train_edge, total=train_edge.shape[0]):
        src, tgt = pair[0].item(), pair[1].item()
        if src in src_tgt_dict.keys():
            src_tgt_dict[src].append(tgt)
        else:
            src_tgt_dict[src] = [tgt]
            
    
    return num_nodes, train_edge, src_tgt_dict, v_feat, a_feat, t_feat


class TrainingDataset(Dataset):
    def __init__(self, num_nodes, train_nodes, labels):
        self.num_nodes = num_nodes
        self.train_nodes = torch.LongTensor(train_nodes)
        self.labels = labels

    def __len__(self):
        return len(self.train_nodes)

    def __getitem__(self, index):
        src = self.train_nodes[index].item()
        label = self.labels[src].item()
        return torch.LongTensor([src]), torch.LongTensor([label])

</file>

<file path="nc/MMGCN/Full_vt.py">
from tqdm import tqdm
import torch
import torch.nn as nn
from torch.autograd import no_grad
import numpy as np

def full_vt(epoch, model, data, prefix, writer=None):   
    print(prefix+' start...')
    model.eval()

    with no_grad():
        precision, recall, ndcg_score = model.full_accuracy(data)
        print('---------------------------------{0}-th Precition:{1:.4f} Recall:{2:.4f} NDCG:{3:.4f}---------------------------------'.format(
            epoch, precision, recall, ndcg_score))
        if writer is not None:
            writer.add_scalar(prefix+'_Precition', precision, epoch)
            writer.add_scalar(prefix+'_Recall', recall, epoch)
            writer.add_scalar(prefix+'_NDCG', ndcg_score, epoch)

            writer.add_histogram(prefix+'_visual_distribution', model.v_rep, epoch)
            writer.add_histogram(prefix+'_acoustic_distribution', model.a_rep, epoch)
            writer.add_histogram(prefix+'_textual_distribution', model.t_rep, epoch)
            
            writer.add_histogram(prefix+'_user_visual_distribution', model.user_preferences[:,:44], epoch)
            writer.add_histogram(prefix+'_user_acoustic_distribution', model.user_preferences[:, 44:-44], epoch)
            writer.add_histogram(prefix+'_user_textual_distribution', model.user_preferences[:, -44:], epoch)

            writer.add_embedding(model.v_rep)
            writer.add_embedding(model.a_rep)
            writer.add_embedding(model.t_rep)
            
        return precision, recall, ndcg_score




</file>

<file path="nc/MMGCN/Model_MMGCN.py">
import math
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import Parameter
from BaseModel import BaseModel

class GCN(torch.nn.Module):
    def __init__(self, edge_index, batch_size, num_node, dim_feat, dim_id, aggr_mode, concate, has_id, num_layers, dim_latent=None):
        super(GCN, self).__init__()
        self.batch_size = batch_size
        self.num_node = num_node
        self.dim_id = dim_id
        self.dim_feat = dim_feat
        self.dim_latent = dim_latent
        self.edge_index = edge_index
        self.aggr_mode = aggr_mode
        self.concate = concate
        self.has_id = has_id
        self.num_layers = num_layers

        if self.dim_latent:
            self.MLP = nn.Linear(self.dim_feat, self.dim_latent)
            self.conv_embed_1 = BaseModel(self.dim_latent, self.dim_latent, aggr=self.aggr_mode)
            nn.init.xavier_normal_(self.conv_embed_1.weight)
            self.linear_layer1 = nn.Linear(self.dim_latent, self.dim_id)
            nn.init.xavier_normal_(self.linear_layer1.weight)
            self.g_layer1 = nn.Linear(self.dim_latent+self.dim_id, self.dim_id) if self.concate else nn.Linear(self.dim_latent, self.dim_id)    
            nn.init.xavier_normal_(self.g_layer1.weight) 

        else:
            self.conv_embed_1 = BaseModel(self.dim_feat, self.dim_feat, aggr=self.aggr_mode)
            nn.init.xavier_normal_(self.conv_embed_1.weight)
            self.linear_layer1 = nn.Linear(self.dim_feat, self.dim_id)
            nn.init.xavier_normal_(self.linear_layer1.weight)
            self.g_layer1 = nn.Linear(self.dim_feat+self.dim_id, self.dim_id) if self.concate else nn.Linear(self.dim_feat, self.dim_id)     
            nn.init.xavier_normal_(self.g_layer1.weight)              
          
        if num_layers >= 2:
            self.conv_embed_2 = BaseModel(self.dim_id, self.dim_id, aggr=self.aggr_mode)
            nn.init.xavier_normal_(self.conv_embed_2.weight)
            self.linear_layer2 = nn.Linear(self.dim_id, self.dim_id)
            nn.init.xavier_normal_(self.linear_layer2.weight)
            self.g_layer2 = nn.Linear(self.dim_id+self.dim_id, self.dim_id) if self.concate else nn.Linear(self.dim_id, self.dim_id)

        if num_layers >= 3:
            self.conv_embed_3 = BaseModel(self.dim_id, self.dim_id, aggr=self.aggr_mode)
            nn.init.xavier_normal_(self.conv_embed_3.weight)
            self.linear_layer3 = nn.Linear(self.dim_id, self.dim_id)
            nn.init.xavier_normal_(self.linear_layer3.weight)
            self.g_layer3 = nn.Linear(self.dim_id+self.dim_id, self.dim_id) if self.concate else nn.Linear(self.dim_id, self.dim_id)
        
        if num_layers >= 4:
            self.conv_embed_4 = BaseModel(self.dim_id, self.dim_id, aggr=self.aggr_mode)
            nn.init.xavier_normal_(self.conv_embed_4.weight)
            self.linear_layer4 = nn.Linear(self.dim_id, self.dim_id)
            nn.init.xavier_normal_(self.linear_layer4.weight)
            self.g_layer4 = nn.Linear(self.dim_id+self.dim_id, self.dim_id) if self.concate else nn.Linear(self.dim_id, self.dim_id)
        
        if num_layers >= 5:
            self.conv_embed_5 = BaseModel(self.dim_id, self.dim_id, aggr=self.aggr_mode)
            nn.init.xavier_normal_(self.conv_embed_5.weight)
            self.linear_layer5 = nn.Linear(self.dim_id, self.dim_id)
            nn.init.xavier_normal_(self.linear_layer5.weight)
            self.g_layer5 = nn.Linear(self.dim_id+self.dim_id, self.dim_id) if self.concate else nn.Linear(self.dim_id, self.dim_id)

    def forward(self, features, id_embedding):
        temp_features = self.MLP(features) if self.dim_latent else features

        x = temp_features
        x = F.normalize(x).cuda()

        h = F.leaky_relu(self.conv_embed_1(x, self.edge_index))#equation 1
        x_hat = F.leaky_relu(self.linear_layer1(x)) + id_embedding if self.has_id else F.leaky_relu(self.linear_layer1(x))#equation 5 
        x = F.leaky_relu(self.g_layer1(torch.cat((h, x_hat), dim=1))) if self.concate else F.leaky_relu(self.g_layer1(h)+x_hat)

        if self.num_layers >= 2:
            h = F.leaky_relu(self.conv_embed_2(x, self.edge_index))#equation 1
            x_hat = F.leaky_relu(self.linear_layer2(x)) + id_embedding if self.has_id else F.leaky_relu(self.linear_layer2(x))#equation 5
            x = F.leaky_relu(self.g_layer2(torch.cat((h, x_hat), dim=1))) if self.concate else F.leaky_relu(self.g_layer2(h)+x_hat)

        if self.num_layers >= 3:
            h = F.leaky_relu(self.conv_embed_3(x, self.edge_index))#equation 1
            x_hat = F.leaky_relu(self.linear_layer3(x)) + id_embedding if self.has_id else F.leaky_relu(self.linear_layer3(x))#equation 5
            x = F.leaky_relu(self.g_layer3(torch.cat((h, x_hat), dim=1))) if self.concate else F.leaky_relu(self.g_layer3(h)+x_hat)
        
        if self.num_layers >= 4:
            h = F.leaky_relu(self.conv_embed_4(x, self.edge_index))#equation 1
            x_hat = F.leaky_relu(self.linear_layer4(x)) + id_embedding if self.has_id else F.leaky_relu(self.linear_layer4(x))#equation 6
            x = F.leaky_relu(self.g_layer4(torch.cat((h, x_hat), dim=1))) if self.concate else F.leaky_relu(self.g_layer4(h)+x_hat)
        
        if self.num_layers >= 5:
            h = F.leaky_relu(self.conv_embed_5(x, self.edge_index))#equation 1
            x_hat = F.leaky_relu(self.linear_layer5(x)) + id_embedding if self.has_id else F.leaky_relu(self.linear_layer5(x))#equation 7
            x = F.leaky_relu(self.g_layer5(torch.cat((h, x_hat), dim=1))) if self.concate else F.leaky_relu(self.g_layer5(h)+x_hat)

        return x


class Net(torch.nn.Module):
    def __init__(self, v_feat, t_feat, edge_index, batch_size, num_node, aggr_mode, concate, num_layers, has_id, reg_weight, dim_x):
        super(Net, self).__init__()
        self.batch_size = batch_size
        self.num_node = num_node
        self.aggr_mode = aggr_mode
        self.concate = concate
        self.weight = torch.tensor([[1.0],[-1.0]]).cuda()
        self.reg_weight = reg_weight
        
        self.edge_index = torch.tensor(edge_index).t().contiguous().cuda()
        self.edge_index = torch.cat((self.edge_index, self.edge_index[[1,0]]), dim=1)
        self.num_modal = 0

        self.v_feat = torch.tensor(v_feat,dtype=torch.float).cuda()
        self.v_gcn = GCN(self.edge_index, batch_size, num_node, self.v_feat.size(1), dim_x, self.aggr_mode, self.concate, num_layers=num_layers, has_id=has_id, dim_latent=256)

        self.t_feat = torch.tensor(t_feat,dtype=torch.float).cuda()
        self.t_gcn = GCN(self.edge_index, batch_size, num_node, self.t_feat.size(1), dim_x, self.aggr_mode, self.concate, num_layers=num_layers, has_id=has_id)

        self.id_embedding = nn.init.xavier_normal_(torch.rand((num_node, dim_x), requires_grad=True)).cuda()
        self.result = nn.init.xavier_normal_(torch.rand((num_node, dim_x))).cuda()

        self.transform = nn.Sequential(
            nn.Linear(dim_x, dim_x),
            nn.ReLU(),
            nn.Linear(dim_x, dim_x),
            nn.ReLU(),
            nn.Linear(dim_x, 12))


    def forward(self):
        v_rep = self.v_gcn(self.v_feat, self.id_embedding)
        a_rep = None

        # # self.t_feat = torch.tensor(scatter_('mean', self.word_embedding(self.words_tensor[1]), self.words_tensor[0])).cuda()
        t_rep = self.t_gcn(self.t_feat, self.id_embedding)
        
        if a_rep is not None:
            representation = (v_rep+a_rep+t_rep)/3
        else:
            representation = (v_rep+t_rep)/2

        representation = self.transform(representation)

        return representation

    def loss(self, node, label):
        label = label.cuda()
        node = node.view(-1)
        out = self.forward()
        y_hat = out[node]
        loss = F.cross_entropy(y_hat, label[:,0])
        return loss

</file>

<file path="nc/MMGCN/main.py">
import argparse
import os
import dgl
import dgl.nn as dglnn
import torch
import torch.nn as nn
import torch.nn.functional as F
import tqdm
from dgl.data import AsNodePredDataset
from dgl.dataloading import (
    DataLoader,
    MultiLayerFullNeighborSampler,
    NeighborSampler,
)
from ogb.nodeproppred import DglNodePropPredDataset
from dgl.nn import GATv2Conv
import hydra
import logging
from omegaconf import DictConfig, OmegaConf
from Model_MMGCN import Net

PROJETC_DIR = os.path.join(os.path.dirname(os.path.realpath(__file__)), './')
CONFIG_DIR = os.path.join(PROJETC_DIR, "configs")
log = logging.getLogger(__name__)

class Logger(object):
    def __init__(self, runs):
        self.results = [[] for _ in range(runs)]

    def add_result(self, run, result):
        assert 0 <= run < len(self.results)
        self.results[run].append(result)

    def print_statistics(self, run=None):
        if run is not None:
            result = 100 * torch.tensor(self.results[run])
            argmax = result[:, 0].argmax().item()
            log.info(f'Run {run + 1:02d}:')
            log.info(f'Highest Valid: {result[:, 0].max():.2f}')
            log.info(f'   Final Test: {result[argmax, 1]:.2f}')
        else:
            result = 100 * torch.tensor(self.results)

            best_results = []
            for r in result:
                valid = r[:, 0].max().item()
                test = r[r[:, 0].argmax(), 1].item()
                best_results.append((valid, test))

            best_result = torch.tensor(best_results)

            log.info(f'All runs:')
            r = best_result[:, 0]
            log.info(f'Highest Valid: {r.mean():.2f} ± {r.std():.2f}')
            r = best_result[:, 1]
            log.info(f'   Final Test: {r.mean():.2f} ± {r.std():.2f}')

def to_bidirected_with_reverse_mapping(g):
    """Makes a graph bidirectional, and returns a mapping array ``mapping`` where ``mapping[i]``
    is the reverse edge of edge ID ``i``. Does not work with graphs that have self-loops.
    """
    g_simple, mapping = dgl.to_simple(
        dgl.add_reverse_edges(g), return_counts='count', writeback_mapping=True)
    c = g_simple.edata['count']
    num_edges = g.num_edges()
    mapping_offset = torch.zeros(g_simple.num_edges() + 1, dtype=g_simple.idtype)
    mapping_offset[1:] = c.cumsum(0)
    idx = mapping.argsort()
    idx_uniq = idx[mapping_offset[:-1]]
    reverse_idx = torch.where(idx_uniq >= num_edges, idx_uniq - num_edges, idx_uniq + num_edges)
    reverse_mapping = mapping[reverse_idx]
    # sanity check
    src1, dst1 = g_simple.edges()
    src2, dst2 = g_simple.find_edges(reverse_mapping)
    assert torch.equal(src1, dst2)
    assert torch.equal(src2, dst1)
    return g_simple, reverse_mapping


def evaluate(cfg, model, graph, dataloader, num_classes):
    model.eval()
    ys = []
    y_hats = []
    for it, (input_nodes, output_nodes, blocks) in enumerate(dataloader):
        with torch.no_grad():
            if cfg.model_name == "MLP":
                x = blocks[-1].dstdata["feat"]
            else:
                x = blocks[0].srcdata["feat"]
            ys.append(blocks[-1].dstdata["label"])
            y_hats.append(model(blocks, x))
    pred = torch.argmax(torch.cat(y_hats), dim=1)
    acc = (pred == torch.cat(ys)).sum()/torch.cat(ys).shape[0]
    return acc


def layerwise_infer(device, graph, nid, model, num_classes, batch_size):
    model.eval()
    with torch.no_grad():
        pred = model.inference(
            graph, device, batch_size
        )  # pred in buffer_device
        pred = pred[nid]
        label = graph.ndata["label"][nid].to(pred.device)
        pred = torch.argmax(pred, dim=1)
        acc = (pred == label).sum()/label.shape[0]
        return acc


def train(cfg, device, g, dataset, model, num_classes, run):
    # create sampler & dataloader
    if not os.path.exists(cfg.checkpoint_folder):
        os.makedirs(cfg.checkpoint_folder)
    checkpoint_path = cfg.checkpoint_folder + cfg.model_name + "_" + cfg.dataset + "_" + "batch_size_" + str(
            cfg.batch_size) + "_n_layers_" + str(cfg.num_layers) + "_hidden_dim_" + str(cfg.hidden_dim) + "_lr_" + str(
            cfg.lr) + "_exclude_degree_" + str(cfg.exclude_target_degree) + "_full_neighbor_" + str(
            cfg.full_neighbor) + "_accu_num_" + str(cfg.accum_iter_number) + "_trail_" + str(run) + "_best.pth"
    train_idx = torch.LongTensor(dataset['train_idx']).to(device)
    val_idx = torch.LongTensor(dataset['val_idx']).to(device)
    if cfg.full_neighbor:
        log.info("We use the full neighbor of the target node to train the models. ")
        sampler = MultiLayerFullNeighborSampler(num_layers=cfg.num_layers, prefetch_node_feats=['feat'])
    else:
        log.info("We sample the neighbor node of the target node to train the models. ")
        sampler = NeighborSampler([cfg.num_of_neighbors] * cfg.num_layers, prefetch_node_feats=['feat'])
    use_uva = cfg.mode == "mixed"
    train_dataloader = DataLoader(
        g,
        train_idx,
        sampler,
        device=device,
        batch_size=512,
        shuffle=True,
        drop_last=False,
        num_workers=0,
        use_uva=use_uva,
    )

    val_dataloader = DataLoader(
        g,
        val_idx,
        sampler,
        device=device,
        batch_size=512,
        shuffle=True,
        drop_last=False,
        num_workers=0,
        use_uva=use_uva,
    )

    opt = torch.optim.Adam(model.parameters(), lr=cfg.lr, weight_decay=5e-4)
    # opt = torch.optim.SGD(model.parameters(), lr=cfg.lr)
    lr_scheduler = torch.optim.lr_scheduler.StepLR(
        opt, 
        step_size=cfg.lr_scheduler_step_size,
        gamma=cfg.lr_scheduler_gamma,
    )

    best_acc = 0
    for epoch in range(cfg.n_epochs):
        model.train()
        total_loss = 0
        valid_result = 0
        for it, (input_nodes, output_nodes, blocks) in enumerate(
            train_dataloader
        ):
            if cfg.model_name == "MLP":
                x = blocks[-1].dstdata["feat"]
            else:
                x = blocks[0].srcdata["feat"]
            y = blocks[-1].dstdata["label"]
            breakpoint()
            y_hat = Net(blocks, x)
            loss = F.cross_entropy(y_hat, y)
            opt.zero_grad()
            loss.backward()
            opt.step()
            total_loss += loss.item()
        acc = evaluate(cfg, model, g, val_dataloader, num_classes)
        log.info(
            "Epoch {:05d} | Loss {:.4f} | Accuracy {:.4f} ".format(
                epoch, total_loss / (it + 1), acc.item()
            )
        )
        lr_scheduler.step()

        if acc > valid_result:
            valid_result = acc
            torch.save({'epoch': epoch + 1, 'model_state_dict': model.state_dict(),}, checkpoint_path)
            
            # test the model
            log.info("Validation...")
            log.info("Validation Accuracy {:.4f}".format(acc.item()))
            log.info("Testing...")
            acc = layerwise_infer(
                device, g, torch.LongTensor(dataset['test_idx']), model, num_classes, batch_size=1024
            )
            log.info("Test Accuracy {:.4f}".format(acc.item()))
        
        if acc.item() > best_acc:
            best_acc = acc.item()
    
    return best_acc


@hydra.main(config_path=CONFIG_DIR, config_name="defaults", version_base='1.2')
def main(cfg: DictConfig):
    # save configs
    if not os.path.isfile("config.yaml"):
        OmegaConf.save(config=cfg, f=os.path.join("config.yaml"))

    # load and preprocess dataset
    device = torch.device('cpu' if cfg.mode == 'cpu' else 'cuda')

    # load and preprocess dataset
    log.info("Loading data")
    BOOKS_PATH = "/nfs/turbo/coe-dkoutra/jing/Next-GraphGPT/books/nc"
    # BOOKS_PATH = "/nfs/turbo/coe-dkoutra/jing/Next-GraphGPT/amazon_nc/fashion"
    edges = torch.load(os.path.join(BOOKS_PATH, "nc_edges-nodeid.pt"))
    labels = torch.LongTensor(torch.load(os.path.join(BOOKS_PATH, "labels-w-missing.pt")))
    splits = torch.load(os.path.join(BOOKS_PATH, "split.pt"))
    if cfg.feat == 'clip':
        clip_feat = torch.load(os.path.join(BOOKS_PATH, 'clip_feat.pt'))
    elif cfg.feat == 'imagebind':
        clip_feat = torch.load(os.path.join(BOOKS_PATH, 'imagebind_feat.pt'))
    else: 
        clip_feat = torch.load(os.path.join(BOOKS_PATH, 't5vit_feat.pt'))
    if cfg.use_feature == 'text':
        clip_feat = clip_feat[:,:512]    
    edges = torch.LongTensor(edges)
    g = dgl.graph((edges[:,0], edges[:,1]))
    g.ndata['feat'] = clip_feat
    g.ndata['label'] = labels
    g = dgl.remove_self_loop(g)
    g, reverse_eids = to_bidirected_with_reverse_mapping(g)
    g = g.to("cuda" if cfg.mode == "puregpu" else "cpu")
    num_classes = len(torch.unique(labels))
    # num_classes = 12

    # create GraphSAGE model
    in_size = g.ndata["feat"].shape[1]
    out_size = num_classes
    accs = []
    for run in range(cfg.runs):
        log.info("Run {}/{}".format(run + 1, cfg.runs))
        if cfg.model_name == "SAGE":
            model = SAGE(in_size, cfg.hidden_dim, out_size, cfg.num_layers).to(device)
        elif cfg.model_name == "GCN":
            model = GCN(in_size, cfg.hidden_dim, out_size, cfg.num_layers).to(device)
            if cfg.add_self_loop:
                g = dgl.add_self_loop(g)
        elif cfg.model_name == "MLP":
            model = MLP(in_size, cfg.hidden_dim, out_size, cfg.num_layers).to(device)        
        # # model training
        log.info("Training...")
        acc = train(cfg, device, g, splits, model, num_classes, run)
        accs.append(acc)

    mean_acc = torch.mean(torch.tensor(accs)).item()
    std_acc = torch.std(torch.tensor(accs)).item()
    print(mean_acc)
    print(std_acc)

    return mean_acc

if __name__=='__main__':
    main()
</file>

<file path="nc/MMGCN/mmgcn.sh">

python run.py \
    --l_r 0.01233773647859825 \
    --weight_decay 1e-5 \
    --batch_size 1024 \
    --num_epoch 30 \
    --num_workers 4 \
    --aggr_mode mean \
    --num_layers 1 \
    --has_a False \
    --feat_path /nfs/turbo/coe-dkoutra/jing/Next-GraphGPT/amazon_nc/fashion/t5dino_feat.pt \
    --project_name mmgcn_nc \
    --wandb_run_name hard_clip\
    --PATH_best_weight_save './mmgcn_amazon_nc_ckpt.pt' \
    --PATH_best_metrics_save './mmgcn_amazon_nc_best_metrics.txt' \
    --PATH_best_hyperparameter_save './mmgcn_amazon_nc_best_hyperparameter.txt' \
    # --do_hyperparameter_search
</file>

<file path="nc/MMGCN/run.py">
import argparse
import sys
import os
import time
import random
import numpy as np
import torch
from Dataset import TrainingDataset, data_load, data_load_old
from Model_MMGCN import Net
from torch.utils.data import DataLoader, TensorDataset
# from Train import train
from Full_vt import full_vt
import wandb
from tqdm import tqdm
import optuna



def batch_evaluate_model(representation, triple_data, batch_size=128, verbose=False):
    dataset = TensorDataset(
        torch.tensor(triple_data['source_node']),
        torch.tensor(triple_data['target_node']),
        torch.tensor(triple_data['target_node_neg'], dtype=torch.long))

    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)

    rank_ls = []
    for src, pos_tgt, neg_tgt_ls in tqdm(dataloader, disable=not verbose):
        src_ls = src.repeat_interleave(neg_tgt_ls.size(1) + 1, dim=0)
        tgt_ls = torch.cat((pos_tgt.unsqueeze(1), neg_tgt_ls), dim=1).view(-1)

        src_score = representation[src_ls]
        tgt_score = representation[tgt_ls]

        pred = torch.sum(src_score * tgt_score, dim=1).detach().cpu()
        rank = get_rank_2d(pred.view(src.size(0), -1))
        rank_ls.extend(rank.numpy().tolist())

    hit_10 = calculate_hit(rank_ls, 10)
    hit_1 = calculate_hit(rank_ls, 1)
    mrr = calculate_mrr(rank_ls)
    return mrr, hit_1, hit_10


def train(dataloader, model, optimizer, batch_size, max_step=None):
    model.train()
    sum_loss = 0.0
    step = 0.0
    num_pbar = 0
    for idx, batch in enumerate(tqdm(dataloader)):
        node, label = batch
        optimizer.zero_grad()
        loss = model.loss(node, label)
        loss.backward(retain_graph=True)
        optimizer.step()
        num_pbar += batch_size
        sum_loss += loss.cpu().item()
        step += 1.0
        if max_step is not None and idx > max_step:
            break

    return loss


def hyperparameter_search(train_dataloader, args, v_feat, a_feat, t_feat, train_edge, num_nodes,
                          weight_decay, dim_E, valid_nodes, test_nodes, labels):
    def objective(trial):
        lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)
        num_layers = trial.suggest_int('num_layers', 1, 3)

        model = Net(v_feat, t_feat, train_edge, args.batch_size, num_nodes, 'mean', 'False', num_layers,
                    True, weight_decay, dim_E).cuda()
        optimizer = torch.optim.Adam([{'params': model.parameters(), 'lr': lr}])


        val_max_mrr = 0
        for epoch in range(args.num_epoch):
            train(train_dataloader, model, optimizer, args.batch_size, max_step=None)
            torch.cuda.empty_cache()

            if (epoch + 1) % 5 == 0:
                with torch.no_grad():
                    representation = model.forward()
                    val_rep = representation[valid_nodes].cpu()
                    test_rep = representation[test_nodes].cpu()
                    val_label = labels[valid_nodes]
                    val_pred = torch.argmax(val_rep, dim=1)
                    val_acc = (val_pred == val_label).sum()/val_label.shape[0]
                    test_label = labels[test_nodes]
                    test_pred = torch.argmax(test_rep, dim=1)
                    test_acc = (test_pred == test_label).sum()/test_label.shape[0]

                    if val_acc > val_max_mrr:
                        val_max_mrr = val_acc
                    print(f"{lr}, {num_layers}")
                    print(f"Epoch: {epoch}, Val Acc: {val_max_mrr}, Test Acc: {test_acc}")

        return val_max_mrr

    study = optuna.create_study(direction='maximize')
    study.optimize(objective, n_trials=args.n_trials)

    print(study.best_params)

    if args.PATH_best_hyperparameter_save is not None:
        with open(args.PATH_best_hyperparameter_save, 'w') as f:
            f.write(study.best_params)
            f.write('\n')
            f.write(study.best_value)


def main():
    parser = argparse.ArgumentParser()

    # using default values is recommended
    parser.add_argument('--seed', type=int, default=1, help='Seed init.')
    parser.add_argument('--no-cuda', action='store_true', default=False, help='Disables CUDA training.')
    parser.add_argument('--data_path', default='movielens', help='Dataset path')
    parser.add_argument('--save_file', default='', help='Filename')
    parser.add_argument('--PATH_weight_load', default=None, help='Loading weight filename.')
    parser.add_argument('--num_workers', type=int, default=1, help='Workers number.')
    parser.add_argument('--dim_E', type=int, default=64, help='Embedding dimension.')
    parser.add_argument('--prefix', default='', help='Prefix of save_file.')
    parser.add_argument('--aggr_mode', default='add', help='Aggregation Mode.')
    parser.add_argument('--topK', type=int, default=10, help='Workers number.')
    parser.add_argument('--has_entropy_loss', default='False', help='Has Cross Entropy loss.')
    parser.add_argument('--has_weight_loss', default='False', help='Has Weight Loss.')
    parser.add_argument('--has_v', default='True', help='Has Visual Features.')
    parser.add_argument('--has_a', default='True', help='Has Acoustic Features.')
    parser.add_argument('--has_t', default='True', help='Has Textual Features.')
    parser.add_argument('--weight_decay', type=float, default=1e-5, help='Weight decay.')

    # your data here
    parser.add_argument('--v_feat_path', default=None)
    parser.add_argument('--t_feat_path', default=None)
    parser.add_argument('--feat_path', default=None)

    # your setting here
    parser.add_argument('--l_r', type=float, default=1e-4, help='Learning rate.')
    parser.add_argument('--num_layers', default=3, type=int)
    parser.add_argument('--batch_size', type=int, default=1024, help='Batch size.')
    parser.add_argument('--num_epoch', type=int, default=1000, help='Epoch number.')
    parser.add_argument('--repeat_times', type=int, default=3, help='Repeat times.')

    parser.add_argument('--PATH_best_weight_save', default=None, help='Writing weight filename.')
    parser.add_argument('--PATH_best_metrics_save', default=None)

    # if want to do hyperparameter search
    parser.add_argument('--do_hyperparameter_search', action="store_true")
    parser.add_argument('--PATH_best_hyperparameter_save', default=None)
    parser.add_argument('--n_trials', type=int, default=20)
    parser.add_argument('--hyperparameter_search_max_step', type=int, default=10000)

    parser.add_argument('--project_name', default='MGAT')
    parser.add_argument('--wandb_run_name', default='untitled_run')
    parser.add_argument('--wandb_key', default=None)
    parser.add_argument('--report_to', default=None)

    args = parser.parse_args()

    if args.wandb_key is not None and args.report_to == 'wandb':
        wandb_key = 'ab1e2fd95c62273341f8fe8bbe29b9a6ee33725a'
        wandb.login(key=wandb_key)

    seed = args.seed
    np.random.seed(seed)
    device = torch.device("cuda:0" if torch.cuda.is_available() and not args.no_cuda else "cpu")
    ##########################################################################################################################################
    data_path = args.data_path
    save_file = args.save_file

    learning_rate = args.l_r
    weight_decay = args.weight_decay
    batch_size = args.batch_size
    num_workers = args.num_workers
    num_epoch = args.num_epoch
    topK = args.topK
    prefix = args.prefix
    aggr_mode = args.aggr_mode

    has_v = True if args.has_v == 'True' else False
    has_a = True if args.has_a == 'True' else False
    has_t = True if args.has_t == 'True' else False

    has_entropy_loss = True if args.has_entropy_loss == 'True' else False
    has_weight_loss = True if args.has_weight_loss == 'True' else False
    dim_E = args.dim_E
    writer = None

    BOOKS_PATH = "/nfs/turbo/coe-dkoutra/jing/Next-GraphGPT/amazon_nc/fashion"
    edges = torch.load(os.path.join(BOOKS_PATH, "nc_edges-nodeid.pt"))
    labels = torch.LongTensor(torch.load(os.path.join(BOOKS_PATH, "labels-w-missing.pt")))
    splits = torch.load(os.path.join(BOOKS_PATH, "split.pt"))
    train_edge = torch.LongTensor(edges)
    num_classes = 12
    num_nodes = labels.shape[0]
    clip_feat = torch.load(args.feat_path)
    t_feat = clip_feat[:, :int(clip_feat.shape[1] / 2)]
    v_feat = clip_feat[:, int(clip_feat.shape[1] / 2):]

    ##########################################################################################################################################
    print('Data loading ...')
    # if args.feat_path is None:
    #     num_nodes, train_edge, src_tgt_dict, v_feat, a_feat, t_feat = data_load_old(train_edge, args.v_feat_path,
    #                                                                                 args.t_feat_path)
    # else:
    #     num_nodes, train_edge, src_tgt_dict, v_feat, a_feat, t_feat = data_load(train_edge, args.feat_path)

    train_dataset = TrainingDataset(num_nodes, splits['train_idx'], labels)

    print('Data has been loaded.')

    if args.do_hyperparameter_search:
        valid_nodes = splits['val_idx']                
        test_nodes = splits['test_idx']
        a_feat = None
        train_dataloader = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=4)
        hyperparameter_search(train_dataloader, args, v_feat, a_feat, t_feat, train_edge, num_nodes,
                              weight_decay, dim_E, valid_nodes, test_nodes, labels)
        return

    num_layers = args.num_layers
    learning_rate = args.l_r
    num_epoch = args.num_epoch
    batch_size = args.batch_size

    global_step = 0
    mrrs, h1s, h10s = [], [], []
    for try_time in range(args.repeat_times):
        model = Net(v_feat, t_feat, train_edge, batch_size, num_nodes, 'mean', 'False', num_layers, True, weight_decay, dim_E).cuda()
        optimizer = torch.optim.Adam([{'params': model.parameters(), 'lr': learning_rate}])

        if args.report_to == 'wandb':
            wandb.init(
                project=args.project_name,
                name=args.wandb_run_name,
                config={
                    "learning_rate": learning_rate,
                    'num_layers': num_layers,
                    "epochs": num_epoch,
                }
            )

        max_acc = 0.0
        max_hit_1 = 0.0
        max_hit_10 = 0.0
        val_max_acc = 0.0
        num_decreases = 0
        for epoch in range(num_epoch):
            train_dataloader = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=4)
            loss = train(train_dataloader, model, optimizer, batch_size, max_step=None)
            torch.cuda.empty_cache()

            with torch.no_grad():
                representation = model.forward()
                valid_nodes = splits['val_idx']
                val_rep = representation[valid_nodes].cpu()
                test_nodes = splits['test_idx']
                test_rep = representation[test_nodes].cpu()
                val_label = labels[valid_nodes]
                val_pred = torch.argmax(val_rep, dim=1)
                val_acc = (val_pred == val_label).sum()/val_label.shape[0]
                test_label = labels[test_nodes]
                test_pred = torch.argmax(test_rep, dim=1)
                test_acc = (test_pred == test_label).sum()/test_label.shape[0]

            print({
                'loss': loss,
                "val_acc": val_acc,
                "test_acc": test_acc,
            })

            if val_acc > val_max_acc:
                val_max_acc = val_acc
                max_acc = test_acc
                num_decreases = 0

                if args.PATH_best_weight_save is not None:
                    torch.save(model.state_dict(), args.PATH_best_weight_save)

                if args.PATH_best_metrics_save is not None:
                    with open(args.PATH_best_metrics_save, 'w') as f:
                        f.write(f"MRR: {max_acc}")
            global_step += 1

        mrrs.append(test_acc)

    mean_mrr = torch.mean(torch.tensor(mrrs)).item()
    std_mrr = torch.std(torch.tensor(mrrs)).item()
    print(mean_mrr)
    print(std_mrr)


if __name__ == '__main__':
    main()

</file>

<file path="nc/configs/defaults.yaml">
defaults:
  # - override hydra/launcher: submitit_slurm
  - _self_

mode: 'puregpu'
model_name: 'SAGE'
checkpoint_folder: 'output/'
load_checkpoint_folder: None
dataset: ogbn-arxiv
dataset_finetuned: False
nodes_image_ratio: 0.1
overlap_ratio: 0.1
n_epochs: 30
runs: 3
log_steps: 1
accum_iter_number: 1
lr: 5e-4
hidden_dim: 256
num_layers: 3
batch_size: 512
exclude_target_degree: 0
num_of_neighbors: 15
full_neighbor: False
inference_mode: 'train'
preload_node_emb: True
add_self_loop: False
eval_dir: ''
use_feature: 'original'
feat: 'clip'
use_concat: False
no_eval: False
lr_scheduler_step_size: 5
lr_scheduler_gamma: 0.1
patton_dir: /nfs/turbo/coe-dkoutra/jing/Next-GraphGPT/Patton/cloth-coview
hydra:
  job:
    chdir: True
  # launcher:
  #   name: 'gnn'
  #   timeout_min: 4320
  #   cpus_per_task: 4
  #   gres: 'gpu:1'
  #   tasks_per_node: 1
  #   mem_per_cpu: 10000
  #   partition: 'spgpu'
  #   account: 'dkoutra1'
  #   array_parallelism: 3 # limit the number of jobs can be run at the same time
  #   exclude: 'gl1507, gl1510, gl1513'
</file>

<file path="nc/configs/optuna.yaml">
defaults:
  - defaults
  - override hydra/sweeper: optuna
  - _self_


hydra:
  sweeper:
    sampler:
      seed: 123
    direction: maximize
    study_name: gnn
    storage: null
    n_trials: 20
    n_jobs: 1
    # max_failure_rate: 0.0
    params:
      lr: tag(log, interval(1e-4, 1e-2))
      num_layers: choice(1, 2, 3)


</file>

<file path="nc/main.py">
import argparse
import os
import dgl
import dgl.nn as dglnn
import torch
import torch.nn as nn
import torch.nn.functional as F
import tqdm
from dgl.data import AsNodePredDataset
from dgl.dataloading import (
    DataLoader,
    MultiLayerFullNeighborSampler,
    NeighborSampler,
)
from nc_dataset import NodeClassificationDataset, NodeClassificationEvaluator
from dgl.nn import GATv2Conv
import hydra
import logging
from omegaconf import DictConfig, OmegaConf
from models import SAGE, GCN, MLP, MMGCN

PROJETC_DIR = os.path.join(os.path.dirname(os.path.realpath(__file__)), './')
CONFIG_DIR = os.path.join(PROJETC_DIR, "configs")
log = logging.getLogger(__name__)

class Logger(object):
    def __init__(self, runs):
        self.results = [[] for _ in range(runs)]

    def add_result(self, run, result):
        assert 0 <= run < len(self.results)
        self.results[run].append(result)

    def print_statistics(self, run=None):
        if run is not None:
            result = 100 * torch.tensor(self.results[run])
            argmax = result[:, 0].argmax().item()
            log.info(f'Run {run + 1:02d}:')
            log.info(f'Highest Valid: {result[:, 0].max():.2f}')
            log.info(f'   Final Test: {result[argmax, 1]:.2f}')
        else:
            result = 100 * torch.tensor(self.results)

            best_results = []
            for r in result:
                valid = r[:, 0].max().item()
                test = r[r[:, 0].argmax(), 1].item()
                best_results.append((valid, test))

            best_result = torch.tensor(best_results)

            log.info(f'All runs:')
            r = best_result[:, 0]
            log.info(f'Highest Valid: {r.mean():.2f} ± {r.std():.2f}')
            r = best_result[:, 1]
            log.info(f'   Final Test: {r.mean():.2f} ± {r.std():.2f}')

def to_bidirected_with_reverse_mapping(g):
    """Makes a graph bidirectional, and returns a mapping array ``mapping`` where ``mapping[i]``
    is the reverse edge of edge ID ``i``. Does not work with graphs that have self-loops.
    """
    g_simple, mapping = dgl.to_simple(
        dgl.add_reverse_edges(g), return_counts='count', writeback_mapping=True)
    c = g_simple.edata['count']
    num_edges = g.num_edges()
    mapping_offset = torch.zeros(g_simple.num_edges() + 1, dtype=g_simple.idtype)
    mapping_offset[1:] = c.cumsum(0)
    idx = mapping.argsort()
    idx_uniq = idx[mapping_offset[:-1]]
    reverse_idx = torch.where(idx_uniq >= num_edges, idx_uniq - num_edges, idx_uniq + num_edges)
    reverse_mapping = mapping[reverse_idx]
    # sanity check
    src1, dst1 = g_simple.edges()
    src2, dst2 = g_simple.find_edges(reverse_mapping)
    assert torch.equal(src1, dst2)
    assert torch.equal(src2, dst1)
    return g_simple, reverse_mapping


def evaluate(cfg, model, graph, dataloader, num_classes):
    model.eval()
    ys = []
    y_hats = []
    for it, (input_nodes, output_nodes, blocks) in enumerate(dataloader):
        with torch.no_grad():
            if cfg.model_name == "MLP":
                x = blocks[-1].dstdata["feat"]
            else:
                x = blocks[0].srcdata["feat"]
            ys.append(blocks[-1].dstdata["label"])
            y_hats.append(model(blocks, x))
    pred = torch.argmax(torch.cat(y_hats), dim=1)
    acc = (pred == torch.cat(ys)).sum()/torch.cat(ys).shape[0]
    return acc


def layerwise_infer(device, graph, nid, model, num_classes, batch_size):
    model.eval()
    with torch.no_grad():
        pred = model.inference(
            graph, device, batch_size
        )  # pred in buffer_device
        pred = pred[nid]
        label = graph.ndata["label"][nid].to(pred.device)
        pred = torch.argmax(pred, dim=1)
        acc = (pred == label).sum()/label.shape[0]
        return acc


def train(cfg, device, g, dataset, model, num_classes, run):
    # create sampler & dataloader
    if not os.path.exists(cfg.checkpoint_folder):
        os.makedirs(cfg.checkpoint_folder)
    checkpoint_path = cfg.checkpoint_folder + cfg.model_name + "_" + cfg.dataset + "_" + "batch_size_" + str(
            cfg.batch_size) + "_n_layers_" + str(cfg.num_layers) + "_hidden_dim_" + str(cfg.hidden_dim) + "_lr_" + str(
            cfg.lr) + "_exclude_degree_" + str(cfg.exclude_target_degree) + "_full_neighbor_" + str(
            cfg.full_neighbor) + "_accu_num_" + str(cfg.accum_iter_number) + "_trail_" + str(run) + "_best.pth"
    train_idx = torch.LongTensor(dataset['train_idx']).to(device)
    val_idx = torch.LongTensor(dataset['val_idx']).to(device)
    if cfg.full_neighbor:
        log.info("We use the full neighbor of the target node to train the models. ")
        sampler = MultiLayerFullNeighborSampler(num_layers=cfg.num_layers, prefetch_node_feats=['feat'])
    else:
        log.info("We sample the neighbor node of the target node to train the models. ")
        sampler = NeighborSampler([cfg.num_of_neighbors] * cfg.num_layers, prefetch_node_feats=['feat'])
    use_uva = cfg.mode == "mixed"
    train_dataloader = DataLoader(
        g,
        train_idx,
        sampler,
        device=device,
        batch_size=512,
        shuffle=True,
        drop_last=False,
        num_workers=0,
        use_uva=use_uva,
    )

    val_dataloader = DataLoader(
        g,
        val_idx,
        sampler,
        device=device,
        batch_size=512,
        shuffle=True,
        drop_last=False,
        num_workers=0,
        use_uva=use_uva,
    )

    opt = torch.optim.Adam(model.parameters(), lr=cfg.lr, weight_decay=5e-4)
    lr_scheduler = torch.optim.lr_scheduler.StepLR(
        opt, 
        step_size=cfg.lr_scheduler_step_size,
        gamma=cfg.lr_scheduler_gamma,
    )

    best_acc = 0
    for epoch in range(cfg.n_epochs):
        model.train()
        total_loss = 0
        valid_result = 0
        for it, (input_nodes, output_nodes, blocks) in enumerate(
            train_dataloader
        ):
            if cfg.model_name == "MLP":
                x = blocks[-1].dstdata["feat"]
            else:
                x = blocks[0].srcdata["feat"]
            y = blocks[-1].dstdata["label"]
            y_hat = model(blocks, x)
            loss = F.cross_entropy(y_hat, y)
            opt.zero_grad()
            loss.backward()
            opt.step()
            total_loss += loss.item()
        acc = evaluate(cfg, model, g, val_dataloader, num_classes)
        log.info(
            "Epoch {:05d} | Loss {:.4f} | Accuracy {:.4f} ".format(
                epoch, total_loss / (it + 1), acc.item()
            )
        )
        lr_scheduler.step()

        if acc > valid_result:
            valid_result = acc
            torch.save({'epoch': epoch + 1, 'model_state_dict': model.state_dict(),}, checkpoint_path)
            
            # test the model
            log.info("Validation...")
            log.info("Validation Accuracy {:.4f}".format(acc.item()))
            log.info("Testing...")
            acc = layerwise_infer(
                device, g, torch.LongTensor(dataset['test_idx']), model, num_classes, batch_size=1024
            )
            log.info("Test Accuracy {:.4f}".format(acc.item()))
        
        if acc.item() > best_acc:
            best_acc = acc.item()
    
    return best_acc


@hydra.main(config_path=CONFIG_DIR, config_name="defaults", version_base='1.2')
def main(cfg: DictConfig):
    # save configs
    if not os.path.isfile("config.yaml"):
        OmegaConf.save(config=cfg, f=os.path.join("config.yaml"))

    # load and preprocess dataset
    device = torch.device('cpu' if cfg.mode == 'cpu' else 'cuda')

    # load and preprocess dataset
    log.info("Loading data")

    data_path = './Multimodal-Graph-Completed-Graph' # replace this with the path where you save the datasets
    dataset_name = 'books-nc'
    feat_name = 't5vit'
    verbose = True
    device = 'cpu' # use 'cuda' if GPU is available

    dataset = NodeClassificationDataset(
        root=os.path.join(data_path, dataset_name),
        feat_name=feat_name,
        verbose=verbose,
        device=device
    )

    g = dataset.graph
    labels = dataset.labels
    if cfg.feat == 'clip':
        clip_feat = torch.load(os.path.join(BOOKS_PATH, 'clip_feat.pt'))
    elif cfg.feat == 'imagebind':
        clip_feat = torch.load(os.path.join(BOOKS_PATH, 'imagebind_feat.pt'))
    elif cfg.feat == 'dino':
        clip_feat = torch.load(os.path.join(BOOKS_PATH, 't5dino_feat.pt'))
    else: 
        clip_feat = torch.load(os.path.join(BOOKS_PATH, 't5vit_feat.pt'))
    if cfg.use_feature == 'text':
        clip_feat = clip_feat[:,:768]    
    g.ndata['feat'] = clip_feat
    g.ndata['label'] = labels
    g = dgl.remove_self_loop(g)
    g, reverse_eids = to_bidirected_with_reverse_mapping(g)
    g = g.to("cuda" if cfg.mode == "puregpu" else "cpu")
    if cfg.dataset == 'books':
        print("using books")
        num_classes = len(torch.unique(labels))
    else:
        num_classes = 12

    splits ={}         
    splits['train_idx'] = ndata['train_mask'].nonzero()
    splits['val_idx'] = ndata['val_mask'].nonzero()
    splits['test_idx'] = ndata['test_mask'].nonzero()
    
    num_nodes = g.num_nodes()
    # create GraphSAGE model
    in_size = g.ndata["feat"].shape[1]
    out_size = num_classes
    accs = []
    for run in range(cfg.runs):
        log.info("Run {}/{}".format(run + 1, cfg.runs))
        if cfg.model_name == "SAGE":
            model = SAGE(in_size, cfg.hidden_dim, out_size, cfg.num_layers).to(device)
        elif cfg.model_name == "GCN":
            model = GCN(in_size, cfg.hidden_dim, out_size, cfg.num_layers).to(device)
            if cfg.add_self_loop:
                g = dgl.add_self_loop(g)
        elif cfg.model_name == "MLP":
            model = MLP(in_size, cfg.hidden_dim, out_size, cfg.num_layers).to(device) 
        elif cfg.model_name == "MMGCN":
            model = MMGCN(in_size, 64, out_size, cfg.num_layers, num_nodes).to(device)          
        # # model training
        log.info("Training...")
        acc = train(cfg, device, g, splits, model, num_classes, run)
        accs.append(acc)

    mean_acc = torch.mean(torch.tensor(accs)).item()
    std_acc = torch.std(torch.tensor(accs)).item()
    print(mean_acc)
    print(std_acc)

    return mean_acc

if __name__=='__main__':
    main()
</file>

<file path="nc/models.py">
import torch
import torch.nn.functional as F
import dgl.nn as dglnn
import tqdm
import torch.nn as nn
from dgl.dataloading import DataLoader, NeighborSampler, MultiLayerFullNeighborSampler
from dgl.nn import GATv2Conv
from dgl.nn.pytorch.conv import GINConv
from torch.nn import Linear

class SAGE(nn.Module):
    def __init__(self, in_size, hid_size, out_size, num_layers=3, dropout=0.5):
        super().__init__()
        self.layers = nn.ModuleList()
        if num_layers == 1:
            self.layers.append(dglnn.SAGEConv(in_size, out_size, 'mean'))
        elif num_layers == 2:
            self.layers.append(dglnn.SAGEConv(in_size, hid_size, 'mean'))
            self.layers.append(dglnn.SAGEConv(hid_size, out_size, 'mean'))
        elif num_layers == 3:
            self.layers.append(dglnn.SAGEConv(in_size, hid_size, 'mean'))
            self.layers.append(dglnn.SAGEConv(hid_size, hid_size, 'mean'))
            self.layers.append(dglnn.SAGEConv(hid_size, out_size, 'mean'))
        self.dropout = nn.Dropout(dropout)
        self.hid_size = hid_size
        self.out_size = out_size

    def forward(self, blocks, x):
        h = x
        for l, (layer, block) in enumerate(zip(self.layers, blocks)):
            h = layer(block, h)
            if l != len(self.layers) - 1:
                h = F.relu(h)
                h = self.dropout(h)
        return h

    def inference(self, g, device, batch_size):
        """Conduct layer-wise inference to get all the node embeddings."""
        feat = g.ndata["feat"]
        sampler = MultiLayerFullNeighborSampler(1, prefetch_node_feats=["feat"])
        dataloader = DataLoader(
            g,
            torch.arange(g.num_nodes()).to(g.device),
            sampler,
            device=device,
            batch_size=batch_size,
            shuffle=False,
            drop_last=False,
            num_workers=0,
        )
        buffer_device = torch.device("cpu")
        pin_memory = buffer_device != device

        for l, layer in enumerate(self.layers):
            y = torch.empty(
                g.num_nodes(),
                self.hid_size if l != len(self.layers) - 1 else self.out_size,
                device=buffer_device,
                pin_memory=pin_memory,
            )
            feat = feat.to(device)
            for input_nodes, output_nodes, blocks in tqdm.tqdm(dataloader):
                x = feat[input_nodes]
                h = layer(blocks[0], x)  # len(blocks) = 1
                if l != len(self.layers) - 1:
                    h = F.relu(h)
                    h = self.dropout(h)
                # by design, our output nodes are contiguous
                y[output_nodes[0] : output_nodes[-1] + 1] = h.to(buffer_device)
            feat = y
        return y



class GCN(nn.Module):
    def __init__(self, in_size, hid_size, out_size, num_layers, dropout=0.5):
        super().__init__()
        self.layers = nn.ModuleList()
        if num_layers == 1:
            self.layers.append(dglnn.GraphConv(in_size, out_size, norm='both', weight=True, bias=True, allow_zero_in_degree=True))
        elif num_layers == 2:
            self.layers.append(dglnn.GraphConv(in_size, hid_size, norm='both', weight=True, bias=True, allow_zero_in_degree=True))
            self.layers.append(dglnn.GraphConv(hid_size, out_size, norm='both', weight=True, bias=True, allow_zero_in_degree=True))
        elif num_layers == 3:
            self.layers.append(dglnn.GraphConv(in_size, hid_size, norm='both', weight=True, bias=True, allow_zero_in_degree=True))
            self.layers.append(dglnn.GraphConv(hid_size, hid_size, norm='both', weight=True, bias=True, allow_zero_in_degree=True))
            self.layers.append(dglnn.GraphConv(hid_size, out_size, norm='both', weight=True, bias=True, allow_zero_in_degree=True))
        self.dropout = nn.Dropout(dropout)
        self.hid_size = hid_size
        self.out_size = out_size

    def forward(self, blocks, x):
        h = x
        for l, (layer, block) in enumerate(zip(self.layers, blocks)):
            h = layer(block, h)
            if l != len(self.layers) - 1:
                h = F.relu(h)
                h = self.dropout(h)
        return h

    def inference(self, g, device, batch_size):
        """Conduct layer-wise inference to get all the node embeddings."""
        feat = g.ndata["feat"]
        sampler = MultiLayerFullNeighborSampler(1, prefetch_node_feats=["feat"])
        dataloader = DataLoader(
            g,
            torch.arange(g.num_nodes()).to(g.device),
            sampler,
            device=device,
            batch_size=batch_size,
            shuffle=False,
            drop_last=False,
            num_workers=0,
        )
        buffer_device = torch.device("cpu")
        pin_memory = buffer_device != device

        for l, layer in enumerate(self.layers):
            y = torch.empty(
                g.num_nodes(),
                self.hid_size if l != len(self.layers) - 1 else self.out_size,
                device=buffer_device,
                pin_memory=pin_memory,
            )
            feat = feat.to(device)
            for input_nodes, output_nodes, blocks in tqdm.tqdm(dataloader):
                x = feat[input_nodes]
                h = layer(blocks[0], x)  # len(blocks) = 1
                if l != len(self.layers) - 1:
                    h = F.relu(h)
                    h = self.dropout(h)
                # by design, our output nodes are contiguous
                y[output_nodes[0] : output_nodes[-1] + 1] = h.to(buffer_device)
            feat = y
        return y




class MLP(nn.Module):
    def __init__(self, in_size, hid_size, out_size, num_layers=3, dropout=0.5):
        super().__init__()
        self.layers = nn.ModuleList()
        if num_layers == 1:
            self.layers.append(nn.Linear(in_size, out_size))
        elif num_layers == 2:
            self.layers.append(nn.Linear(in_size, hid_size))
            self.layers.append(nn.Linear(hid_size, out_size))
        elif num_layers == 3:
            self.layers.append(nn.Linear(in_size, hid_size))
            self.layers.append(nn.Linear(hid_size, hid_size))
            self.layers.append(nn.Linear(hid_size, out_size))
        self.dropout = nn.Dropout(dropout)
        self.hid_size = hid_size
        self.out_size = out_size

    def forward(self, blocks, x):
        h = x
        for l, (layer, block) in enumerate(zip(self.layers, blocks)):
            h = layer(h)
            if l != len(self.layers) - 1:
                h = F.relu(h)
                h = self.dropout(h)
        return h

    def inference(self, g, device, batch_size):
        """Conduct layer-wise inference to get all the node embeddings."""
        feat = g.ndata["feat"]
        sampler = MultiLayerFullNeighborSampler(1, prefetch_node_feats=["feat"])
        dataloader = DataLoader(
            g,
            torch.arange(g.num_nodes()).to(g.device),
            sampler,
            device=device,
            batch_size=batch_size,
            shuffle=False,
            drop_last=False,
            num_workers=0,
        )
        buffer_device = torch.device("cpu")
        pin_memory = buffer_device != device

        for l, layer in enumerate(self.layers):
            y = torch.empty(
                g.num_nodes(),
                self.hid_size if l != len(self.layers) - 1 else self.out_size,
                device=buffer_device,
                pin_memory=pin_memory,
            )
            feat = feat.to(device)
            for input_nodes, output_nodes, blocks in tqdm.tqdm(dataloader):
                x = feat[output_nodes]
                h = layer(x)  # len(blocks) = 1
                if l != len(self.layers) - 1:
                    h = F.relu(h)
                    h = self.dropout(h)
                # by design, our output nodes are contiguous
                y[output_nodes[0] : output_nodes[-1] + 1] = h.to(buffer_device)
            feat = y
        return y



class GATv2(nn.Module):
    def __init__(self, in_size, hid_size, num_layers, heads, activation, feat_drop, attn_drop,
                 negative_slope, residual):
        super(GATv2, self).__init__()
        self.num_layers = num_layers
        self.gatv2_layers = nn.ModuleList()
        self.activation = activation
        self.heads = heads
        self.hid_size = hid_size
        self.layer_norms = torch.nn.ModuleList()
        # input projection (no residual)
        self.gatv2_layers.append(
            GATv2Conv(in_size, hid_size, heads[0], feat_drop, attn_drop, negative_slope, False, self.activation,
                      bias=False, share_weights=True)
        )
        # hidden layers
        for l in range(num_layers - 1):
            # due to multi-head, the in_dim = num_hidden * num_heads
            self.layer_norms.append(nn.LayerNorm(hid_size * heads[l]))
            self.gatv2_layers.append(
                GATv2Conv(hid_size * heads[l], hid_size, heads[l + 1], feat_drop, attn_drop, negative_slope, residual,
                          self.activation, bias=False, share_weights=True)
            )
        # output projection
        self.predictor = nn.Sequential(
            nn.Linear(hid_size * heads[-1], hid_size),
            nn.ReLU(),
            nn.Linear(hid_size, hid_size),
            nn.ReLU(),
            nn.Linear(hid_size, 1))

    def forward(self, pair_graph, neg_pair_graph, blocks, x):
        h = x
        for l, (layer, block) in enumerate(zip(self.gatv2_layers, blocks)):
            h = layer(block, h).flatten(1)
        pos_src, pos_dst = pair_graph.edges()
        neg_src, neg_dst = neg_pair_graph.edges()
        h_pos = self.predictor(h[pos_src] * h[pos_dst])
        h_neg = self.predictor(h[neg_src] * h[neg_dst])
        return h_pos, h_neg

    def inference(self, g, device, batch_size):
        """Layer-wise inference algorithm to compute GNN node embeddings."""
        feat = g.ndata['feat'].float()
        sampler = MultiLayerFullNeighborSampler(1, prefetch_node_feats=['feat'])
        dataloader = DataLoader(
            g, torch.arange(g.num_nodes()).to(g.device), sampler, device=device,
            batch_size=batch_size, shuffle=False, drop_last=False,
            num_workers=0)
        buffer_device = torch.device('cpu')
        pin_memory = (buffer_device != device)
        for l, layer in enumerate(self.gatv2_layers):
            y = torch.empty(g.num_nodes(), self.hid_size * self.heads[l], device=buffer_device,
                            pin_memory=pin_memory)
            feat = feat.to(device)
            for input_nodes, output_nodes, blocks in tqdm.tqdm(dataloader, desc='Inference'):
                x = feat[input_nodes]
                h = layer(blocks[0], x).flatten(1)
                y[output_nodes] = h.to(buffer_device)
            feat = y
        return y


class GINMLP(nn.Module):
    """Construct two-layer MLP-type aggreator for GIN model"""

    def __init__(self, input_dim, hidden_dim, output_dim):
        super().__init__()
        self.linears = nn.ModuleList()
        # two-layer MLP
        self.linears.append(nn.Linear(input_dim, hidden_dim, bias=False))
        self.linears.append(nn.Linear(hidden_dim, output_dim, bias=False))
        self.batch_norm = nn.BatchNorm1d((hidden_dim))

    def forward(self, x):
        h = x
        h = F.relu(self.batch_norm(self.linears[0](h)))
        return self.linears[1](h)


class MMGCN(nn.Module):
    def __init__(self, in_size, hid_size, out_size, num_layers, num_nodes, dropout=0.5):
        super().__init__()

        self.id_embedding = nn.init.xavier_normal_(torch.rand((num_nodes, hid_size), requires_grad=True)).cuda()
        self.result = nn.init.xavier_normal_(torch.rand((num_nodes, hid_size))).cuda()


        self.v_gcn = nn.ModuleList()
        if num_layers == 1:
            self.v_gcn.append(dglnn.GraphConv(in_size, out_size, norm='both', weight=True, bias=True, allow_zero_in_degree=True))
        elif num_layers == 2:
            self.v_gcn.append(dglnn.GraphConv(in_size, hid_size, norm='both', weight=True, bias=True, allow_zero_in_degree=True))
            self.v_gcn.append(dglnn.GraphConv(hid_size+hid_size, out_size, norm='both', weight=True, bias=True, allow_zero_in_degree=True))
        elif num_layers == 3:
            self.v_gcn.append(dglnn.GraphConv(in_size, hid_size, norm='both', weight=True, bias=True, allow_zero_in_degree=True))
            self.v_gcn.append(dglnn.GraphConv(hid_size+hid_size, hid_size, norm='both', weight=True, bias=True, allow_zero_in_degree=True))
            self.v_gcn.append(dglnn.GraphConv(hid_size+hid_size, out_size, norm='both', weight=True, bias=True, allow_zero_in_degree=True))

        self.t_gcn = nn.ModuleList()
        if num_layers == 1:
            self.t_gcn.append(dglnn.GraphConv(in_size, out_size, norm='both', weight=True, bias=True, allow_zero_in_degree=True))
        elif num_layers == 2:
            self.t_gcn.append(dglnn.GraphConv(in_size, hid_size, norm='both', weight=True, bias=True, allow_zero_in_degree=True))
            self.t_gcn.append(dglnn.GraphConv(hid_size+hid_size, out_size, norm='both', weight=True, bias=True, allow_zero_in_degree=True))
        elif num_layers == 3:
            self.t_gcn.append(dglnn.GraphConv(in_size, hid_size, norm='both', weight=True, bias=True, allow_zero_in_degree=True))
            self.t_gcn.append(dglnn.GraphConv(hid_size+hid_size, hid_size, norm='both', weight=True, bias=True, allow_zero_in_degree=True))
            self.t_gcn.append(dglnn.GraphConv(hid_size+hid_size, out_size, norm='both', weight=True, bias=True, allow_zero_in_degree=True))

        self.dropout = nn.Dropout(dropout)
        self.hid_size = hid_size
        self.out_size = out_size

    def forward(self, blocks, x):
        x = F.normalize(x)
        h_v = x
        for l, (layer, block) in enumerate(zip(self.v_gcn, blocks)):
            h_temp = layer(block, h_v)
            x_hat =  layer(block, h_v) + self.id_embedding[block.dstdata['_ID']]
            h_v = torch.cat((h_v, x_hat), dim=1)
            if l != len(self.layers) - 1:
                h_v =  F.leaky_relu(h_v)

        h_t = x
        for l, (layer, block) in enumerate(zip(self.t_gcn, blocks)):
            x_hat =  layer(block, h_t) + self.id_embedding[block.dstdata['_ID']]
            h_t = torch.cat((h_t, x_hat), dim=1)
            if l != len(self.layers) - 1:
                h_t =  F.leaky_relu(h_t)

        h = (h_v + h_t)/2
        breakpoint()
        return h

    def inference(self, g, device, batch_size):
        """Conduct layer-wise inference to get all the node embeddings."""
        feat = g.ndata["feat"]
        sampler = MultiLayerFullNeighborSampler(1, prefetch_node_feats=["feat"])
        dataloader = DataLoader(
            g,
            torch.arange(g.num_nodes()).to(g.device),
            sampler,
            device=device,
            batch_size=batch_size,
            shuffle=False,
            drop_last=False,
            num_workers=0,
        )
        buffer_device = torch.device("cpu")
        pin_memory = buffer_device != device

        for l, layer in enumerate(self.v_gcn):
            y_v = torch.empty(
                g.num_nodes(),
                self.hid_size if l != len(self.v_gcn) - 1 else self.out_size,
                device=buffer_device,
                pin_memory=pin_memory,
            )
            feat = g.ndata["feat"].to(device)
            for input_nodes, output_nodes, blocks in tqdm.tqdm(dataloader):
                x = feat[input_nodes]
                h_v = layer(blocks[0], x)  # len(blocks) = 1
                if l != len(self.layers) - 1:
                    h_v = F.leaky_relu(h_v) + self.id_embedding
                # by design, our output nodes are contiguous
                y_v[output_nodes[0] : output_nodes[-1] + 1] = h_v.to(buffer_device)
            feat = y_v

        for l, layer in enumerate(self.t_gcn):
            y_t = torch.empty(
                g.num_nodes(),
                self.hid_size if l != len(self.t_gcn) - 1 else self.out_size,
                device=buffer_device,
                pin_memory=pin_memory,
            )
            feat = g.ndata["feat"].to(device)
            for input_nodes, output_nodes, blocks in tqdm.tqdm(dataloader):
                x = feat[input_nodes]
                breakpoint()
                h_t = layer(blocks[0], x)  # len(blocks) = 1
                if l != len(self.layers) - 1:
                    h_t = F.leaky_relu(h_t) + + self.id_embedding
                # by design, our output nodes are contiguous
                y_t[output_nodes[0] : output_nodes[-1] + 1] = h_t.to(buffer_device)
            feat = y_t

        y = (y_v + y_t)/2
        breakpoint()
        return y


</file>

<file path="nc/nc_dataset.py">
import os

import pandas as pd
import numpy as np
import torch
import dgl
from sklearn.metrics import roc_auc_score

class NodeClassificationDataset(object):
    def __init__(self, root: str, feat_name: str, verbose: bool=True, device: str="cpu"):
        """
        Args:
            root (str): root directory to store the dataset folder.
            feat_name (str): the name of the node features, e.g., "t5vit".
            verbose (bool): whether to print the information.
            device (str): device to use.
        """
        root = os.path.normpath(root)
        self.name = os.path.basename(root)
        self.verbose = verbose
        self.root = root
        self.feat_name = feat_name
        self.device = device
        if self.verbose:
            print(f"Dataset name: {self.name}")
            print(f'Feature name: {self.feat_name}')
            print(f'Device: {self.device}')
        
        edge_path = os.path.join(root, 'nc_edges-nodeid.pt')
        self.edge = torch.tensor(torch.load(edge_path), dtype=torch.int64).to(self.device)
        feat_path = os.path.join(root, f'{self.feat_name}_feat.pt')
        feat = torch.load(feat_path, map_location=self.device)
        self.num_nodes = feat.shape[0]
        
        src, dst = self.edge.t()[0], self.edge.t()[1]
        self.graph = dgl.graph((src, dst), num_nodes=self.num_nodes).to(self.device)
        self.graph.ndata['feat'] = feat
        
        labels_path = os.path.join(root, 'labels-w-missing.pt')
        self.labels = torch.tensor(torch.load(labels_path), dtype=torch.int64).to(self.device)
        self.graph.ndata['label'] = self.labels
        
        node_split_path = os.path.join(root, 'split.pt')
        self.node_split = torch.load(node_split_path)
        
        train_mask = torch.zeros(self.num_nodes, dtype=torch.bool).to(self.device)
        val_mask = torch.zeros(self.num_nodes, dtype=torch.bool).to(self.device)
        test_mask = torch.zeros(self.num_nodes, dtype=torch.bool).to(self.device)

        train_mask[self.node_split['train_idx']] = True
        val_mask[self.node_split['val_idx']] = True
        test_mask[self.node_split['test_idx']] = True

        self.graph.ndata['train_mask'] = train_mask
        self.graph.ndata['val_mask'] = val_mask
        self.graph.ndata['test_mask'] = test_mask
        
    def get_idx_split(self):
        return self.node_split
    
    def __getitem__(self, idx: int):
        assert idx == 0, 'This dataset has only one graph'
        return self.graph
    
    def __len__(self):
        return 1
    
    def __repr__(self):
        return '{}({})'.format(self.__class__.__name__, len(self))

# borrowed from OGB
class NodeClassificationEvaluator:
    def __init__(self, eval_metric: str):
        """
        Args:
            eval_metric (str): evaluation metric, can be "rocauc" or "acc".
        """
        self.num_tasks = 1
        self.eval_metric = eval_metric


    def _parse_and_check_input(self, input_dict):
        if self.eval_metric == 'rocauc' or self.eval_metric == 'acc':
            if not 'y_true' in input_dict:
                raise RuntimeError('Missing key of y_true')
            if not 'y_pred' in input_dict:
                raise RuntimeError('Missing key of y_pred')

            y_true, y_pred = input_dict['y_true'], input_dict['y_pred']

            '''
                y_true: numpy ndarray or torch tensor of shape (num_nodes num_tasks)
                y_pred: numpy ndarray or torch tensor of shape (num_nodes num_tasks)
            '''

            # converting to torch.Tensor to numpy on cpu
            if torch is not None and isinstance(y_true, torch.Tensor):
                y_true = y_true.detach().cpu().numpy()

            if torch is not None and isinstance(y_pred, torch.Tensor):
                y_pred = y_pred.detach().cpu().numpy()

            ## check type
            if not (isinstance(y_true, np.ndarray) and isinstance(y_true, np.ndarray)):
                raise RuntimeError('Arguments to Evaluator need to be either numpy ndarray or torch tensor')

            if not y_true.shape == y_pred.shape:
                raise RuntimeError('Shape of y_true and y_pred must be the same')

            if not y_true.ndim == 2:
                raise RuntimeError('y_true and y_pred must to 2-dim arrray, {}-dim array given'.format(y_true.ndim))

            if not y_true.shape[1] == self.num_tasks:
                raise RuntimeError('Number of tasks should be {} but {} given'.format(self.num_tasks, y_true.shape[1]))

            return y_true, y_pred

        else:
            raise ValueError('Undefined eval metric %s ' % (self.eval_metric))


    def eval(self, input_dict):

        if self.eval_metric == 'rocauc':
            y_true, y_pred = self._parse_and_check_input(input_dict)
            return self._eval_rocauc(y_true, y_pred)
        elif self.eval_metric == 'acc':
            y_true, y_pred = self._parse_and_check_input(input_dict)
            return self._eval_acc(y_true, y_pred)
        else:
            raise ValueError('Undefined eval metric %s ' % (self.eval_metric))

    @property
    def expected_input_format(self):
        desc = '==== Expected input format of Evaluator\n'
        if self.eval_metric == 'rocauc':
            desc += '{\'y_true\': y_true, \'y_pred\': y_pred}\n'
            desc += '- y_true: numpy ndarray or torch tensor of shape (num_nodes num_tasks)\n'
            desc += '- y_pred: numpy ndarray or torch tensor of shape (num_nodes num_tasks)\n'
            desc += 'where y_pred stores score values (for computing ROC-AUC),\n'
            desc += 'num_task is {}, and '.format(self.num_tasks)
            desc += 'each row corresponds to one node.\n'
        elif self.eval_metric == 'acc':
            desc += '{\'y_true\': y_true, \'y_pred\': y_pred}\n'
            desc += '- y_true: numpy ndarray or torch tensor of shape (num_nodes num_tasks)\n'
            desc += '- y_pred: numpy ndarray or torch tensor of shape (num_nodes num_tasks)\n'
            desc += 'where y_pred stores predicted class label (integer),\n'
            desc += 'num_task is {}, and '.format(self.num_tasks)
            desc += 'each row corresponds to one node.\n'
        else:
            raise ValueError('Undefined eval metric %s ' % (self.eval_metric))

        return desc

    @property
    def expected_output_format(self):
        desc = '==== Expected output format of Evaluator\n'
        if self.eval_metric == 'rocauc':
            desc += '{\'rocauc\': rocauc}\n'
            desc += '- rocauc (float): ROC-AUC score averaged across {} task(s)\n'.format(self.num_tasks)
        elif self.eval_metric == 'acc':
            desc += '{\'acc\': acc}\n'
            desc += '- acc (float): Accuracy score averaged across {} task(s)\n'.format(self.num_tasks)
        else:
            raise ValueError('Undefined eval metric %s ' % (self.eval_metric))

        return desc

    def _eval_rocauc(self, y_true, y_pred):
        '''
            compute ROC-AUC and AP score averaged across tasks
        '''

        rocauc_list = []

        for i in range(y_true.shape[1]):
            #AUC is only defined when there is at least one positive data.
            if np.sum(y_true[:,i] == 1) > 0 and np.sum(y_true[:,i] == 0) > 0:
                is_labeled = y_true[:,i] == y_true[:,i]
                rocauc_list.append(roc_auc_score(y_true[is_labeled,i], y_pred[is_labeled,i]))

        if len(rocauc_list) == 0:
            raise RuntimeError('No positively labeled data available. Cannot compute ROC-AUC.')

        return {'rocauc': sum(rocauc_list)/len(rocauc_list)}

    def _eval_acc(self, y_true, y_pred):
        acc_list = []

        for i in range(y_true.shape[1]):
            is_labeled = y_true[:,i] == y_true[:,i]
            correct = y_true[is_labeled,i] == y_pred[is_labeled,i]
            acc_list.append(float(np.sum(correct))/len(correct))

        return {'acc': sum(acc_list)/len(acc_list)}

</file>

</source>
</onefilellm_output>